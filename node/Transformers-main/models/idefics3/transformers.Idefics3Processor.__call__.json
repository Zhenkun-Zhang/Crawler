{
    "api": "transformers.Idefics3Processor.__call__",
    "type": "function",
    "version": "main",
    "args_list": [
        "images:",
        "typing.Union[ForwardRef('PIL.Image.Image'),",
        "numpy.ndarray,",
        "ForwardRef('torch.Tensor'),",
        "list['PIL.Image.Image'],",
        "list[numpy.ndarray],",
        "list['torch.Tensor'],",
        "typing.List[typing.Union[ForwardRef('PIL.Image.Image'),",
        "numpy.ndarray,",
        "ForwardRef('torch.Tensor'),",
        "list['PIL.Image.Image'],",
        "list[numpy.ndarray],",
        "list['torch.Tensor']]],",
        "typing.List[typing.List[typing.Union[ForwardRef('PIL.Image.Image'),",
        "numpy.ndarray,",
        "ForwardRef('torch.Tensor'),",
        "list['PIL.Image.Image'],",
        "list[numpy.ndarray],",
        "list['torch.Tensor']]]]]",
        "text",
        "ForwardRef('PreTokenizedInput'),",
        "typing.List[str],",
        "typing.List[ForwardRef('PreTokenizedInput')]]",
        "audio",
        "videos",
        "image_seq_len",
        "**kwargs"
    ],
    "params": [
        {
            "name": "images",
            "type": "PIL.Image.Image, np.ndarray, torch.Tensor, List[PIL.Image.Image], List[np.ndarray], List[torch.Tensor]",
            "optional": true,
            "default": "",
            "description": "The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorchtensor. If is of type List[ImageInput], its assumed that this is for a single prompt i.e. of batch size 1."
        },
        {
            "name": "text",
            "type": "Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]",
            "optional": true,
            "default": "",
            "description": "The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must setis_split_into_words=True (to lift the ambiguity with a batch of sequences).Wherever an image token, <image> is encountered it is expanded to<fake_token_around_image> + <row_x_col_y> + <image>  image_seq_len  `."
        },
        {
            "name": "image_seq_len",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The length of the image sequence. If not provided, the default value of self.image_seq_len is used.image_seq_len should be equal to int(((image_size // patch_size)  2) / (scale_factor2))"
        },
        {
            "name": "return_tensors",
            "type": "Union[str, TensorType]",
            "optional": true,
            "default": "",
            "description": "If set, will return tensors of a particular framework. See PreTrainedTokenizerFast.call() for moreinformation."
        }
    ],
    "return": ""
}