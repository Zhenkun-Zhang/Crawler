{
    "api": "transformers.DeepseekV3Config",
    "type": "class",
    "version": "main",
    "args_list": [
        "vocab_size",
        "hidden_size",
        "intermediate_size",
        "moe_intermediate_size",
        "num_hidden_layers",
        "num_attention_heads",
        "num_key_value_heads",
        "n_shared_experts",
        "n_routed_experts",
        "routed_scaling_factor",
        "kv_lora_rank",
        "q_lora_rank",
        "qk_rope_head_dim",
        "v_head_dim",
        "qk_nope_head_dim",
        "n_group",
        "topk_group",
        "num_experts_per_tok",
        "first_k_dense_replace",
        "norm_topk_prob",
        "hidden_act",
        "max_position_embeddings",
        "initializer_range",
        "rms_norm_eps",
        "use_cache",
        "pad_token_id",
        "bos_token_id",
        "eos_token_id",
        "pretraining_tp",
        "tie_word_embeddings",
        "rope_theta",
        "rope_scaling",
        "rope_interleave",
        "attention_bias",
        "attention_dropout",
        "**kwargs"
    ],
    "params": [
        {
            "name": "vocab_size",
            "type": "int",
            "optional": true,
            "default": "129280",
            "description": "Vocabulary size of the Deep model. Defines the number of different tokens that can be represented by theinputs_ids passed when calling DeepseekV3Model"
        },
        {
            "name": "hidden_size",
            "type": "int",
            "optional": true,
            "default": "7168",
            "description": "Dimension of the hidden representations."
        },
        {
            "name": "intermediate_size",
            "type": "int",
            "optional": true,
            "default": "18432",
            "description": "Dimension of the MLP representations."
        },
        {
            "name": "moe_intermediate_size",
            "type": "int",
            "optional": true,
            "default": "2048",
            "description": "Dimension of the MoE representations."
        },
        {
            "name": "num_hidden_layers",
            "type": "int",
            "optional": true,
            "default": "61",
            "description": "Number of hidden layers in the Transformer decoder."
        },
        {
            "name": "num_attention_heads",
            "type": "int",
            "optional": true,
            "default": "128",
            "description": "Number of attention heads for each attention layer in the Transformer decoder."
        },
        {
            "name": "num_key_value_heads",
            "type": "int",
            "optional": true,
            "default": "128",
            "description": "This is the number of key_value heads that should be used to implement Grouped Query Attention. Ifnum_key_value_heads=num_attention_heads, the model will use Multi Head Attention (MHA), ifnum_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group. For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to num_attention_heads`."
        },
        {
            "name": "n_shared_experts",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Number of shared experts."
        },
        {
            "name": "n_routed_experts",
            "type": "int",
            "optional": true,
            "default": "256",
            "description": "Number of routed experts."
        },
        {
            "name": "routed_scaling_factor",
            "type": "float",
            "optional": true,
            "default": "2.5",
            "description": "Scaling factor or routed experts."
        },
        {
            "name": "kv_lora_rank",
            "type": "int",
            "optional": true,
            "default": "512",
            "description": "Rank of the LoRA matrices for key and value projections."
        },
        {
            "name": "q_lora_rank",
            "type": "int",
            "optional": true,
            "default": "1536",
            "description": "Rank of the LoRA matrices for query projections."
        },
        {
            "name": "qk_rope_head_dim",
            "type": "int",
            "optional": true,
            "default": "64",
            "description": "Dimension of the query/key heads that use rotary position embeddings."
        },
        {
            "name": "v_head_dim",
            "type": "int",
            "optional": true,
            "default": "128",
            "description": "Dimension of the value heads."
        },
        {
            "name": "qk_nope_head_dim",
            "type": "int",
            "optional": true,
            "default": "128",
            "description": "Dimension of the query/key heads that dont use rotary position embeddings."
        },
        {
            "name": "n_group",
            "type": "int",
            "optional": true,
            "default": "8",
            "description": "Number of groups for routed experts."
        },
        {
            "name": "topk_group",
            "type": "int",
            "optional": true,
            "default": "4",
            "description": "Number of selected groups for each token(for each token, ensuring the selected experts is only within topk_group groups)."
        },
        {
            "name": "num_experts_per_tok",
            "type": "int",
            "optional": true,
            "default": "8",
            "description": "Number of selected experts, None means dense model."
        },
        {
            "name": "first_k_dense_replace",
            "type": "int",
            "optional": true,
            "default": "3",
            "description": "Number of dense layers in shallow layers(embed->dense->dense->…->dense->moe->moe…->lm_head).--k dense layers—/"
        },
        {
            "name": "norm_topk_prob",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether to normalize the weights of the routed experts."
        },
        {
            "name": "hidden_act",
            "type": "str,function",
            "optional": true,
            "default": "silu",
            "description": "The non-linear activation function (function or string) in the decoder."
        },
        {
            "name": "max_position_embeddings",
            "type": "int",
            "optional": true,
            "default": "4096",
            "description": "The maximum sequence length that this model might ever be used with."
        },
        {
            "name": "initializer_range",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "rms_norm_eps",
            "type": "float",
            "optional": true,
            "default": "1e-06",
            "description": "The epsilon used by the rms normalization layers."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not the model should return the last key/values attentions (not used by all models). Onlyrelevant if config.is_decoder=True."
        },
        {
            "name": "pad_token_id",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Padding token id."
        },
        {
            "name": "bos_token_id",
            "type": "int",
            "optional": true,
            "default": "0",
            "description": "Beginning of stream token id."
        },
        {
            "name": "eos_token_id",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "End of stream token id."
        },
        {
            "name": "pretraining_tp",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Experimental feature. Tensor parallelism rank used during pretraining. Please refer to thisdocument to understand more about it. This value isnecessary to ensure exact reproducibility of the pretraining results. Please refer to thisissue."
        },
        {
            "name": "tie_word_embeddings",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to tie weight embeddings"
        },
        {
            "name": "rope_theta",
            "type": "float",
            "optional": true,
            "default": "10000.0",
            "description": "The base period of the RoPE embeddings."
        },
        {
            "name": "rope_scaling",
            "type": "Dict",
            "optional": true,
            "default": "None",
            "description": "Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scalingstrategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is{type: strategy name, factor: scaling factor}. When using this flag, dont updatemax_position_embeddings to the expected new maximum."
        },
        {
            "name": "rope_interleave",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether to interleave the rotary position embeddings."
        },
        {
            "name": "attention_bias",
            "type": "bool, defaults to False",
            "optional": true,
            "default": "False",
            "description": "Whether to use a bias in the query, key, value and output projection layers during self-attention."
        },
        {
            "name": "attention_dropout",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "The dropout ratio for the attention probabilities."
        }
    ],
    "return": ""
}