{
    "api": "transformers.LxmertConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "vocab_size",
        "hidden_size",
        "num_attention_heads",
        "num_qa_labels",
        "num_object_labels",
        "num_attr_labels",
        "intermediate_size",
        "hidden_act",
        "hidden_dropout_prob",
        "attention_probs_dropout_prob",
        "max_position_embeddings",
        "type_vocab_size",
        "initializer_range",
        "layer_norm_eps",
        "l_layers",
        "x_layers",
        "r_layers",
        "visual_feat_dim",
        "visual_pos_dim",
        "visual_loss_normalizer",
        "task_matched",
        "task_mask_lm",
        "task_obj_predict",
        "task_qa",
        "visual_obj_loss",
        "visual_attr_loss",
        "visual_feat_loss",
        "**kwargs"
    ],
    "params": [
        {
            "name": "vocab_size",
            "type": "int",
            "optional": true,
            "default": "30522",
            "description": "Vocabulary size of the LXMERT model. Defines the number of different tokens that can be represented by theinputs_ids passed when calling LxmertModel or TFLxmertModel."
        },
        {
            "name": "hidden_size",
            "type": "int",
            "optional": true,
            "default": "768",
            "description": "Dimensionality of the encoder layers and the pooler layer."
        },
        {
            "name": "num_attention_heads",
            "type": "int",
            "optional": true,
            "default": "12",
            "description": "Number of attention heads for each attention layer in the Transformer encoder."
        },
        {
            "name": "num_qa_labels",
            "type": "int",
            "optional": true,
            "default": "9500",
            "description": "This represents the total number of different question answering (QA) labels there are. If using more thanone dataset with QA, the user will need to account for the total number of labels that all of the datasetshave in total."
        },
        {
            "name": "num_object_labels",
            "type": "int",
            "optional": true,
            "default": "1600",
            "description": "This represents the total number of semantically unique objects that lxmert will be able to classify apooled-object feature as belonging too."
        },
        {
            "name": "num_attr_labels",
            "type": "int",
            "optional": true,
            "default": "400",
            "description": "This represents the total number of semantically unique attributes that lxmert will be able to classify apooled-object feature as possessing."
        },
        {
            "name": "intermediate_size",
            "type": "int",
            "optional": true,
            "default": "3072",
            "description": "Dimensionality of the intermediate (often named feed-forward) layer in the Transformer encoder."
        },
        {
            "name": "hidden_act",
            "type": "str,Callable",
            "optional": true,
            "default": "gelu",
            "description": "The non-linear activation function (function or string) in the encoder and pooler. If string, gelu,relu, silu and gelu_new are supported."
        },
        {
            "name": "hidden_dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout probability for all fully connected layers in the embeddings, encoder, and pooler."
        },
        {
            "name": "attention_probs_dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout ratio for the attention probabilities."
        },
        {
            "name": "max_position_embeddings",
            "type": "int",
            "optional": true,
            "default": "512",
            "description": "The maximum sequence length that this model might ever be used with. Typically set this to something largejust in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "name": "type_vocab_size",
            "type": "int",
            "optional": true,
            "default": "2",
            "description": "The vocabulary size of the token_type_ids passed into BertModel."
        },
        {
            "name": "initializer_range",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "layer_norm_eps",
            "type": "float",
            "optional": true,
            "default": "1e-12",
            "description": "The epsilon used by the layer normalization layers."
        },
        {
            "name": "l_layers",
            "type": "int",
            "optional": true,
            "default": "9",
            "description": "Number of hidden layers in the Transformer language encoder."
        },
        {
            "name": "x_layers",
            "type": "int",
            "optional": true,
            "default": "5",
            "description": "Number of hidden layers in the Transformer cross modality encoder."
        },
        {
            "name": "r_layers",
            "type": "int",
            "optional": true,
            "default": "5",
            "description": "Number of hidden layers in the Transformer visual encoder."
        },
        {
            "name": "visual_feat_dim",
            "type": "int",
            "optional": true,
            "default": "2048",
            "description": "This represents the last dimension of the pooled-object features used as input for the model, representingthe size of each object feature itself."
        },
        {
            "name": "visual_pos_dim",
            "type": "int",
            "optional": true,
            "default": "4",
            "description": "This represents the number of spacial features that are mixed into the visual features. The default is setto 4 because most commonly this will represent the location of a bounding box. i.e., (x, y, width, height)"
        },
        {
            "name": "visual_loss_normalizer",
            "type": "float",
            "optional": true,
            "default": "6.67",
            "description": "This represents the scaling factor in which each visual loss is multiplied by if during pretraining, onedecided to train with multiple vision-based loss objectives."
        },
        {
            "name": "task_matched",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "This task is used for sentence-image matching. If the sentence correctly describes the image the label willbe 1. If the sentence does not correctly describe the image, the label will be 0."
        },
        {
            "name": "task_mask_lm",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not to add masked language modeling (as used in pretraining models such as BERT) to the lossobjective."
        },
        {
            "name": "task_obj_predict",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not to add object prediction, attribute prediction and feature regression to the loss objective."
        },
        {
            "name": "task_qa",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not to add the question-answering loss to the objective"
        },
        {
            "name": "visual_obj_loss",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not to calculate the object-prediction loss objective"
        },
        {
            "name": "visual_attr_loss",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not to calculate the attribute-prediction loss objective"
        },
        {
            "name": "visual_feat_loss",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not to calculate the feature-regression loss objective"
        }
    ],
    "return": ""
}