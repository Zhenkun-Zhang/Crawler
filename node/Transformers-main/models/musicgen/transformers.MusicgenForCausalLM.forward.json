{
    "api": "transformers.MusicgenForCausalLM.forward",
    "type": "function",
    "version": "main",
    "args_list": [
        "input_ids:",
        "LongTensor",
        "attention_mask",
        "encoder_hidden_states",
        "encoder_attention_mask",
        "head_mask",
        "cross_attn_head_mask",
        "past_key_values",
        "inputs_embeds",
        "labels",
        "use_cache",
        "output_attentions",
        "output_hidden_states",
        "return_dict",
        "**kwargs",
        ")"
    ],
    "params": [
        {
            "name": "input_ids",
            "type": "torch.LongTensor of shape (batch_size * num_codebooks, sequence_length)",
            "optional": false,
            "default": "",
            "description": "Indices of input sequence tokens in the vocabulary, corresponding to the sequence of audio codes.Indices can be obtained by encoding an audio prompt with an audio encoder model to predict audio codes,such as with the EncodecModel. See EncodecModel.encode() for details.What are input IDs?The input_ids will automatically be converted from shape (batch_size * num_codebooks, target_sequence_length) to (batch_size, num_codebooks, target_sequence_length) in the forward pass. Ifyou obtain audio codes from an audio encoding model, such as EncodecModel, ensure that the number offrames is equal to 1, and that you reshape the audio codes from (frames, batch_size, num_codebooks, target_sequence_length) to (batch_size * num_codebooks, target_sequence_length) prior to passing them asinput_ids."
        },
        {
            "name": "attention_mask",
            "type": "torch.Tensor of shape (batch_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:1 for tokens that are not masked,0 for tokens that are masked.What are attention masks?"
        },
        {
            "name": "encoder_hidden_states",
            "type": "torch.FloatTensor of shape (batch_size, encoder_sequence_length, hidden_size",
            "optional": true,
            "default": "",
            "description": "Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention ofthe decoder."
        },
        {
            "name": "encoder_attention_mask",
            "type": "torch.LongTensor of shape (batch_size, encoder_sequence_length",
            "optional": true,
            "default": "",
            "description": "Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask valuesselected in [0, 1]:1 for tokens that are not masked,0 for tokens that are masked.What are attention masks?"
        },
        {
            "name": "head_mask",
            "type": "torch.Tensor of shape (decoder_layers, decoder_attention_heads",
            "optional": true,
            "default": "",
            "description": "Mask to nullify selected heads of the attention modules. Mask values selected in [0, 1]:1 indicates the head is not masked,0 indicates the head is masked."
        },
        {
            "name": "cross_attn_head_mask",
            "type": "torch.Tensor of shape (decoder_layers, decoder_attention_heads",
            "optional": true,
            "default": "",
            "description": "Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performingcross-attention on hidden heads. Mask values selected in [0, 1]:1 indicates the head is not masked,0 indicates the head is masked."
        },
        {
            "name": "past_key_values",
            "type": "tuple(tuple(torch.FloatTensor)",
            "optional": true,
            "default": "",
            "description": "Tuple of tuple(torch.FloatTensor) of length config.n_layers, with each tuple having 2 tensors of shape(batch_size, num_heads, sequence_length, embed_size_per_head)) and 2 additional tensors of shape(batch_size, num_heads, encoder_sequence_length, embed_size_per_head).Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attentionblocks) that can be used (see past_key_values input) to speed up sequential decoding.If past_key_values are used, the user can optionally input only the last decoder_input_ids (those thatdont have their past key value states given to this model) of shape (batch_size, 1) instead of alldecoder_input_ids of shape (batch_size, sequence_length)."
        },
        {
            "name": "inputs_embeds",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length, hidden_size",
            "optional": true,
            "default": "",
            "description": "Optionally, instead of passing input_ids you can choose to directly pass an embedded representation.This is useful if you want more control over how to convert input_ids indices into associated vectorsthan the models internal embedding lookup matrix."
        },
        {
            "name": "output_attentions",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the attentions tensors of all attention layers. See attentions under returnedtensors for more detail."
        },
        {
            "name": "output_hidden_states",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the hidden states of all layers. See hidden_states under returned tensors formore detail."
        },
        {
            "name": "return_dict",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return a ModelOutput instead of a plain tuple."
        },
        {
            "name": "labels",
            "type": "torch.LongTensor of shape (batch_size, sequence_length, num_codebooks",
            "optional": true,
            "default": "",
            "description": "Labels for language modeling. Note that the labels are shifted inside the model, i.e. you can setlabels = input_ids Indices are selected in [-100, 0, ..., config.vocab_size] All labels set to -100are ignored (masked), the loss is only computed for labels in [0, ..., config.vocab_size]"
        }
    ],
    "return": "transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)"
}