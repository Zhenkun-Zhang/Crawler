{
    "api": "transformers.UdopEncoderModel.forward",
    "type": "function",
    "version": "main",
    "args_list": [
        "input_ids:",
        "bbox",
        "typing.Any]",
        "attention_mask",
        "pixel_values",
        "visual_bbox",
        "typing.Any]",
        "head_mask",
        "inputs_embeds",
        "output_attentions",
        "output_hidden_states",
        "return_dict",
        ")"
    ],
    "params": [
        {
            "name": "input_ids",
            "type": "torch.LongTensor of shape (batch_size, sequence_length)",
            "optional": false,
            "default": "",
            "description": "Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so youshould be able to pad the inputs on both the right and the left.Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() andPreTrainedTokenizer.call() for detail.To know more on how to prepare input_ids for pretraining take a look a T5 Training."
        },
        {
            "name": "attention_mask",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:1 for tokens that are not masked,0 for tokens that are masked.What are attention masks?"
        },
        {
            "name": "bbox",
            "type": "torch.LongTensor of shape ({0}, 4",
            "optional": true,
            "default": "",
            "description": "Bounding boxes of each input sequence tokens. Selected in the range [0, config.max_2d_position_embeddings-1]. Each bounding box should be a normalized version in (x0, y0, x1, y1)format, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1,y1) represents the position of the lower right corner.Note that sequence_length = token_sequence_length + patch_sequence_length + 1 where 1 is for [CLS]token. See pixel_values for patch_sequence_length."
        },
        {
            "name": "pixel_values",
            "type": "torch.FloatTensor of shape (batch_size, num_channels, height, width)",
            "optional": false,
            "default": "",
            "description": "Batch of document images. Each image is divided into patches of shape (num_channels, config.patch_size, config.patch_size) and the total number of patches (=patch_sequence_length) equals to ((height / config.patch_size) * (width / config.patch_size))."
        },
        {
            "name": "visual_bbox",
            "type": "torch.LongTensor of shape (batch_size, patch_sequence_length, 4",
            "optional": true,
            "default": "",
            "description": "Bounding boxes of each patch in the image. If not provided, bounding boxes are created in the model."
        },
        {
            "name": "head_mask",
            "type": "torch.FloatTensor of shape (num_heads,),(num_layers, num_heads",
            "optional": true,
            "default": "",
            "description": "Mask to nullify selected heads of the self-attention modules. Mask values selected in [0, 1]:1 indicates the head is not masked,0 indicates the head is masked."
        },
        {
            "name": "inputs_embeds",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length, hidden_size",
            "optional": true,
            "default": "",
            "description": "Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. Thisis useful if you want more control over how to convert input_ids indices into associated vectors than themodels internal embedding lookup matrix."
        },
        {
            "name": "output_attentions",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the attentions tensors of all attention layers. See attentions under returnedtensors for more detail."
        },
        {
            "name": "output_hidden_states",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the hidden states of all layers. See hidden_states under returned tensors formore detail."
        },
        {
            "name": "return_dict",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return a ModelOutput instead of a plain tuple."
        }
    ],
    "return": "transformers.models.udop.modeling_udop.BaseModelOutputWithAttentionMask or tuple(torch.FloatTensor)"
}