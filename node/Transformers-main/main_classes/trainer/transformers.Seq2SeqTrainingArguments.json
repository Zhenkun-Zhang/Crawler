{
    "api": "transformers.Seq2SeqTrainingArguments",
    "type": "class",
    "version": "main",
    "args_list": [
        "output_dir:",
        "overwrite_output_dir",
        "do_train",
        "do_eval",
        "do_predict",
        "eval_strategy",
        "str]",
        "prediction_loss_only",
        "per_device_train_batch_size",
        "per_device_eval_batch_size",
        "per_gpu_train_batch_size",
        "per_gpu_eval_batch_size",
        "gradient_accumulation_steps",
        "eval_accumulation_steps",
        "eval_delay",
        "torch_empty_cache_steps",
        "learning_rate",
        "weight_decay",
        "adam_beta1",
        "adam_beta2",
        "adam_epsilon",
        "max_grad_norm",
        "num_train_epochs",
        "max_steps",
        "lr_scheduler_type",
        "str]",
        "lr_scheduler_kwargs",
        "str,",
        "NoneType]",
        "warmup_ratio",
        "warmup_steps",
        "log_level",
        "log_level_replica",
        "log_on_each_node",
        "logging_dir",
        "logging_strategy",
        "str]",
        "logging_first_step",
        "logging_steps",
        "logging_nan_inf_filter",
        "save_strategy",
        "str]",
        "save_steps",
        "save_total_limit",
        "save_safetensors",
        "save_on_each_node",
        "save_only_model",
        "restore_callback_states_from_checkpoint",
        "no_cuda",
        "use_cpu",
        "use_mps_device",
        "seed",
        "data_seed",
        "jit_mode_eval",
        "use_ipex",
        "bf16",
        "fp16",
        "fp16_opt_level",
        "half_precision_backend",
        "bf16_full_eval",
        "fp16_full_eval",
        "tf32",
        "local_rank",
        "ddp_backend",
        "tpu_num_cores",
        "tpu_metrics_debug",
        "debug",
        "list[transformers.debug_utils.DebugOption]]",
        "dataloader_drop_last",
        "eval_steps",
        "dataloader_num_workers",
        "dataloader_prefetch_factor",
        "past_index",
        "run_name",
        "disable_tqdm",
        "remove_unused_columns",
        "label_names",
        "load_best_model_at_end",
        "metric_for_best_model",
        "greater_is_better",
        "ignore_data_skip",
        "fsdp",
        "str,",
        "NoneType]",
        "fsdp_min_num_params",
        "fsdp_config",
        "str,",
        "NoneType]",
        "tp_size",
        "fsdp_transformer_layer_cls_to_wrap",
        "accelerator_config",
        "str,",
        "NoneType]",
        "deepspeed",
        "str,",
        "NoneType]",
        "label_smoothing_factor",
        "optim",
        "str]",
        "optim_args",
        "adafactor",
        "group_by_length",
        "length_column_name",
        "report_to",
        "str,",
        "list[str]]",
        "ddp_find_unused_parameters",
        "ddp_bucket_cap_mb",
        "ddp_broadcast_buffers",
        "dataloader_pin_memory",
        "dataloader_persistent_workers",
        "skip_memory_metrics",
        "use_legacy_prediction_loop",
        "push_to_hub",
        "resume_from_checkpoint",
        "hub_model_id",
        "hub_strategy",
        "str]",
        "hub_token",
        "hub_private_repo",
        "hub_always_push",
        "gradient_checkpointing",
        "gradient_checkpointing_kwargs",
        "str,",
        "NoneType]",
        "include_inputs_for_metrics",
        "include_for_metrics",
        "eval_do_concat_batches",
        "fp16_backend",
        "push_to_hub_model_id",
        "push_to_hub_organization",
        "push_to_hub_token",
        "mp_parameters",
        "auto_find_batch_size",
        "full_determinism",
        "torchdynamo",
        "ray_scope",
        "ddp_timeout",
        "torch_compile",
        "torch_compile_backend",
        "torch_compile_mode",
        "include_tokens_per_second",
        "include_num_input_tokens_seen",
        "neftune_noise_alpha",
        "optim_target_modules",
        "str,",
        "list[str]]",
        "batch_eval_metrics",
        "eval_on_start",
        "use_liger_kernel",
        "eval_use_gather_object",
        "average_tokens_across_devices",
        "sortish_sampler",
        "predict_with_generate",
        "generation_max_length",
        "generation_num_beams",
        "generation_config",
        "pathlib.Path,",
        "transformers.generation.configuration_utils.GenerationConfig,",
        "NoneType]"
    ],
    "params": [
        {
            "name": "output_dir",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The output directory where the model predictions and checkpoints will be written."
        },
        {
            "name": "overwrite_output_dir",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "If True, overwrite the content of the output directory. Use this to continue training if output_dirpoints to a checkpoint directory."
        },
        {
            "name": "do_train",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to run training or not. This argument is not directly used by Trainer, its intended to be usedby your training/evaluation scripts instead. See the examplescripts for more details."
        },
        {
            "name": "do_eval",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to run evaluation on the validation set or not. Will be set to True if eval_strategy isdifferent from no. This argument is not directly used by Trainer, its intended to be used by yourtraining/evaluation scripts instead. See the examplescripts for more details."
        },
        {
            "name": "do_predict",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to run predictions on the test set or not. This argument is not directly used by Trainer, itsintended to be used by your training/evaluation scripts instead. See the examplescripts for more details."
        },
        {
            "name": "eval_strategy",
            "type": "str,IntervalStrategy",
            "optional": true,
            "default": "",
            "description": "The evaluation strategy to adopt during training. Possible values are:no: No evaluation is done during training.steps: Evaluation is done (and logged) every eval_steps.epoch: Evaluation is done at the end of each epoch."
        },
        {
            "name": "prediction_loss_only",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "When performing evaluation and generating predictions, only returns the loss."
        },
        {
            "name": "per_device_train_batch_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The batch size per device accelerator core/CPU for training."
        },
        {
            "name": "per_device_eval_batch_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The batch size per device accelerator core/CPU for evaluation."
        },
        {
            "name": "gradient_accumulation_steps",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of updates steps to accumulate the gradients for, before performing a backward/update pass.When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,evaluation, save will be conducted every gradient_accumulation_steps * xxx_step training examples."
        },
        {
            "name": "eval_accumulation_steps",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. Ifleft unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster butrequires more memory)."
        },
        {
            "name": "eval_delay",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Number of epochs or steps to wait for before the first evaluation can be performed, depending on theeval_strategy."
        },
        {
            "name": "torch_empty_cache_steps",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of steps to wait before calling torch.<device>.empty_cache(). If left unset or set to None, cache will not be emptied.This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about 10% slower performance."
        },
        {
            "name": "learning_rate",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The initial learning rate for AdamW optimizer."
        },
        {
            "name": "weight_decay",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamWoptimizer."
        },
        {
            "name": "adam_beta1",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The beta1 hyperparameter for the AdamW optimizer."
        },
        {
            "name": "adam_beta2",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The beta2 hyperparameter for the AdamW optimizer."
        },
        {
            "name": "adam_epsilon",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The epsilon hyperparameter for the AdamW optimizer."
        },
        {
            "name": "max_grad_norm",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Maximum gradient norm (for gradient clipping)."
        },
        {
            "name": "num_train_epochs(float,",
            "type": "optional, defaults to 3.0",
            "optional": true,
            "default": "",
            "description": "Total number of training epochs to perform (if not an integer, will perform the decimal part percents ofthe last epoch before stopping training)."
        },
        {
            "name": "max_steps",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs.For a finite dataset, training is reiterated through the dataset (if all data is exhausted) untilmax_steps is reached."
        },
        {
            "name": "lr_scheduler_type",
            "type": "str,SchedulerType",
            "optional": true,
            "default": "",
            "description": "The scheduler type to use. See the documentation of SchedulerType for all possible values."
        },
        {
            "name": "lr_scheduler_kwargs",
            "type": "‘dict’",
            "optional": true,
            "default": "",
            "description": "The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values."
        },
        {
            "name": "warmup_ratio",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Ratio of total training steps used for a linear warmup from 0 to learning_rate."
        },
        {
            "name": "warmup_steps",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio."
        },
        {
            "name": "log_level",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Logger log level to use on the main process. Possible choices are the log levels as strings: debug,info, warning, error and critical, plus a passive level which doesnt set anything and keeps thecurrent log level for the Transformers library (which will be warning by default)."
        },
        {
            "name": "log_level_replica",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Logger log level to use on replicas. Same choices as log_level"
        },
        {
            "name": "log_on_each_node",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "In multinode distributed training, whether to log using log_level once per node, or only on the mainnode."
        },
        {
            "name": "logging_dir",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "TensorBoard log directory. Will default to*output_dir/runs/CURRENT_DATETIME_HOSTNAME*."
        },
        {
            "name": "logging_strategy",
            "type": "str,IntervalStrategy",
            "optional": true,
            "default": "",
            "description": "The logging strategy to adopt during training. Possible values are:no: No logging is done during training.epoch: Logging is done at the end of each epoch.steps: Logging is done every logging_steps."
        },
        {
            "name": "logging_first_step",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to log the first global_step or not."
        },
        {
            "name": "logging_steps",
            "type": "int,float",
            "optional": true,
            "default": "",
            "description": "Number of update steps between two logs if logging_strategy=steps. Should be an integer or a float inrange [0,1). If smaller than 1, will be interpreted as ratio of total training steps."
        },
        {
            "name": "logging_nan_inf_filter",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to filter nan and inf losses for logging. If set to True the loss of every step that is nanor inf is filtered and the average loss of the current logging window is taken instead.logging_nan_inf_filter only influences the logging of loss values, it does not change the behavior thegradient is computed or applied to the model."
        },
        {
            "name": "save_strategy",
            "type": "str,SaveStrategy",
            "optional": true,
            "default": "",
            "description": "The checkpoint save strategy to adopt during training. Possible values are:no: No save is done during training.epoch: Save is done at the end of each epoch.steps: Save is done every save_steps.best: Save is done whenever a new best_metric is achieved.If epoch or steps is chosen, saving will also be performed at thevery end of training, always."
        },
        {
            "name": "save_steps",
            "type": "int,float",
            "optional": true,
            "default": "",
            "description": "Number of updates steps before two checkpoint saves if save_strategy=steps. Should be an integer or afloat in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps."
        },
        {
            "name": "save_total_limit",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints inoutput_dir. When load_best_model_at_end is enabled, the best checkpoint according tometric_for_best_model will always be retained in addition to the most recent ones. For example, forsave_total_limit=5 and load_best_model_at_end, the four last checkpoints will always be retainedalongside the best model. When save_total_limit=1 and load_best_model_at_end, it is possible that twocheckpoints are saved: the last one and the best one (if they are different)."
        },
        {
            "name": "save_safetensors",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Use safetensors saving and loading for state dicts instead ofdefault torch.load and torch.save."
        },
        {
            "name": "save_on_each_node",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "When doing multi-node distributed training, whether to save models and checkpoints on each node, or only onthe main one.This should not be activated when the different nodes use the same storage as the files will be saved withthe same names for each node."
        },
        {
            "name": "save_only_model",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.Note that when this is true, you wont be able to resume training from checkpoint.This enables you to save storage by not storing the optimizer, scheduler & rng state.You can only load the model using from_pretrained with this option set to True."
        },
        {
            "name": "restore_callback_states_from_checkpoint",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to restore the callback states from the checkpoint. If True, will overridecallbacks passed to the Trainer if they exist in the checkpoint."
        },
        {
            "name": "use_cpu",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to use cpu. If set to False, we will use cuda or mps device if available."
        },
        {
            "name": "seed",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the~Trainer.model_init function to instantiate the model if it has some randomly initialized parameters."
        },
        {
            "name": "data_seed",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Random seed to be used with data samplers. If not set, random generators for data sampling will use thesame seed as seed. This can be used to ensure reproducibility of data sampling, independent of the modelseed."
        },
        {
            "name": "jit_mode_eval",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to use PyTorch jit trace for inference."
        },
        {
            "name": "use_ipex",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Use Intel extension for PyTorch when it is available. IPEXinstallation."
        },
        {
            "name": "bf16",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higherNVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change."
        },
        {
            "name": "fp16",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training."
        },
        {
            "name": "fp16_opt_level",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "For fp16 training, Apex AMP optimization level selected in [O0, O1, O2, and O3]. See details onthe Apex documentation."
        },
        {
            "name": "fp16_backend",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "This argument is deprecated. Use half_precision_backend instead."
        },
        {
            "name": "half_precision_backend",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The backend to use for mixed precision training. Must be one of auto, apex, cpu_amp. auto willuse CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force therequested backend."
        },
        {
            "name": "bf16_full_eval",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harmmetric values. This is an experimental API and it may change."
        },
        {
            "name": "fp16_full_eval",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harmmetric values."
        },
        {
            "name": "tf32",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value dependson PyTorchs version default of torch.backends.cuda.matmul.allow_tf32. For more details please refer tothe TF32 documentation. This is anexperimental API and it may change."
        },
        {
            "name": "local_rank",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Rank of the process during distributed training."
        },
        {
            "name": "ddp_backend",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The backend to use for distributed training. Must be one of nccl, mpi, ccl, gloo, hccl."
        },
        {
            "name": "tpu_num_cores",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "When training on TPU, the number of TPU cores (automatically passed by launcher script)."
        },
        {
            "name": "dataloader_drop_last",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)or not."
        },
        {
            "name": "eval_steps",
            "type": "int,float",
            "optional": true,
            "default": "",
            "description": "Number of update steps between two evaluations if eval_strategy=steps. Will default to the samevalue as logging_steps if not set. Should be an integer or a float in range [0,1). If smaller than 1,will be interpreted as ratio of total training steps."
        },
        {
            "name": "dataloader_num_workers",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in themain process."
        },
        {
            "name": "past_index",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Some models like TransformerXL or XLNet can make use ofthe past hidden states for their predictions. If this argument is set to a positive int, the Trainer willuse the corresponding output (usually index 2) as the past state and feed it to the model at the nexttraining step under the keyword argument mems."
        },
        {
            "name": "run_name",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "A descriptor for the run. Typically used for wandb,mlflow, comet and swanlablogging. If not specified, will be the same as output_dir."
        },
        {
            "name": "disable_tqdm",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to disable the tqdm progress bars and table of metrics produced by~notebook.NotebookTrainingTracker in Jupyter Notebooks. Will default to True if the logging level isset to warn or lower (default), False otherwise."
        },
        {
            "name": "remove_unused_columns",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to automatically remove the columns unused by the model forward method."
        },
        {
            "name": "label_names",
            "type": "List[str]",
            "optional": true,
            "default": "",
            "description": "The list of keys in your dictionary of inputs that correspond to the labels.Will eventually default to the list of argument names accepted by the model that contain the word label,except if the model used is one of the XxxForQuestionAnswering in which case it will also include the[start_positions, end_positions] keys."
        },
        {
            "name": "load_best_model_at_end",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to load the best model found during training at the end of training. When this option isenabled, the best checkpoint will always be saved. Seesave_total_limitfor more.When set to True, the parameters save_strategy needs to be the same as eval_strategy, and inthe case it is steps, save_steps must be a round multiple of eval_steps."
        },
        {
            "name": "metric_for_best_model",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Use in conjunction with load_best_model_at_end to specify the metric to use to compare two differentmodels. Must be the name of a metric returned by the evaluation with or without the prefix eval_.If not specified, this will default to loss when either load_best_model_at_end == Trueor lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU (to use the evaluation loss).If you set this value, greater_is_better will default to True unless the name ends with loss.Dont forget to set it to False if your metric is better when lower."
        },
        {
            "name": "greater_is_better",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Use in conjunction with load_best_model_at_end and metric_for_best_model to specify if better modelsshould have a greater metric or not. Will default to:True if metric_for_best_model is set to a value that doesnt end in loss.False if metric_for_best_model is not set, or set to a value that ends in loss."
        },
        {
            "name": "ignore_data_skip",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "When resuming training, whether or not to skip the epochs and batches to get the data loading at the samestage as in the previous training. If set to True, the training will begin faster (as that skipping stepcan take a long time) but will not yield the same results as the interrupted training would have."
        },
        {
            "name": "fsdp",
            "type": "bool, str,list of FSDPOption",
            "optional": true,
            "default": "",
            "description": "Use PyTorch Distributed Parallel Training (in distributed training only).A list of options along the following:full_shard: Shard parameters, gradients and optimizer states.shard_grad_op: Shard optimizer states and gradients.hybrid_shard: Apply FULL_SHARD within a node, and replicate parameters across nodes.hybrid_shard_zero2: Apply SHARD_GRAD_OP within a node, and replicate parameters across nodes.offload: Offload parameters and gradients to CPUs (only compatible with full_shard andshard_grad_op).auto_wrap: Automatically recursively wrap layers with FSDP using default_auto_wrap_policy."
        },
        {
            "name": "fsdp_config",
            "type": "str,dict",
            "optional": true,
            "default": "",
            "description": "Config to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location offsdp json config file (e.g., fsdp_config.json) or an already loaded json file as dict.A List of config and its options:min_num_params (int, optional, defaults to 0):FSDPs minimum number of parameters for Default Auto Wrapping. (useful only when fsdp field ispassed).transformer_layer_cls_to_wrap (List[str], optional):List of transformer layer class names (case-sensitive) to wrap, e.g, BertLayer, GPTJBlock,T5Block … (useful only when fsdp flag is passed).backward_prefetch (str, optional)FSDPs backward prefetch mode. Controls when to prefetch next set of parameters (useful only whenfsdp field is passed).A list of options along the following:backward_pre : Prefetches the next set of parameters before the current set of parametersgradientcomputation.backward_post : This prefetches the next set of parameters after the current set ofparametersgradient computation.forward_prefetch (bool, optional, defaults to False)FSDPs forward prefetch mode (useful only when fsdp field is passed).If True, then FSDP explicitly prefetches the next upcoming all-gather while executing in theforward pass.limit_all_gathers (bool, optional, defaults to False)FSDPs limit_all_gathers (useful only when fsdp field is passed).If True, FSDP explicitly synchronizes the CPU thread to prevent too many in-flightall-gathers.use_orig_params (bool, optional, defaults to True)If True, allows non-uniform requires_grad during init, which means support for interspersedfrozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Pleaserefer this[blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019sync_module_states (bool, optional, defaults to True)If True, each individually wrapped FSDP unit will broadcast module parameters from rank 0 toensure they are the same across all ranks after initializationcpu_ram_efficient_loading (bool, optional, defaults to False)If True, only the first process loads the pretrained model checkpoint while all other processeshave empty weights.  When this setting as True, sync_module_states also must to be True,otherwise all the processes except the main process would have random weights leading to unexpectedbehaviour during training.activation_checkpointing (bool, optional, defaults to False):If True, activation checkpointing is a technique to reduce memory usage by clearing activations ofcertain layers and recomputing them during a backward pass. Effectively, this trades extracomputation time for reduced memory usage.xla (bool, optional, defaults to False):Whether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental featureand its API may evolve in the future.xla_fsdp_settings (dict, optional)The value is a dictionary which stores the XLA FSDP wrapping parameters.For a complete list of options, please see here.xla_fsdp_grad_ckpt (bool, optional, defaults to False):Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only beused when the xla flag is set to true, and an auto wrapping policy is specified throughfsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap."
        },
        {
            "name": "tp_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Use tp_size to enable PyTorch tensor parallelism. Tensor parallelism support is only available to models having base_tp_planin their respective config classes.Set a value greater than 1 to activate TP. The same is used to prepare device mesh internally. Requires accelerate>1.3.0."
        },
        {
            "name": "deepspeed",
            "type": "str,dict",
            "optional": true,
            "default": "",
            "description": "Use Deepspeed. This is an experimental feature and its API mayevolve in the future. The value is either the location of DeepSpeed json config file (e.g.,ds_config.json) or an already loaded json file as a dictIf enabling any Zero-init, make sure that your model is not initialized until*after* initializing the `TrainingArguments`, else it will not be applied."
        },
        {
            "name": "accelerator_config",
            "type": "str, dict,,AcceleratorConfig",
            "optional": true,
            "default": "",
            "description": "Config to be used with the internal Accelerator implementation. The value is either a location ofaccelerator json config file (e.g., accelerator_config.json), an already loaded json file as dict,or an instance of AcceleratorConfig.A list of config and its options:split_batches (bool, optional, defaults to False):Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. IfTrue the actual batch size used will be the same on any kind of distributed processes, but it must be around multiple of the num_processes you are using. If False, actual batch size used will be the one setin your script multiplied by the number of processes.dispatch_batches (bool, optional):If set to True, the dataloader prepared by the Accelerator is only iterated through on the main processand then the batches are split and broadcast to each process. Will default to True for DataLoader whoseunderlying dataset is an IterableDataset, False otherwise.even_batches (bool, optional, defaults to True):If set to True, in cases where the total batch size across all processes does not exactly divide thedataset, samples at the start of the dataset will be duplicated so the batch can be divided equally amongall workers.use_seedable_sampler (bool, optional, defaults to True):Whether or not use a fully seedable random sampler (accelerate.data_loader.SeedableRandomSampler). Ensurestraining results are fully reproducible using a different sampling technique. While seed-to-seed resultsmay differ, on average the differences are negligible when using multiple different seeds to compare. Shouldalso be ran with ~utils.set_seed for the best results.use_configured_state (bool, optional, defaults to False):Whether or not to use a pre-configured AcceleratorState or PartialState defined before calling TrainingArguments.If True, an Accelerator or PartialState must be initialized. Note that by doing so, this could lead to issueswith hyperparameter tuning."
        },
        {
            "name": "label_smoothing_factor",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encodedlabels are changed from 0s and 1s to label_smoothing_factor/num_labels and 1 - label_smoothing_factor + label_smoothing_factor/num_labels respectively."
        },
        {
            "name": "debug",
            "type": "str,list of DebugOption",
            "optional": true,
            "default": "",
            "description": "Enable one or more debug features. This is an experimental feature.Possible options are:underflow_overflow: detects overflow in models input/outputs and reports the last frames that led tothe eventtpu_metrics_debug: print debug metrics on TPUThe options should be separated by whitespaces."
        },
        {
            "name": "optim",
            "type": "str,training_args.OptimizerNames",
            "optional": true,
            "default": "",
            "description": "The optimizer to use, such as adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision,adafactor. See OptimizerNames in training_args.pyfor a full list of optimizers."
        },
        {
            "name": "optim_args",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore."
        },
        {
            "name": "group_by_length",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to group together samples of roughly the same length in the training dataset (to minimizepadding applied and be more efficient). Only useful if applying dynamic padding."
        },
        {
            "name": "length_column_name",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Column name for precomputed lengths. If the column exists, grouping by length will use these values ratherthan computing them on train startup. Ignored unless group_by_length is True and the dataset is aninstance of Dataset."
        },
        {
            "name": "report_to",
            "type": "str,List[str]",
            "optional": true,
            "default": "",
            "description": "The list of integrations to report the results and logs to. Supported platforms are azure_ml,clearml, codecarbon, comet_ml, dagshub, dvclive, flyte, mlflow, neptune,swanlab, tensorboard, and wandb. Use all to report to all integrations installed, nonefor no integrations."
        },
        {
            "name": "ddp_find_unused_parameters",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "When using distributed training, the value of the flag find_unused_parameters passed toDistributedDataParallel. Will default to False if gradient checkpointing is used, True otherwise."
        },
        {
            "name": "ddp_bucket_cap_mb",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "When using distributed training, the value of the flag bucket_cap_mb passed to DistributedDataParallel."
        },
        {
            "name": "ddp_broadcast_buffers",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "When using distributed training, the value of the flag broadcast_buffers passed toDistributedDataParallel. Will default to False if gradient checkpointing is used, True otherwise."
        },
        {
            "name": "dataloader_pin_memory",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether you want to pin memory in data loaders or not. Will default to True."
        },
        {
            "name": "dataloader_persistent_workers",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "If True, the data loader will not shut down the worker processes after a dataset has been consumed once.This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but willincrease RAM usage. Will default to False."
        },
        {
            "name": "dataloader_prefetch_factor",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of batches loaded in advance by each worker.2 means there will be a total of 2 * num_workers batches prefetched across all workers."
        },
        {
            "name": "skip_memory_metrics",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slowsdown the training and evaluation speed."
        },
        {
            "name": "push_to_hub",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to push the model to the Hub every time the model is saved. If this is activated,output_dir will begin a git directory synced with the repo (determined by hub_model_id) and the contentwill be pushed each time a save is triggered (depending on your save_strategy). Callingsave_model() will also trigger a push.If output_dir exists, it needs to be a local clone of the repository to which the Trainer will bepushed."
        },
        {
            "name": "resume_from_checkpoint",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The path to a folder with a valid checkpoint for your model. This argument is not directly used byTrainer, its intended to be used by your training/evaluation scripts instead. See the examplescripts for more details."
        },
        {
            "name": "hub_model_id",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The name of the repository to keep in sync with the local output_dir. It can be a simple model ID inwhich case the model will be pushed in your namespace. Otherwise it should be the whole repository name,for instance user_name/model, which allows you to push to an organization you are a member of withorganization_name/model. Will default to user_name/output_dir_name with output_dir_name being thename of output_dir.Will default to the name of output_dir."
        },
        {
            "name": "hub_strategy",
            "type": "str,HubStrategy",
            "optional": true,
            "default": "",
            "description": "Defines the scope of what is pushed to the Hub and when. Possible values are:end: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the Trainer) and adraft of a model card when the save_model() method is called.every_save: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the Trainer) anda draft of a model card each time there is a model save. The pushes are asynchronous to not blocktraining, and in case the save are very frequent, a new push is only attempted if the previous one isfinished. A last push is made with the final model at the end of training.checkpoint: like every_save but the latest checkpoint is also pushed in a subfolder namedlast-checkpoint, allowing you to resume training easily withtrainer.train(resume_from_checkpoint=last-checkpoint).all_checkpoints: like checkpoint but all checkpoints are pushed like they appear in the outputfolder (so you will get one checkpoint folder per folder in your final repository)"
        },
        {
            "name": "hub_token",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The token to use to push the model to the Hub. Will default to the token in the cache folder obtained withhuggingface-cli login."
        },
        {
            "name": "hub_private_repo",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to make the repo private. If None (default), the repo will be public unless the organizations default is private. This value is ignored if the repo already exists."
        },
        {
            "name": "hub_always_push",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Unless this is True, the Trainer will skip pushing a checkpoint when the previous push is not finished."
        },
        {
            "name": "gradient_checkpointing",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "If True, use gradient checkpointing to save memory at the expense of slower backward pass."
        },
        {
            "name": "gradient_checkpointing_kwargs",
            "type": "dict",
            "optional": true,
            "default": "",
            "description": "Key word arguments to be passed to the gradient_checkpointing_enable method."
        },
        {
            "name": "include_inputs_for_metrics",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "This argument is deprecated. Use include_for_metrics instead, e.g, include_for_metrics = [inputs]."
        },
        {
            "name": "include_for_metrics",
            "type": "List[str]",
            "optional": true,
            "default": "",
            "description": "Include additional data in the compute_metrics function if needed for metrics computation.Possible options to add to include_for_metrics list:inputs: Input data passed to the model, intended for calculating input dependent metrics.loss: Loss values computed during evaluation, intended for calculating loss dependent metrics."
        },
        {
            "name": "eval_do_concat_batches",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to recursively concat inputs/losses/labels/predictions across batches. If False,will instead store them as lists, with each batch kept separate."
        },
        {
            "name": "auto_find_batch_size",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to find a batch size that will fit into memory automatically through exponential decay, avoidingCUDA Out-of-Memory errors. Requires accelerate to be installed (pip install accelerate)"
        },
        {
            "name": "full_determinism",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "If True, enable_full_determinism() is called instead of set_seed() to ensure reproducible results indistributed training. Important: this will negatively impact the performance, so only use it for debugging."
        },
        {
            "name": "torchdynamo",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "If set, the backend compiler for TorchDynamo. Possible choices are eager, aot_eager, inductor,nvfuser, aot_nvfuser, aot_cudagraphs, ofi, fx2trt, onnxrt and ipex."
        },
        {
            "name": "ray_scope",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The scope to use when doing hyperparameter search with Ray. By default, last will be used. Ray willthen use the last checkpoint of all trials, compare those, and select the best one. However, other optionsare also available. See the Ray documentation formore options."
        },
        {
            "name": "ddp_timeout",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The timeout for torch.distributed.init_process_group calls, used to avoid GPU socket timeouts whenperforming slow operations in distributed runnings. Please refer the [PyTorch documentation](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for moreinformation."
        },
        {
            "name": "use_mps_device",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "This argument is deprecated.mps device will be used if it is available similar to cuda device."
        },
        {
            "name": "torch_compile",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to compile the model using PyTorch 2.0torch.compile.This will use the best defaults for the torch.compileAPI.You can customize the defaults with the argument torch_compile_backend and torch_compile_mode but wedont guarantee any of them will work as the support is progressively rolled in in PyTorch.This flag and the whole compile API is experimental and subject to change in future releases."
        },
        {
            "name": "torch_compile_backend",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The backend to use in torch.compile. If set to any value, torch_compile will be set to True.Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.This flag is experimental and subject to change in future releases."
        },
        {
            "name": "torch_compile_mode",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The mode to use in torch.compile. If set to any value, torch_compile will be set to True.Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.This flag is experimental and subject to change in future releases."
        },
        {
            "name": "include_tokens_per_second",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to compute the number of tokens per second per device for training speed metrics.This will iterate over the entire training dataloader once beforehand,and will slow down the entire process."
        },
        {
            "name": "include_num_input_tokens_seen",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to track the number of input tokens seen throughout training.May be slower in distributed training as gather operations must be called."
        },
        {
            "name": "neftune_noise_alpha",
            "type": "Optional[float]",
            "optional": false,
            "default": "",
            "description": "If not None, this will activate NEFTune noise embeddings. This can drastically improve model performancefor instruction fine-tuning. Check out the original paper and theoriginal code. Support transformers PreTrainedModel and alsoPeftModel from peft. The original paper used values in the range [5.0, 15.0]."
        },
        {
            "name": "optim_target_modules",
            "type": "Union[str, List[str]]",
            "optional": true,
            "default": "",
            "description": "The target modules to optimize, i.e. the module names that you would like to train.Currently used for the GaLore algorithm (https://arxiv.org/abs/2403.03507) and APOLLO algorithm (https://arxiv.org/abs/2412.05270).See GaLore implementation (https://github.com/jiaweizzhao/GaLore) and APOLLO implementation (https://github.com/zhuhanqing/APOLLO) for more details.You need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: apollo_adamw, galore_adamw, galore_adamw_8bit, galore_adafactor and make sure that the target modules are nn.Linear modules only."
        },
        {
            "name": "batch_eval_metrics",
            "type": "Optional[bool], defaults to False",
            "optional": false,
            "default": "",
            "description": "If set to True, evaluation will call compute_metrics at the end of each batch to accumulate statisticsrather than saving all eval logits in memory. When set to True, you must pass a compute_metrics functionthat takes a boolean argument compute_result, which when passed True, will trigger the final globalsummary statistics from the batch-level summary statistics youve accumulated over the evaluation set."
        },
        {
            "name": "eval_on_start",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly."
        },
        {
            "name": "eval_use_gather_object",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch."
        },
        {
            "name": "use_liger_kernel",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether enable Liger Kernel for LLM model training.It can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box withflash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models."
        },
        {
            "name": "average_tokens_across_devices",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronizenum_tokens_in_batch for precise loss calculation. Reference:https://github.com/huggingface/transformers/issues/34242"
        },
        {
            "name": "predict_with_generate",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to use generate to calculate generative metrics (ROUGE, BLEU)."
        },
        {
            "name": "generation_max_length",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The max_length to use on each evaluation loop when predict_with_generate=True. Will default to themax_length value of the model configuration."
        },
        {
            "name": "generation_num_beams",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The num_beams to use on each evaluation loop when predict_with_generate=True. Will default to thenum_beams value of the model configuration."
        },
        {
            "name": "generation_config",
            "type": "str,Path,GenerationConfig",
            "optional": true,
            "default": "",
            "description": "Allows to load a GenerationConfig from the from_pretrained method. This can be either:a string, the model id of a pretrained model configuration hosted inside a model repo onhuggingface.co.a path to a directory containing a configuration file saved using thesave_pretrained() method, e.g., ./my_model_directory/.a GenerationConfig object."
        }
    ],
    "return": ""
}