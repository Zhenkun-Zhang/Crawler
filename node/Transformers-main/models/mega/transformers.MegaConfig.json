{
    "api": "transformers.MegaConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "vocab_size",
        "hidden_size",
        "num_hidden_layers",
        "intermediate_size",
        "ema_projection_size",
        "bidirectional",
        "shared_representation_size",
        "use_chunking",
        "chunk_size",
        "truncation",
        "normalize_before_mega",
        "normalization_type",
        "norm_affine",
        "activation",
        "attention_activation",
        "dropout_prob",
        "hidden_dropout_prob",
        "attention_probs_dropout_prob",
        "use_feature_dropout",
        "use_normalized_ffn",
        "nffn_hidden_size",
        "normalize_before_ffn",
        "nffn_activation_dropout_prob",
        "max_positions",
        "add_token_type_embeddings",
        "type_vocab_size",
        "initializer_range",
        "ema_delta_alpha_range",
        "ema_beta_range",
        "ema_gamma_omega_range",
        "pad_token_id",
        "bos_token_id",
        "eos_token_id",
        "relative_positional_bias",
        "classifier_dropout",
        "use_cache",
        "add_lm_hidden_dense_layer",
        "**kwargs"
    ],
    "params": [
        {
            "name": "vocab_size",
            "type": "int",
            "optional": true,
            "default": "30522",
            "description": "Vocabulary size of the Mega model. Defines the number of different tokens that can be represented by theinputs_ids passed when calling MegaModel."
        },
        {
            "name": "hidden_size",
            "type": "int",
            "optional": true,
            "default": "128",
            "description": "Dimensionality of the encoder layers and the pooler layer."
        },
        {
            "name": "num_hidden_layers",
            "type": "int",
            "optional": true,
            "default": "4",
            "description": "Number of hidden layers in the Mega encoder."
        },
        {
            "name": "intermediate_size",
            "type": "int",
            "optional": true,
            "default": "256",
            "description": "Dimensionality of the hidden size (self-attention value projection) within the Mega encoder"
        },
        {
            "name": "ema_projection_size",
            "type": "int",
            "optional": true,
            "default": "16",
            "description": "Dimensionality of the MegaMultiDimensionDampedEma"
        },
        {
            "name": "bidirectional",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether the MegaMultiDimensionDampedEma used in Megas self-attention should work bidirectionally (True)or unidirectionally (False). Bidirectional EMA is incompatible with causal decoding, so this should beFalse if you intend to use the model as a decoder."
        },
        {
            "name": "shared_representation_size",
            "type": "int",
            "optional": true,
            "default": "64",
            "description": "Dimensionality of the linear projection for shared representation of self-attention queries and keys"
        },
        {
            "name": "use_chunking",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to chunk inputs for linear self-attention complexity (described as Mega-chunk in the paper)"
        },
        {
            "name": "chunk_size",
            "type": "int",
            "optional": true,
            "default": "-1",
            "description": "If use_chunking is set to True, determines the size of the chunks to apply to the input sequence. Ifchunking is used, input sequences must be padded to a multiple of chunk_size"
        },
        {
            "name": "truncation",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "If specified, the sequence length for which to truncate MegaMultiDimensionDampedEma"
        },
        {
            "name": "normalize_before_mega",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether to normalize before (True) or after (False) passing through Mega encoder blocks"
        },
        {
            "name": "normalization_type",
            "type": "str",
            "optional": true,
            "default": "scalenorm",
            "description": "Type of normalization to use in Mega encoder blocks. Choose one of scalenorm, layernorm,rmsnorm, batchnorm, or syncbatchnorm (GPU required for syncbatchnorm)"
        },
        {
            "name": "norm_affine",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "If True, applies a parameterized affine transformation to inputs during normalization"
        },
        {
            "name": "activation",
            "type": "str",
            "optional": true,
            "default": "silu",
            "description": "Activation function to apply within Mega encoder blocks. Choose one of silu, relu, linear,gelu, or gelu_accurate"
        },
        {
            "name": "attention_activation",
            "type": "str",
            "optional": true,
            "default": "softmax",
            "description": "Activation function to apply for single-headed self-attention (a la Transformer). Choose one ofsoftmax, laplace, or relu2"
        },
        {
            "name": "dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout probability for EMA self-attention"
        },
        {
            "name": "hidden_dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout probability for all fully connected layers in the embeddings, encoder, and pooler."
        },
        {
            "name": "attention_probs_dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout ratio for the attention probabilities."
        },
        {
            "name": "use_feature_dropout",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to use feature-based (True) or standard dropout (False)"
        },
        {
            "name": "use_normalized_ffn",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether to use the normalized feed-forward sub-layer in Mega blocks (True) or pass Mega encoder outputas-is (False)"
        },
        {
            "name": "nffn_hidden_size",
            "type": "int",
            "optional": true,
            "default": "256",
            "description": "If using the normalized feed-forward network (NFFN) layer within Mega (use_normalized_ffn = True), thisis the hidden size of the NFFN"
        },
        {
            "name": "normalize_before_ffn",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether to normalize before (True) or after (False) the feed-forward portion of NFFN"
        },
        {
            "name": "nffn_activation_dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout ratio for the NFFN component."
        },
        {
            "name": "max_positions",
            "type": "int",
            "optional": true,
            "default": "2048",
            "description": "The maximum sequence length to use for positional representations. For simple relative positional bias,this is a hard limit on input length, rotary relative positional bias will extrapolate to longersequences"
        },
        {
            "name": "add_token_type_embeddings",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to account for token types in embeddings. Left as optional to maintain compatibility with originalimplementation while adding support for token types."
        },
        {
            "name": "type_vocab_size",
            "type": "int",
            "optional": true,
            "default": "2",
            "description": "The vocabulary size of the token_type_ids passed when calling MegaModel. Only used ifadd_token_type_embeddings = True"
        },
        {
            "name": "initializer_range",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "ema_delta_alpha_range",
            "type": "float",
            "optional": true,
            "default": "0.2",
            "description": "The standard deviation for initializing the delta (damping factor) and alpha (decay factor) parameters inMegaMultiDimensionDampedEma."
        },
        {
            "name": "ema_beta_range",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation for initializing the beta parameter (expansion matrix) inMegaMultiDimensionDampedEma."
        },
        {
            "name": "ema_gamma_omega_range",
            "type": "float",
            "optional": true,
            "default": "1.0",
            "description": "The standard deviation for initializing the gamma (projection matrix) and omega (residual weight)parameters in MultiDimensionEMA."
        },
        {
            "name": "relative_positional_bias",
            "type": "str",
            "optional": true,
            "default": "rotary",
            "description": "Type of relative positional encoding. Choose one of rotary or simple. If simple is selected,max_positions is used as a limit on input size, while rotary extrapolates beyond max_positions."
        },
        {
            "name": "is_decoder",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether the model is used as a decoder or not. If False, the model is used as an encoder."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not the model should return the last key/values attentions (not used by all models). Onlyrelevant if config.is_decoder=True."
        },
        {
            "name": "classifier_dropout",
            "type": "float",
            "optional": true,
            "default": "None",
            "description": "The dropout ratio for the classification head."
        },
        {
            "name": "add_lm_hidden_dense_layer",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether to include a hidden layer for projection between encoder outputs and LM heads (True) or passhidden states directly to LM head (False). Remains optional for compatibility with originalimplementation"
        }
    ],
    "return": ""
}