{
    "api": "transformers.CpmAntConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "vocab_size:",
        "hidden_size",
        "num_attention_heads",
        "dim_head",
        "dim_ff",
        "num_hidden_layers",
        "dropout_p",
        "position_bias_num_buckets",
        "position_bias_max_distance",
        "eps",
        "init_std",
        "prompt_types",
        "prompt_length",
        "segment_types",
        "use_cache",
        "**kwargs"
    ],
    "params": [
        {
            "name": "vocab_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Vocabulary size of the CPMAnt model. Defines the number of different tokens that can be represented by theinput passed when calling CpmAntModel."
        },
        {
            "name": "hidden_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Dimension of the encoder layers."
        },
        {
            "name": "num_attention_heads",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of attention heads in the Transformer encoder."
        },
        {
            "name": "dim_head",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Dimension of attention heads for each attention layer in the Transformer encoder."
        },
        {
            "name": "dim_ff",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Dimension of the intermediate (i.e., feed-forward) layer in the Transformer encoder."
        },
        {
            "name": "num_hidden_layers",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of layers of the Transformer encoder."
        },
        {
            "name": "dropout_p",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The dropout probability for all fully connected layers in the embeddings, encoder."
        },
        {
            "name": "position_bias_num_buckets",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The number of position_bias buckets."
        },
        {
            "name": "position_bias_max_distance",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The maximum sequence length that this model might ever be used with. Typically set this to something largejust in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "name": "eps",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The epsilon used by the layer normalization layers."
        },
        {
            "name": "init_std",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Initialize parameters with std = init_std."
        },
        {
            "name": "prompt_types",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The type of prompt."
        },
        {
            "name": "prompt_length",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The length of prompt."
        },
        {
            "name": "segment_types",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The type of segment."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to use cache."
        }
    ],
    "return": ""
}