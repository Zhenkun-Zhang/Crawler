{
    "api": "paddlenlp.transformers.model_outputs.BaseModelOutputWithPast",
    "type": "class",
    "version": "stable",
    "args_list": {
        "last_hidden_state": "Tensor or None = None",
        "past_key_values": "Tuple[Tuple[Tensor]] or None = None",
        "hidden_states": "Tuple[Tensor] or None = None",
        "attentions": "Tuple[Tensor] or None = None"
    },
    "Bases": "",
    "description": "Base class for modelâ€™s outputs that may also contain a past key/values (to speed up sequential decoding).",
    "params": [
        {
            "name": "last_hidden_state",
            "type": "paddle.Tensor of shape batch_size, sequence_length, hidden_size",
            "description": "Sequence of hidden-states at the output of the last layer of the model.If past_key_values is used only the last hidden-state of the sequences of shape (batch_size, 1,hidden_size) is output.Sequence of hidden-states at the output of the last layer of the model.",
            "default": "",
            "optional": false
        },
        {
            "name": "past_key_values",
            "type": "tupletuplepaddle.Tensor",
            "description": "Tuple of tuple(paddle.Tensor) of length config.n_layers, with each tuple having 2 tensors of shape(batch_size, num_heads, sequence_length, embed_size_per_head)) and optionally ifconfig.is_encoder_decoder=True 2 additional tensors of shape (batch_size, num_heads,encoder_sequence_length, embed_size_per_head).Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally ifconfig.is_encoder_decoder=True in the cross-attention blocks) that can be used (see past_key_valuesinput) to speed up sequential decoding.Tuple of tuple(paddle.Tensor) of length config.n_layers, with each tuple having 2 tensors of shape(batch_size, num_heads, sequence_length, embed_size_per_head)) and optionally ifconfig.is_encoder_decoder=True 2 additional tensors of shape (batch_size, num_heads,encoder_sequence_length, embed_size_per_head).",
            "default": "",
            "optional": true
        },
        {
            "name": "hidden_states",
            "type": "tuplepaddle.Tensor",
            "description": "Tuple of paddle.Tensor (one for the output of the embeddings, if the model has an embedding layer, +one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.Tuple of paddle.Tensor (one for the output of the embeddings, if the model has an embedding layer, +one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).",
            "default": "",
            "optional": true
        },
        {
            "name": "attentions",
            "type": "tuplepaddle.Tensor",
            "description": "Tuple of paddle.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length,sequence_length).Attentions weights after the attention softmax, used to compute the weighted average in the self-attentionheads.Tuple of paddle.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length,sequence_length).",
            "default": "",
            "optional": true
        }
    ],
    "return": {
        "description": "",
        "type": ""
    }
}