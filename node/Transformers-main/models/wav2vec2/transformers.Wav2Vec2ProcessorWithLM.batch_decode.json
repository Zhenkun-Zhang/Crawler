{
    "api": "transformers.Wav2Vec2ProcessorWithLM.batch_decode",
    "type": "function",
    "version": "main",
    "args_list": [
        "logits:",
        "ndarray",
        "pool",
        "method",
        "BaseContext.Pool",
        "of",
        "<multiprocessing.context.DefaultContext",
        "object",
        "at",
        "0x7fd36d32bd30>>]",
        "num_processes",
        "beam_width",
        "beam_prune_logp",
        "token_min_logp",
        "hotwords",
        "hotword_weight",
        "alpha",
        "beta",
        "unk_score_offset",
        "lm_score_boundary",
        "output_word_offsets",
        "n_best"
    ],
    "params": [
        {
            "name": "logits",
            "type": "np.ndarray",
            "optional": false,
            "default": "",
            "description": "The logits output vector of the model representing the log probabilities for each token."
        },
        {
            "name": "pool",
            "type": "multiprocessing.Pool",
            "optional": true,
            "default": "",
            "description": "An optional user-managed pool. If not set, one will be automatically created and closed. The poolshould be instantiated after Wav2Vec2ProcessorWithLM. Otherwise, the LM wont be available to thepools sub-processes.Currently, only pools created with a fork context can be used. If a spawn pool is passed, it willbe ignored and sequential decoding will be used instead."
        },
        {
            "name": "num_processes",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "If pool is not set, number of processes on which the function should be parallelized over. Defaultsto the number of available CPUs."
        },
        {
            "name": "beam_width",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Maximum number of beams at each step in decoding. Defaults to pyctcdecodes DEFAULT_BEAM_WIDTH."
        },
        {
            "name": "beam_prune_logp",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Beams that are much worse than best beam will be pruned Defaults to pyctcdecodes DEFAULT_PRUNE_LOGP."
        },
        {
            "name": "token_min_logp",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Tokens below this logp are skipped unless they are argmax of frame Defaults to pyctcdecodesDEFAULT_MIN_TOKEN_LOGP."
        },
        {
            "name": "hotwords",
            "type": "List[str]",
            "optional": true,
            "default": "",
            "description": "List of words with extra importance, can be OOV for LM"
        },
        {
            "name": "hotword_weight",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Weight factor for hotword importance Defaults to pyctcdecodes DEFAULT_HOTWORD_WEIGHT."
        },
        {
            "name": "alpha",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Weight for language model during shallow fusion"
        },
        {
            "name": "beta",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Weight for length score adjustment of during scoring"
        },
        {
            "name": "unk_score_offset",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Amount of log score offset for unknown tokens"
        },
        {
            "name": "lm_score_boundary",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to have kenlm respect boundaries when scoring"
        },
        {
            "name": "output_word_offsets",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to output word offsets. Word offsets can be used in combination with the sampling rateand model downsampling rate to compute the time-stamps of transcribed words."
        },
        {
            "name": "n_best",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of best hypotheses to return. If n_best is greater than 1, the returned text will be a listof lists of strings, logit_score will be a list of lists of floats, and lm_score will be a list oflists of floats, where the length of the outer list will correspond to the batch size and the length ofthe inner list will correspond to the number of returned hypotheses . The value should be >= 1.Please take a look at the Example of decode() to better understand how tomake use of output_word_offsets. batch_decode() works the same way withbatched output."
        }
    ],
    "return": ""
}