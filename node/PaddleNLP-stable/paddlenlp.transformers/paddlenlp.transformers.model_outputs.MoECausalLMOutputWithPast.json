{
    "api": "paddlenlp.transformers.model_outputs.MoECausalLMOutputWithPast",
    "type": "class",
    "version": "stable",
    "args_list": {
        "loss": "Tensor or None = None",
        "aux_loss": "Tensor or None = None",
        "logits": "Tensor or None = None",
        "past_key_values": "Tuple[Tuple[Tensor]] or None = None",
        "hidden_states": "Tuple[Tensor] or None = None",
        "attentions": "Tuple[Tensor] or None = None",
        "router_logits": "Tuple[Tensor] or None = None"
    },
    "Bases": "",
    "description": "Base class for causal language model (or autoregressive) with mixture of experts outputs.",
    "params": [
        {
            "name": "loss",
            "type": "paddle.Tensor of shape 1,",
            "description": "Language modeling loss (for next-token prediction).",
            "default": "",
            "optional": true
        },
        {
            "name": "logits",
            "type": "paddle.Tensor of shape batch_size, sequence_length, config.vocab_size",
            "description": "Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).",
            "default": "",
            "optional": false
        },
        {
            "name": "aux_loss",
            "type": "paddle.Tensor",
            "description": "aux_loss for the sparse modules.",
            "default": "",
            "optional": true
        },
        {
            "name": "router_logits",
            "type": "tuplepaddle.Tensor",
            "description": "Tuple of paddle.Tensor (one for each layer) of shape (batch_size, sequence_length, num_experts).Raw router logtis (post-softmax) that are computed by MoE routers, these terms are used to compute the auxiliaryloss for Mixture of Experts models.Tuple of paddle.Tensor (one for each layer) of shape (batch_size, sequence_length, num_experts).",
            "default": "",
            "optional": true
        },
        {
            "name": "past_key_values",
            "type": "tupletuplepaddle.Tensor",
            "description": "Tuple of tuple(paddle.Tensor) of length config.n_layers, with each tuple having 2 tensors of shape(batch_size, num_heads, sequence_length, embed_size_per_head))Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (seepast_key_values input) to speed up sequential decoding.Tuple of tuple(paddle.Tensor) of length config.n_layers, with each tuple having 2 tensors of shape(batch_size, num_heads, sequence_length, embed_size_per_head))",
            "default": "",
            "optional": true
        },
        {
            "name": "hidden_states",
            "type": "tuplepaddle.Tensor",
            "description": "Tuple of paddle.Tensor (one for the output of the embeddings, if the model has an embedding layer, +one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.Tuple of paddle.Tensor (one for the output of the embeddings, if the model has an embedding layer, +one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).",
            "default": "",
            "optional": true
        },
        {
            "name": "attentions",
            "type": "tuplepaddle.Tensor",
            "description": "Tuple of paddle.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length,sequence_length).Attentions weights after the attention softmax, used to compute the weighted average in the self-attentionheads.Tuple of paddle.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length,sequence_length).",
            "default": "",
            "optional": true
        }
    ],
    "return": {
        "description": "",
        "type": ""
    }
}