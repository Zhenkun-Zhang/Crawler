{
    "api": "transformers.GraphormerConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "num_classes:",
        "num_atoms",
        "num_edges",
        "num_in_degree",
        "num_out_degree",
        "num_spatial",
        "num_edge_dis",
        "multi_hop_max_dist",
        "spatial_pos_max",
        "edge_type",
        "max_nodes",
        "share_input_output_embed",
        "num_hidden_layers",
        "embedding_dim",
        "ffn_embedding_dim",
        "num_attention_heads",
        "dropout",
        "attention_dropout",
        "activation_dropout",
        "layerdrop",
        "encoder_normalize_before",
        "pre_layernorm",
        "apply_graphormer_init",
        "activation_fn",
        "embed_scale",
        "freeze_embeddings",
        "num_trans_layers_to_freeze",
        "traceable",
        "q_noise",
        "qn_block_size",
        "kdim",
        "vdim",
        "bias",
        "self_attention",
        "pad_token_id",
        "bos_token_id",
        "eos_token_id",
        "**kwargs"
    ],
    "params": [
        {
            "name": "num_classes",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of target classes or labels, set to n for binary classification of n tasks."
        },
        {
            "name": "num_atoms",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of node types in the graphs."
        },
        {
            "name": "num_edges",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of edges types in the graph."
        },
        {
            "name": "num_in_degree",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of in degrees types in the input graphs."
        },
        {
            "name": "num_out_degree",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of out degrees types in the input graphs."
        },
        {
            "name": "num_edge_dis",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of edge dis in the input graphs."
        },
        {
            "name": "multi_hop_max_dist",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Maximum distance of multi hop edges between two nodes."
        },
        {
            "name": "spatial_pos_max",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Maximum distance between nodes in the graph attention bias matrices, used during preprocessing andcollation."
        },
        {
            "name": "edge_type",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Type of edge relation chosen."
        },
        {
            "name": "max_nodes",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Maximum number of nodes which can be parsed for the input graphs."
        },
        {
            "name": "share_input_output_embed",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Shares the embedding layer between encoder and decoder - careful, True is not implemented."
        },
        {
            "name": "num_layers",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of layers."
        },
        {
            "name": "embedding_dim",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Dimension of the embedding layer in encoder."
        },
        {
            "name": "ffn_embedding_dim",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Dimension of the intermediate (often named feed-forward) layer in encoder."
        },
        {
            "name": "num_attention_heads",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of attention heads in the encoder."
        },
        {
            "name": "self_attention",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Model is self attentive (False not implemented)."
        },
        {
            "name": "activation_function",
            "type": "str,function",
            "optional": true,
            "default": "",
            "description": "The non-linear activation function (function or string) in the encoder and pooler. If string, gelu,relu, silu and gelu_new are supported."
        },
        {
            "name": "dropout",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The dropout probability for all fully connected layers in the embeddings, encoder, and pooler."
        },
        {
            "name": "attention_dropout",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The dropout probability for the attention weights."
        },
        {
            "name": "activation_dropout",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The dropout probability for the activation of the linear transformer layer."
        },
        {
            "name": "layerdrop",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The LayerDrop probability for the encoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)for more details."
        },
        {
            "name": "bias",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Uses bias in the attention module - unsupported at the moment."
        },
        {
            "name": "embed_scale(float,",
            "type": "optional, defaults to None",
            "optional": true,
            "default": "",
            "description": "Scaling factor for the node embeddings."
        },
        {
            "name": "num_trans_layers_to_freeze",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of transformer layers to freeze."
        },
        {
            "name": "encoder_normalize_before",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Normalize features before encoding the graph."
        },
        {
            "name": "pre_layernorm",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Apply layernorm before self attention and the feed forward network. Without this, post layernorm will beused."
        },
        {
            "name": "apply_graphormer_init",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Apply a custom graphormer initialisation to the model before training."
        },
        {
            "name": "freeze_embeddings",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Freeze the embedding layer, or train it along the model."
        },
        {
            "name": "encoder_normalize_before",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Apply the layer norm before each encoder block."
        },
        {
            "name": "q_noise",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Amount of quantization noise (see Training with Quantization Noise for Extreme Model Compression). (Formore detail, see fairseqs documentation on quant_noise)."
        },
        {
            "name": "qn_block_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Size of the blocks for subsequent quantization with iPQ (see q_noise)."
        },
        {
            "name": "kdim",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Dimension of the key in the attention, if different from the other values."
        },
        {
            "name": "vdim",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Dimension of the value in the attention, if different from the other values."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not the model should return the last key/values attentions (not used by all models)."
        },
        {
            "name": "traceable",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Changes return value of the encoders inner_state to stacked tensors."
        },
        {
            "name": "Example",
            "type": "",
            "optional": false,
            "default": "",
            "description": ""
        }
    ],
    "return": ""
}