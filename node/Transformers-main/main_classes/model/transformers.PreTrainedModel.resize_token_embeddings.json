{
    "api": "transformers.PreTrainedModel.resize_token_embeddings",
    "type": "function",
    "version": "main",
    "args_list": [
        "new_num_tokens:",
        "pad_to_multiple_of",
        "mean_resizing",
        ")"
    ],
    "params": [
        {
            "name": "new_num_tokens",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The new number of tokens in the embedding matrix. Increasing the size will add newly initializedvectors at the end. Reducing the size will remove vectors from the end. If not provided or None, justreturns a pointer to the input tokens torch.nn.Embedding module of the model without doing anything."
        },
        {
            "name": "pad_to_multiple_of",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "If set will pad the embedding matrix to a multiple of the provided value.If new_num_tokens is set toNone will just pad the embedding to a multiple of pad_to_multiple_of.This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability>= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For moredetails about this, or help on choosing the correct value for resizing, refer to this guide:https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc"
        },
        {
            "name": "mean_resizing",
            "type": "bool",
            "optional": false,
            "default": "",
            "description": "Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings mean andcovariance or to initialize them with a normal distribution that has a mean of zero and std equals config.initializer_range.Setting mean_resizing to True is useful when increasing the size of the embeddings of causal language models,where the generated tokens probabilities wont be affected by the added embeddings because initializing the new embeddings with theold embeddings mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html"
        }
    ],
    "return": "torch.nn.Embedding"
}