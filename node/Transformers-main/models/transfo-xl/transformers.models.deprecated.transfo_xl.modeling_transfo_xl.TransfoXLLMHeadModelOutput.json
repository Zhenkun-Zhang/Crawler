{
    "api": "transformers.models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput",
    "type": "class",
    "version": "main",
    "args_list": [
        "losses:",
        "prediction_scores",
        "mems",
        "hidden_states",
        "attentions",
        "loss"
    ],
    "params": [
        {
            "name": "losses",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length-1",
            "optional": true,
            "default": "",
            "description": "Language modeling losses (not reduced)."
        },
        {
            "name": "prediction_scores",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size)",
            "optional": false,
            "default": "",
            "description": "Prediction scores of the language modeling head (scores for each vocabulary token after SoftMax)."
        },
        {
            "name": "mems",
            "type": "List[torch.FloatTensor] of length config.n_layers",
            "optional": false,
            "default": "",
            "description": "Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see memsinput) to speed up sequential decoding. The token ids which have their past given to this model should notbe passed as input ids as they have already been computed."
        },
        {
            "name": "hidden_states",
            "type": "tuple(torch.FloatTensor",
            "optional": true,
            "default": "",
            "description": "Tuple of torch.FloatTensor (one for the output of the embeddings + one for the output of each layer) ofshape (batch_size, sequence_length, hidden_size).Hidden-states of the model at the output of each layer plus the initial embedding outputs."
        },
        {
            "name": "attentions",
            "type": "tuple(torch.FloatTensor",
            "optional": true,
            "default": "",
            "description": "Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).Attentions weights after the attention softmax, used to compute the weighted average in the self-attentionheads."
        },
        {
            "name": "loss",
            "type": "torch.FloatTensor of shape (",
            "optional": true,
            "default": "",
            "description": "Reduced language modeling loss."
        }
    ],
    "return": ""
}