{
    "api": "transformers.DetrForSegmentation.forward",
    "type": "function",
    "version": "main",
    "args_list": [
        "pixel_values:",
        "FloatTensor",
        "pixel_mask",
        "decoder_attention_mask",
        "encoder_outputs",
        "inputs_embeds",
        "decoder_inputs_embeds",
        "labels",
        "output_attentions",
        "output_hidden_states",
        "return_dict",
        ")"
    ],
    "params": [
        {
            "name": "pixel_values",
            "type": "torch.FloatTensor of shape (batch_size, num_channels, height, width)",
            "optional": false,
            "default": "",
            "description": "Pixel values. Padding will be ignored by default should you provide it.Pixel values can be obtained using AutoImageProcessor. See DetrImageProcessor.call() for details."
        },
        {
            "name": "pixel_mask",
            "type": "torch.LongTensor of shape (batch_size, height, width",
            "optional": true,
            "default": "",
            "description": "Mask to avoid performing attention on padding pixel values. Mask values selected in [0, 1]:1 for pixels that are real (i.e. not masked),0 for pixels that are padding (i.e. masked).What are attention masks?"
        },
        {
            "name": "decoder_attention_mask",
            "type": "torch.FloatTensor of shape (batch_size, num_queries",
            "optional": true,
            "default": "",
            "description": "Not used by default. Can be used to mask object queries."
        },
        {
            "name": "encoder_outputs",
            "type": "tuple(tuple(torch.FloatTensor",
            "optional": true,
            "default": "",
            "description": "Tuple consists of (last_hidden_state, optional: hidden_states, optional: attentions)last_hidden_state of shape (batch_size, sequence_length, hidden_size), optional) is a sequence ofhidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder."
        },
        {
            "name": "inputs_embeds",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length, hidden_size",
            "optional": true,
            "default": "",
            "description": "Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), youcan choose to directly pass a flattened representation of an image."
        },
        {
            "name": "decoder_inputs_embeds",
            "type": "torch.FloatTensor of shape (batch_size, num_queries, hidden_size",
            "optional": true,
            "default": "",
            "description": "Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass anembedded representation."
        },
        {
            "name": "output_attentions",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the attentions tensors of all attention layers. See attentions under returnedtensors for more detail."
        },
        {
            "name": "output_hidden_states",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the hidden states of all layers. See hidden_states under returned tensors formore detail."
        },
        {
            "name": "return_dict",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return a ModelOutput instead of a plain tuple."
        },
        {
            "name": "labels",
            "type": "List[Dict] of len (batch_size,",
            "optional": true,
            "default": "",
            "description": "Labels for computing the bipartite matching loss, DICE/F-1 loss and Focal loss. List of dicts, eachdictionary containing at least the following 3 keys: class_labels, boxes and masks (the class labels,bounding boxes and segmentation masks of an image in the batch respectively). The class labels themselvesshould be a torch.LongTensor of len (number of bounding boxes in the image,), the boxes atorch.FloatTensor of shape (number of bounding boxes in the image, 4) and the masks atorch.FloatTensor of shape (number of bounding boxes in the image, height, width)."
        }
    ],
    "return": "transformers.models.detr.modeling_detr.DetrSegmentationOutput or tuple(torch.FloatTensor)"
}