{
    "api": "transformers.FalconConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "vocab_size",
        "hidden_size",
        "num_hidden_layers",
        "num_attention_heads",
        "num_ln_in_parallel_attn",
        "layer_norm_epsilon",
        "initializer_range",
        "use_cache",
        "hidden_dropout",
        "attention_dropout",
        "num_kv_heads",
        "alibi",
        "new_decoder_architecture",
        "multi_query",
        "parallel_attn",
        "bias",
        "max_position_embeddings",
        "rope_theta",
        "rope_scaling",
        "bos_token_id",
        "eos_token_id",
        "ffn_hidden_size",
        "activation",
        "**kwargs"
    ],
    "params": [
        {
            "name": "vocab_size",
            "type": "int",
            "optional": true,
            "default": "65024",
            "description": "Vocabulary size of the Falcon model. Defines the number of different tokens that can be represented by theinputs_ids passed when calling FalconModel"
        },
        {
            "name": "hidden_size",
            "type": "int",
            "optional": true,
            "default": "4544",
            "description": "Dimension of the hidden representations."
        },
        {
            "name": "num_hidden_layers",
            "type": "int",
            "optional": true,
            "default": "32",
            "description": "Number of hidden layers in the Transformer decoder."
        },
        {
            "name": "num_attention_heads",
            "type": "int",
            "optional": true,
            "default": "71",
            "description": "Number of attention heads for each attention layer in the Transformer encoder."
        },
        {
            "name": "num_ln_in_parallel_attn",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Set to 2 if separate layer norms are to be used for the MLP and the attention output when using parallelattention, otherwise, 1."
        },
        {
            "name": "layer_norm_epsilon",
            "type": "float",
            "optional": true,
            "default": "1e-05",
            "description": "The epsilon used by the layer normalization layers."
        },
        {
            "name": "initializer_range",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether the model should return the last key/values attentions (not used by all models). Only relevant ifconfig.is_decoder=True."
        },
        {
            "name": "hidden_dropout",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "The dropout probability for MLP layers."
        },
        {
            "name": "attention_dropout",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "The dropout probability for attention layers."
        },
        {
            "name": "num_kv_heads",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Number of key-value heads to use per attention layer. If unset, defaults to the same value asnum_attention_heads."
        },
        {
            "name": "alibi",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to use ALiBi positional biases during self-attention."
        },
        {
            "name": "new_decoder_architecture",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to use the new (Falcon-40B) decoder architecture. If True, the multi_query and parallel_attnarguments are ignored, as the new decoder always uses parallel attention."
        },
        {
            "name": "multi_query",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether to use multi-query attention in the decoder. Ignored when new_decoder_architecture is True."
        },
        {
            "name": "parallel_attn",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether to compute attention in parallel with the feedforward layer. If False, they are consecutiveinstead, as in the original Transformer architecture. Ignored when new_decoder_architecture is True."
        },
        {
            "name": "bias",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to use bias on Linear layers."
        },
        {
            "name": "max_position_embeddings",
            "type": "int",
            "optional": true,
            "default": "2048",
            "description": "The maximum sequence length that this model might ever be used with, when alibi is False. PretrainedFalcon models with RoPE support up to 2048 tokens."
        },
        {
            "name": "rope_theta",
            "type": "float",
            "optional": true,
            "default": "10000.0",
            "description": "The base period of the RoPE embeddings."
        },
        {
            "name": "rope_scaling",
            "type": "Dict",
            "optional": true,
            "default": "None",
            "description": "Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope typeand you expect the model to work on longer max_position_embeddings, we recommend you to update this valueaccordingly.Expected contents:rope_type (str):The sub-variant of RoPE to use. Can be one of [default, linear, dynamic, yarn, longrope,llama3], with default being the original RoPE implementation.factor (float, optional):Used with all rope types except default. The scaling factor to apply to the RoPE embeddings. Inmost scaling types, a factor of x will enable the model to handle sequences of length x original maximum pre-trained length.original_max_position_embeddings (int, optional):Used with dynamic, longrope and llama3. The original max position embeddings used duringpretraining.attention_factor (float, optional):Used with yarn and longrope. The scaling factor to be applied on the attentioncomputation. If unspecified, it defaults to value recommended by the implementation, using thefactor field to infer the suggested value.beta_fast (float, optional):Only used with yarn. Parameter to set the boundary for extrapolation (only) in the linearramp function. If unspecified, it defaults to 32.beta_slow (float, optional):Only used with yarn. Parameter to set the boundary for interpolation (only) in the linearramp function. If unspecified, it defaults to 1.short_factor (List[float], optional):Only used with longrope. The scaling factor to be applied to short contexts (<original_max_position_embeddings). Must be a list of numbers with the same length as the hiddensize divided by the number of attention heads divided by 2long_factor (List[float], optional):Only used with longrope. The scaling factor to be applied to long contexts (<original_max_position_embeddings). Must be a list of numbers with the same length as the hiddensize divided by the number of attention heads divided by 2low_freq_factor (float, optional):Only used with llama3. Scaling factor applied to low frequency components of the RoPEhigh_freq_factor (float, optional*):Only used with llama3. Scaling factor applied to high frequency components of the RoPE"
        },
        {
            "name": "bos_token_id",
            "type": "int",
            "optional": true,
            "default": "11",
            "description": "The id of the beginning-of-sequence token."
        },
        {
            "name": "eos_token_id",
            "type": "int",
            "optional": true,
            "default": "11",
            "description": "The id of the end-of-sequence token."
        },
        {
            "name": "ffn_hidden_size",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "The hidden size of the feedforward layer in the Transformer decoder.defaults to 4x hidden dim"
        },
        {
            "name": "activation",
            "type": "str",
            "optional": true,
            "default": "gelu",
            "description": "The activation function used in the feedforward layer."
        }
    ],
    "return": ""
}