{
    "api": "transformers.FSMTConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "langs",
        "'de']",
        "src_vocab_size",
        "tgt_vocab_size",
        "activation_function",
        "d_model",
        "max_length",
        "max_position_embeddings",
        "encoder_ffn_dim",
        "encoder_layers",
        "encoder_attention_heads",
        "encoder_layerdrop",
        "decoder_ffn_dim",
        "decoder_layers",
        "decoder_attention_heads",
        "decoder_layerdrop",
        "attention_dropout",
        "dropout",
        "activation_dropout",
        "init_std",
        "decoder_start_token_id",
        "is_encoder_decoder",
        "scale_embedding",
        "tie_word_embeddings",
        "num_beams",
        "length_penalty",
        "early_stopping",
        "use_cache",
        "pad_token_id",
        "bos_token_id",
        "eos_token_id",
        "forced_eos_token_id",
        "**common_kwargs"
    ],
    "params": [
        {
            "name": "langs",
            "type": "List[str]",
            "optional": false,
            "default": "[en,",
            "description": "A list with source language and target_language (e.g., [en, ru])."
        },
        {
            "name": "src_vocab_size",
            "type": "int",
            "optional": false,
            "default": "42024",
            "description": "Vocabulary size of the encoder. Defines the number of different tokens that can be represented by theinputs_ids passed to the forward method in the encoder."
        },
        {
            "name": "tgt_vocab_size",
            "type": "int",
            "optional": false,
            "default": "42024",
            "description": "Vocabulary size of the decoder. Defines the number of different tokens that can be represented by theinputs_ids passed to the forward method in the decoder."
        },
        {
            "name": "d_model",
            "type": "int",
            "optional": true,
            "default": "1024",
            "description": "Dimensionality of the layers and the pooler layer."
        },
        {
            "name": "encoder_layers",
            "type": "int",
            "optional": true,
            "default": "12",
            "description": "Number of encoder layers."
        },
        {
            "name": "decoder_layers",
            "type": "int",
            "optional": true,
            "default": "12",
            "description": "Number of decoder layers."
        },
        {
            "name": "encoder_attention_heads",
            "type": "int",
            "optional": true,
            "default": "16",
            "description": "Number of attention heads for each attention layer in the Transformer encoder."
        },
        {
            "name": "decoder_attention_heads",
            "type": "int",
            "optional": true,
            "default": "16",
            "description": "Number of attention heads for each attention layer in the Transformer decoder."
        },
        {
            "name": "decoder_ffn_dim",
            "type": "int",
            "optional": true,
            "default": "4096",
            "description": "Dimensionality of the intermediate (often named feed-forward) layer in decoder."
        },
        {
            "name": "encoder_ffn_dim",
            "type": "int",
            "optional": true,
            "default": "4096",
            "description": "Dimensionality of the intermediate (often named feed-forward) layer in decoder."
        },
        {
            "name": "activation_function",
            "type": "str,Callable",
            "optional": true,
            "default": "relu",
            "description": "The non-linear activation function (function or string) in the encoder and pooler. If string, gelu,relu, silu and gelu_new are supported."
        },
        {
            "name": "dropout",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout probability for all fully connected layers in the embeddings, encoder, and pooler."
        },
        {
            "name": "attention_dropout",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "The dropout ratio for the attention probabilities."
        },
        {
            "name": "activation_dropout",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "The dropout ratio for activations inside the fully connected layer."
        },
        {
            "name": "max_position_embeddings",
            "type": "int",
            "optional": true,
            "default": "1024",
            "description": "The maximum sequence length that this model might ever be used with. Typically set this to something largejust in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "name": "init_std",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "scale_embedding",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Scale embeddings by diving by sqrt(d_model)."
        },
        {
            "name": "bos_token_id",
            "type": "int",
            "optional": true,
            "default": "0",
            "description": "Beginning of stream token id."
        },
        {
            "name": "pad_token_id",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Padding token id."
        },
        {
            "name": "eos_token_id",
            "type": "int",
            "optional": true,
            "default": "2",
            "description": "End of stream token id."
        },
        {
            "name": "decoder_start_token_id",
            "type": "int",
            "optional": true,
            "default": "2",
            "description": "This model starts decoding with eos_token_id"
        },
        {
            "name": "encoder_layerdrop",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "Google layerdrop arxiv, as its not explainable in one line."
        },
        {
            "name": "decoder_layerdrop",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "Google layerdrop arxiv, as its not explainable in one line."
        },
        {
            "name": "is_encoder_decoder",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether this is an encoder/decoder model."
        },
        {
            "name": "tie_word_embeddings",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to tie input and output embeddings."
        },
        {
            "name": "num_beams",
            "type": "int",
            "optional": true,
            "default": "5",
            "description": "Number of beams for beam search that will be used by default in the generate method of the model. 1 meansno beam search."
        },
        {
            "name": "length_penalty",
            "type": "float",
            "optional": true,
            "default": "1.0",
            "description": "Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent tothe sequence length, which in turn is used to divide the score of the sequence. Since the score is the loglikelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, whilelength_penalty < 0.0 encourages shorter sequences."
        },
        {
            "name": "early_stopping",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Flag that will be used by default in the generate method of the model. Whether to stop the beam searchwhen at least num_beams sentences are finished per batch or not."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not the model should return the last key/values attentions (not used by all models)."
        },
        {
            "name": "forced_eos_token_id",
            "type": "int",
            "optional": true,
            "default": "2",
            "description": "The id of the token to force as the last generated token when max_length is reached. Usually set toeos_token_id."
        }
    ],
    "return": ""
}