{
    "api": "transformers.Siglip2VisionConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "hidden_size",
        "intermediate_size",
        "num_hidden_layers",
        "num_attention_heads",
        "num_channels",
        "num_patches",
        "patch_size",
        "hidden_act",
        "layer_norm_eps",
        "attention_dropout",
        "**kwargs"
    ],
    "params": [
        {
            "name": "hidden_size",
            "type": "int",
            "optional": true,
            "default": "768",
            "description": "Dimensionality of the encoder layers and the pooler layer."
        },
        {
            "name": "intermediate_size",
            "type": "int",
            "optional": true,
            "default": "3072",
            "description": "Dimensionality of the intermediate (i.e., feed-forward) layer in the Transformer encoder."
        },
        {
            "name": "num_hidden_layers",
            "type": "int",
            "optional": true,
            "default": "12",
            "description": "Number of hidden layers in the Transformer encoder."
        },
        {
            "name": "num_attention_heads",
            "type": "int",
            "optional": true,
            "default": "12",
            "description": "Number of attention heads for each attention layer in the Transformer encoder."
        },
        {
            "name": "num_channels",
            "type": "int",
            "optional": true,
            "default": "3",
            "description": "Number of channels in the input images."
        },
        {
            "name": "num_patches",
            "type": "int",
            "optional": true,
            "default": "256",
            "description": "The number of patches in the image with the size of (patch_size, patch_size).The image is resized to fill maximum of this number of patches, and to preservethe aspect ratio. In case the resulted number of patches is lower, the image ispadded in patch dimension."
        },
        {
            "name": "patch_size",
            "type": "int",
            "optional": true,
            "default": "16",
            "description": "The size (resolution) of each patch."
        },
        {
            "name": "hidden_act",
            "type": "str,function",
            "optional": true,
            "default": "gelu_pytorch_tanh",
            "description": "The non-linear activation function (function or string) in the encoder and pooler. If string, gelu,relu, selu and gelu_new quick_gelu are supported."
        },
        {
            "name": "layer_norm_eps",
            "type": "float",
            "optional": true,
            "default": "1e-06",
            "description": "The epsilon used by the layer normalization layers."
        },
        {
            "name": "attention_dropout",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "The dropout ratio for the attention probabilities."
        }
    ],
    "return": ""
}