{
    "api": "transformers.Qwen2MoeConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "vocab_size",
        "hidden_size",
        "intermediate_size",
        "num_hidden_layers",
        "num_attention_heads",
        "num_key_value_heads",
        "hidden_act",
        "max_position_embeddings",
        "initializer_range",
        "rms_norm_eps",
        "use_cache",
        "tie_word_embeddings",
        "rope_theta",
        "rope_scaling",
        "use_sliding_window",
        "sliding_window",
        "max_window_layers",
        "attention_dropout",
        "decoder_sparse_step",
        "moe_intermediate_size",
        "shared_expert_intermediate_size",
        "num_experts_per_tok",
        "num_experts",
        "norm_topk_prob",
        "output_router_logits",
        "router_aux_loss_coef",
        "mlp_only_layers",
        "qkv_bias",
        "**kwargs"
    ],
    "params": [
        {
            "name": "vocab_size",
            "type": "int",
            "optional": true,
            "default": "151936",
            "description": "Vocabulary size of the Qwen2MoE model. Defines the number of different tokens that can be represented by theinputs_ids passed when calling Qwen2MoeModel"
        },
        {
            "name": "hidden_size",
            "type": "int",
            "optional": true,
            "default": "2048",
            "description": "Dimension of the hidden representations."
        },
        {
            "name": "intermediate_size",
            "type": "int",
            "optional": true,
            "default": "5632",
            "description": "Dimension of the MLP representations."
        },
        {
            "name": "num_hidden_layers",
            "type": "int",
            "optional": true,
            "default": "24",
            "description": "Number of hidden layers in the Transformer encoder."
        },
        {
            "name": "num_attention_heads",
            "type": "int",
            "optional": true,
            "default": "16",
            "description": "Number of attention heads for each attention layer in the Transformer encoder."
        },
        {
            "name": "num_key_value_heads",
            "type": "int",
            "optional": true,
            "default": "16",
            "description": "This is the number of key_value heads that should be used to implement Grouped Query Attention. Ifnum_key_value_heads=num_attention_heads, the model will use Multi Head Attention (MHA), ifnum_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. Whenconverting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructedby meanpooling all the original heads within that group. For more details checkout thispaper. If it is not specified, will default to 32."
        },
        {
            "name": "hidden_act",
            "type": "str,function",
            "optional": true,
            "default": "silu",
            "description": "The non-linear activation function (function or string) in the decoder."
        },
        {
            "name": "max_position_embeddings",
            "type": "int",
            "optional": true,
            "default": "32768",
            "description": "The maximum sequence length that this model might ever be used with."
        },
        {
            "name": "initializer_range",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "rms_norm_eps",
            "type": "float",
            "optional": true,
            "default": "1e-06",
            "description": "The epsilon used by the rms normalization layers."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not the model should return the last key/values attentions (not used by all models). Onlyrelevant if config.is_decoder=True."
        },
        {
            "name": "tie_word_embeddings",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether the models input and output word embeddings should be tied."
        },
        {
            "name": "rope_theta",
            "type": "float",
            "optional": true,
            "default": "10000.0",
            "description": "The base period of the RoPE embeddings."
        },
        {
            "name": "rope_scaling",
            "type": "Dict",
            "optional": true,
            "default": "None",
            "description": "Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope typeand you expect the model to work on longer max_position_embeddings, we recommend you to update this valueaccordingly.Expected contents:rope_type (str):The sub-variant of RoPE to use. Can be one of [default, linear, dynamic, yarn, longrope,llama3], with default being the original RoPE implementation.factor (float, optional):Used with all rope types except default. The scaling factor to apply to the RoPE embeddings. Inmost scaling types, a factor of x will enable the model to handle sequences of length x original maximum pre-trained length.original_max_position_embeddings (int, optional):Used with dynamic, longrope and llama3. The original max position embeddings used duringpretraining.attention_factor (float, optional):Used with yarn and longrope. The scaling factor to be applied on the attentioncomputation. If unspecified, it defaults to value recommended by the implementation, using thefactor field to infer the suggested value.beta_fast (float, optional):Only used with yarn. Parameter to set the boundary for extrapolation (only) in the linearramp function. If unspecified, it defaults to 32.beta_slow (float, optional):Only used with yarn. Parameter to set the boundary for interpolation (only) in the linearramp function. If unspecified, it defaults to 1.short_factor (List[float], optional):Only used with longrope. The scaling factor to be applied to short contexts (<original_max_position_embeddings). Must be a list of numbers with the same length as the hiddensize divided by the number of attention heads divided by 2long_factor (List[float], optional):Only used with longrope. The scaling factor to be applied to long contexts (<original_max_position_embeddings). Must be a list of numbers with the same length as the hiddensize divided by the number of attention heads divided by 2low_freq_factor (float, optional):Only used with llama3. Scaling factor applied to low frequency components of the RoPEhigh_freq_factor (float, optional*):Only used with llama3. Scaling factor applied to high frequency components of the RoPE"
        },
        {
            "name": "use_sliding_window",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to use sliding window attention."
        },
        {
            "name": "sliding_window",
            "type": "int",
            "optional": true,
            "default": "4096",
            "description": "Sliding window attention (SWA) window size. If not specified, will default to 4096."
        },
        {
            "name": "max_window_layers",
            "type": "int",
            "optional": true,
            "default": "28",
            "description": "The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention."
        },
        {
            "name": "attention_dropout",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "The dropout ratio for the attention probabilities."
        },
        {
            "name": "decoder_sparse_step",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "The frequency of the MoE layer."
        },
        {
            "name": "moe_intermediate_size",
            "type": "int",
            "optional": true,
            "default": "1408",
            "description": "Intermediate size of the routed expert."
        },
        {
            "name": "shared_expert_intermediate_size",
            "type": "int",
            "optional": true,
            "default": "5632",
            "description": "Intermediate size of the shared expert."
        },
        {
            "name": "num_experts_per_tok",
            "type": "int",
            "optional": true,
            "default": "4",
            "description": "Number of selected experts."
        },
        {
            "name": "num_experts",
            "type": "int",
            "optional": true,
            "default": "60",
            "description": "Number of routed experts."
        },
        {
            "name": "norm_topk_prob",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to normalize the topk probabilities."
        },
        {
            "name": "output_router_logits",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether or not the router logits should be returned by the model. Enabling this will alsoallow the model to output the auxiliary loss, including load balancing loss and router z-loss."
        },
        {
            "name": "router_aux_loss_coef",
            "type": "float",
            "optional": true,
            "default": "0.001",
            "description": "The aux loss factor for the total loss."
        },
        {
            "name": "mlp_only_layers",
            "type": "List[int]",
            "optional": true,
            "default": "None",
            "description": "Indicate which layers use Qwen2MoeMLP rather than Qwen2MoeSparseMoeBlockThe list contains layer index, from 0 to num_layers-1 if we have num_layers layersIf mlp_only_layers is empty, decoder_sparse_step is used to determine the sparsity."
        },
        {
            "name": "qkv_bias",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether to add a bias to the queries, keys and values."
        }
    ],
    "return": ""
}