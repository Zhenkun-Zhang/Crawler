{
    "api": "transformers.TFGPT2DoubleHeadsModel.call",
    "type": "function",
    "version": "main",
    "args_list": [
        "input_ids:",
        "TFModelInputType",
        "|",
        "past_key_values",
        "tf.Tensor]]]]",
        "attention_mask",
        "|",
        "tf.Tensor",
        "|",
        "token_type_ids",
        "|",
        "tf.Tensor",
        "|",
        "position_ids",
        "|",
        "tf.Tensor",
        "|",
        "head_mask",
        "|",
        "tf.Tensor",
        "|",
        "inputs_embeds",
        "|",
        "tf.Tensor",
        "|",
        "mc_token_ids",
        "|",
        "tf.Tensor",
        "|",
        "use_cache",
        "output_attentions",
        "output_hidden_states",
        "return_dict",
        "training",
        ")"
    ],
    "params": [
        {
            "name": "input_ids",
            "type": "Numpy array,tf.Tensor of shape (batch_size, input_ids_length)",
            "optional": false,
            "default": "",
            "description": "input_ids_length = sequence_length if past_key_values is None else past_key_values[0].shape[-2](sequence_length of input past key value states). Indices of input sequence tokens in the vocabulary.If past_key_values is used, only input IDs that do not have their past calculated should be passed asinput_ids.Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.call() andPreTrainedTokenizer.encode() for details.What are input IDs?"
        },
        {
            "name": "past_key_values",
            "type": "List[tf.Tensor] of length config.n_layers",
            "optional": false,
            "default": "",
            "description": "Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model (seepast_key_values output below). Can be used to speed up sequential decoding. The token ids which havetheir past given to this model should not be passed as input ids as they have already been computed."
        },
        {
            "name": "attention_mask",
            "type": "tf.Tensor,Numpy array of shape (batch_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:1 for tokens that are not masked,0 for tokens that are masked.If past_key_values is used, attention_mask needs to contain the masking strategy that was used forpast_key_values. In other words, the attention_mask always has to have the length:len(past_key_values) + len(input_ids)What are attention masks?"
        },
        {
            "name": "token_type_ids",
            "type": "tf.Tensor,Numpy array of shape (batch_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]:0 corresponds to a sentence A token,1 corresponds to a sentence B token.What are token type IDs?"
        },
        {
            "name": "position_ids",
            "type": "tf.Tensor,Numpy array of shape (batch_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "Indices of positions of each input sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].What are position IDs?"
        },
        {
            "name": "head_mask",
            "type": "Numpy array,tf.Tensor of shape (num_heads,),(num_layers, num_heads",
            "optional": true,
            "default": "",
            "description": "Mask to nullify selected heads of the self-attention modules. Mask values selected in [0, 1]:1 indicates the head is not masked,0 indicates the head is masked."
        },
        {
            "name": "inputs_embeds",
            "type": "tf.Tensor of shape (batch_size, sequence_length, hidden_size",
            "optional": true,
            "default": "",
            "description": "Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. Thisis useful if you want more control over how to convert input_ids indices into associated vectors than themodels internal embedding lookup matrix."
        },
        {
            "name": "output_attentions",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the attentions tensors of all attention layers. See attentions under returnedtensors for more detail. This argument can be used only in eager mode, in graph mode the value in theconfig will be used instead."
        },
        {
            "name": "output_hidden_states",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the hidden states of all layers. See hidden_states under returned tensors formore detail. This argument can be used only in eager mode, in graph mode the value in the config will beused instead."
        },
        {
            "name": "return_dict",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return a ModelOutput instead of a plain tuple. This argument can be used ineager mode, in graph mode the value will always be set to True."
        },
        {
            "name": "training",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to use the model in training mode (some modules like dropout modules have differentbehaviors between training and evaluation)."
        },
        {
            "name": "mc_token_ids",
            "type": "tf.Tensor,Numpy array of shape (batch_size, num_choices",
            "optional": true,
            "default": "",
            "description": "Index of the classification token in each input sequence. Selected in the range [0, input_ids.size(-1) - 1]."
        }
    ],
    "return": "transformers.models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModelOutput or tuple(tf.Tensor)"
}