{
    "api": "transformers.MCTCTConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "vocab_size",
        "hidden_size",
        "num_hidden_layers",
        "intermediate_size",
        "num_attention_heads",
        "attention_head_dim",
        "max_position_embeddings",
        "layer_norm_eps",
        "layerdrop",
        "hidden_act",
        "initializer_range",
        "hidden_dropout_prob",
        "attention_probs_dropout_prob",
        "pad_token_id",
        "bos_token_id",
        "eos_token_id",
        "conv_glu_dim",
        "conv_dropout",
        "num_conv_layers",
        "conv_kernel",
        "conv_stride",
        "input_feat_per_channel",
        "input_channels",
        "conv_channels",
        "ctc_loss_reduction",
        "ctc_zero_infinity",
        "**kwargs"
    ],
    "params": [
        {
            "name": "vocab_size",
            "type": "int",
            "optional": true,
            "default": "8065",
            "description": "Vocabulary size of the M-CTC-T model. Defines the number of different tokens that can be represented by theinputs_ids passed when calling MCTCTModel."
        },
        {
            "name": "hidden_size",
            "type": "int",
            "optional": true,
            "default": "1536",
            "description": "Dimension of the encoder layers and the pooler layer."
        },
        {
            "name": "num_hidden_layers",
            "type": "int",
            "optional": true,
            "default": "36",
            "description": "Number of hidden layers in the Transformer encoder."
        },
        {
            "name": "intermediate_size",
            "type": "int",
            "optional": true,
            "default": "6144",
            "description": "Dimension of the intermediate (i.e., feed-forward) layer in the Transformer encoder."
        },
        {
            "name": "num_attention_heads",
            "type": "int",
            "optional": true,
            "default": "4",
            "description": "Number of attention heads for each attention layer in the Transformer encoder."
        },
        {
            "name": "attention_head_dim",
            "type": "int",
            "optional": true,
            "default": "384",
            "description": "Dimensions of each attention head for each attention layer in the Transformer encoder."
        },
        {
            "name": "max_position_embeddings",
            "type": "int",
            "optional": true,
            "default": "920",
            "description": "The maximum sequence length that this model might ever be used with (after log-mel spectrogram extraction)."
        },
        {
            "name": "layer_norm_eps",
            "type": "float",
            "optional": true,
            "default": "1e-05",
            "description": "The epsilon used by the layer normalization layers."
        },
        {
            "name": "layerdrop",
            "type": "float",
            "optional": true,
            "default": "0.3",
            "description": "The probability of dropping an encoder layer during training. The default 0.3 value is used in the originalimplementation."
        },
        {
            "name": "hidden_act",
            "type": "str,function",
            "optional": true,
            "default": "relu",
            "description": "The non-linear activation function (function or string) in the encoder and pooler. If string, gelu,relu, selu and gelu_new are supported."
        },
        {
            "name": "initializer_range",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "hidden_dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.3",
            "description": "The dropout probability for all fully connected layers in the embeddings, encoder, and pooler."
        },
        {
            "name": "attention_probs_dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.3",
            "description": "The dropout ratio for the attention probabilities."
        },
        {
            "name": "pad_token_id",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "The tokenizer index of the pad token."
        },
        {
            "name": "bos_token_id",
            "type": "int",
            "optional": true,
            "default": "0",
            "description": "The tokenizer index of the bos token."
        },
        {
            "name": "eos_token_id",
            "type": "int",
            "optional": true,
            "default": "2",
            "description": "The tokenizer index of the eos token."
        },
        {
            "name": "conv_glu_dim",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "The dimension of the output of the Conv1dSubsampler layer in which GLU is applied on. Though the originalFlashlight code uses the value of 2, here its adapted to 1 due to transposition differences."
        },
        {
            "name": "conv_dropout",
            "type": "int",
            "optional": true,
            "default": "0.3",
            "description": "The probability of randomly dropping the Conv1dSubsampler layer during training."
        },
        {
            "name": "num_conv_layers",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Number of convolution layers before applying transformer encoder layers."
        },
        {
            "name": "conv_kernel",
            "type": "Sequence[int]",
            "optional": true,
            "default": "(7,)",
            "description": "The kernel size of the 1D convolution applied before transformer layers. len(conv_kernel) must be equalto num_conv_layers."
        },
        {
            "name": "conv_stride",
            "type": "Sequence[int]",
            "optional": true,
            "default": "(3,)",
            "description": "The stride length of the 1D convolution applied before transformer layers. len(conv_stride) must be equalto num_conv_layers."
        },
        {
            "name": "input_feat_per_channel",
            "type": "int",
            "optional": true,
            "default": "80",
            "description": "Feature dimensions of the channels of the input to the Conv1D layer."
        },
        {
            "name": "input_channels",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Number of input channels of the input to the Conv1D layer."
        },
        {
            "name": "conv_channels",
            "type": "List[int]",
            "optional": true,
            "default": "None",
            "description": "Channel sizes of intermediate Conv1D layers."
        },
        {
            "name": "ctc_loss_reduction",
            "type": "str",
            "optional": true,
            "default": "sum",
            "description": "Specifies the reduction to apply to the output of torch.nn.CTCLoss. Only relevant when training aninstance of MCTCTForCTC."
        },
        {
            "name": "ctc_zero_infinity",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to zero infinite losses and the associated gradients of torch.nn.CTCLoss. Infinite losses mainlyoccur when the inputs are too short to be aligned to the targets. Only relevant when training an instanceof MCTCTForCTC."
        }
    ],
    "return": ""
}