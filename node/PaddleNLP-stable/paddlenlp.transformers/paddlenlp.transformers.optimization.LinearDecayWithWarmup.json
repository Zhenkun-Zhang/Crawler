{
    "api": "paddlenlp.transformers.optimization.LinearDecayWithWarmup",
    "type": "class",
    "version": "stable",
    "args_list": {
        "learning_rate": null,
        "total_steps": null,
        "warmup": null,
        "last_epoch": "-1",
        "verbose": "False"
    },
    "Bases": "LambdaDecay",
    "description": "Creates a learning rate scheduler, which increases learning rate linearlyfrom 0 to given learning_rate, after this warmup period learning ratewould be decreased linearly from the base learning rate to 0.",
    "params": [
        {
            "name": "learning_rate",
            "type": "float",
            "description": "The base learning rate. It is a python float number.",
            "default": "",
            "optional": false
        },
        {
            "name": "total_steps",
            "type": "int",
            "description": "The number of training steps.",
            "default": "",
            "optional": false
        },
        {
            "name": "warmup",
            "type": "int or float",
            "description": "If int, it means the number of steps for warmup. If float, it meansthe proportion of warmup in total training steps.",
            "default": "",
            "optional": false
        },
        {
            "name": "last_epoch",
            "type": "int",
            "description": "The index of last epoch. It can be set to restart training. IfNone, it means initial learning rate.Defaults to -1.",
            "default": "-1",
            "optional": true
        },
        {
            "name": "verbose",
            "type": "bool",
            "description": "If True, prints a message to stdout for each update.Defaults to False.",
            "default": "False",
            "optional": true
        }
    ],
    "return": {
        "description": "",
        "type": ""
    }
}