{
    "api": "transformers.InformerConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "prediction_length:",
        "context_length",
        "distribution_output",
        "loss",
        "input_size",
        "lags_sequence",
        "scaling",
        "bool,",
        "NoneType]",
        "num_dynamic_real_features",
        "num_static_real_features",
        "num_static_categorical_features",
        "num_time_features",
        "cardinality",
        "embedding_dimension",
        "d_model",
        "encoder_ffn_dim",
        "decoder_ffn_dim",
        "encoder_attention_heads",
        "decoder_attention_heads",
        "encoder_layers",
        "decoder_layers",
        "is_encoder_decoder",
        "activation_function",
        "dropout",
        "encoder_layerdrop",
        "decoder_layerdrop",
        "attention_dropout",
        "activation_dropout",
        "num_parallel_samples",
        "init_std",
        "use_cache",
        "attention_type",
        "sampling_factor",
        "distil",
        "**kwargs"
    ],
    "params": [
        {
            "name": "prediction_length",
            "type": "int",
            "optional": false,
            "default": "",
            "description": "The prediction length for the decoder. In other words, the prediction horizon of the model. This value istypically dictated by the dataset and we recommend to set it appropriately."
        },
        {
            "name": "context_length",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The context length for the encoder. If None, the context length will be the same as theprediction_length."
        },
        {
            "name": "distribution_output",
            "type": "string",
            "optional": true,
            "default": "",
            "description": "The distribution emission head for the model. Could be either student_t, normal or negative_binomial."
        },
        {
            "name": "loss",
            "type": "string",
            "optional": true,
            "default": "",
            "description": "The loss function for the model corresponding to the distribution_output head. For parametricdistributions it is the negative log likelihood (nll) - which currently is the only supported one."
        },
        {
            "name": "input_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The size of the target variable which by default is 1 for univariate targets. Would be > 1 in case ofmultivariate targets."
        },
        {
            "name": "scaling",
            "type": "string,bool",
            "optional": true,
            "default": "",
            "description": "Whether to scale the input targets via mean scaler, std scaler or no scaler if None. If True, thescaler is set to mean."
        },
        {
            "name": "lags_sequence",
            "type": "list[int]",
            "optional": true,
            "default": "",
            "description": "The lags of the input time series as covariates often dictated by the frequency of the data. Default is[1, 2, 3, 4, 5, 6, 7] but we recommend to change it based on the dataset appropriately."
        },
        {
            "name": "num_time_features",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The number of time features in the input time series."
        },
        {
            "name": "num_dynamic_real_features",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The number of dynamic real valued features."
        },
        {
            "name": "num_static_categorical_features",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The number of static categorical features."
        },
        {
            "name": "num_static_real_features",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The number of static real valued features."
        },
        {
            "name": "cardinality",
            "type": "list[int]",
            "optional": true,
            "default": "",
            "description": "The cardinality (number of different values) for each of the static categorical features. Should be a listof integers, having the same length as num_static_categorical_features. Cannot be None ifnum_static_categorical_features is > 0."
        },
        {
            "name": "embedding_dimension",
            "type": "list[int]",
            "optional": true,
            "default": "",
            "description": "The dimension of the embedding for each of the static categorical features. Should be a list of integers,having the same length as num_static_categorical_features. Cannot be None ifnum_static_categorical_features is > 0."
        },
        {
            "name": "d_model",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Dimensionality of the transformer layers."
        },
        {
            "name": "encoder_layers",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of encoder layers."
        },
        {
            "name": "decoder_layers",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of decoder layers."
        },
        {
            "name": "encoder_attention_heads",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of attention heads for each attention layer in the Transformer encoder."
        },
        {
            "name": "decoder_attention_heads",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of attention heads for each attention layer in the Transformer decoder."
        },
        {
            "name": "encoder_ffn_dim",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Dimension of the intermediate (often named feed-forward) layer in encoder."
        },
        {
            "name": "decoder_ffn_dim",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Dimension of the intermediate (often named feed-forward) layer in decoder."
        },
        {
            "name": "activation_function",
            "type": "str,function",
            "optional": true,
            "default": "",
            "description": "The non-linear activation function (function or string) in the encoder and decoder. If string, gelu andrelu are supported."
        },
        {
            "name": "dropout",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The dropout probability for all fully connected layers in the encoder, and decoder."
        },
        {
            "name": "encoder_layerdrop",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The dropout probability for the attention and fully connected layers for each encoder layer."
        },
        {
            "name": "decoder_layerdrop",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The dropout probability for the attention and fully connected layers for each decoder layer."
        },
        {
            "name": "attention_dropout",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The dropout probability for the attention probabilities."
        },
        {
            "name": "activation_dropout",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The dropout probability used between the two layers of the feed-forward networks."
        },
        {
            "name": "num_parallel_samples",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The number of samples to generate in parallel for each time step of inference."
        },
        {
            "name": "init_std",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The standard deviation of the truncated normal weight initialization distribution."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether to use the past key/values attentions (if applicable to the model) to speed up decoding."
        },
        {
            "name": "attention_type",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Attention used in encoder. This can be set to prob (Informers ProbAttention) or full (vanillatransformers canonical self-attention)."
        },
        {
            "name": "sampling_factor",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "ProbSparse sampling factor (only makes affect when attention_type=prob). It is used to control thereduced query matrix (Q_reduce) input length."
        },
        {
            "name": "distil",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to use distilling in encoder."
        }
    ],
    "return": ""
}