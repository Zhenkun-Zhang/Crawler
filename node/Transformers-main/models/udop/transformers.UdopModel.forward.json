{
    "api": "transformers.UdopModel.forward",
    "type": "function",
    "version": "main",
    "args_list": [
        "input_ids:",
        "attention_mask",
        "bbox",
        "typing.Any]",
        "pixel_values",
        "visual_bbox",
        "typing.Any]",
        "decoder_input_ids",
        "decoder_attention_mask",
        "inputs_embeds",
        "encoder_outputs",
        "past_key_values",
        "head_mask",
        "decoder_inputs_embeds",
        "decoder_head_mask",
        "cross_attn_head_mask",
        "use_cache",
        "output_attentions",
        "output_hidden_states",
        "return_dict",
        "cache_position",
        ")"
    ],
    "params": [
        {
            "name": "input_ids",
            "type": "torch.LongTensor of shape (batch_size, sequence_length)",
            "optional": false,
            "default": "",
            "description": "Indices of input sequence tokens in the vocabulary. UDOP is a model with relative position embeddings soyou should be able to pad the inputs on both the right and the left. Indices can be obtained usingAutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call() for detail.What are input IDs?"
        },
        {
            "name": "attention_mask",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:1 for tokens that are not masked,0 for tokens that are masked.What are attention masks?"
        },
        {
            "name": "bbox",
            "type": "torch.LongTensor of shape ({0}, 4",
            "optional": true,
            "default": "",
            "description": "Bounding boxes of each input sequence tokens. Selected in the range [0, config.max_2d_position_embeddings-1]. Each bounding box should be a normalized version in (x0, y0, x1, y1)format, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1,y1) represents the position of the lower right corner.Note that sequence_length = token_sequence_length + patch_sequence_length + 1 where 1 is for [CLS]token. See pixel_values for patch_sequence_length."
        },
        {
            "name": "pixel_values",
            "type": "torch.FloatTensor of shape (batch_size, num_channels, height, width)",
            "optional": false,
            "default": "",
            "description": "Batch of document images. Each image is divided into patches of shape (num_channels, config.patch_size, config.patch_size) and the total number of patches (=patch_sequence_length) equals to ((height / config.patch_size) * (width / config.patch_size))."
        },
        {
            "name": "visual_bbox",
            "type": "torch.LongTensor of shape (batch_size, patch_sequence_length, 4",
            "optional": true,
            "default": "",
            "description": "Bounding boxes of each patch in the image. If not provided, bounding boxes are created in the model."
        },
        {
            "name": "decoder_input_ids",
            "type": "torch.LongTensor of shape (batch_size, target_sequence_length",
            "optional": true,
            "default": "",
            "description": "Indices of decoder input sequence tokens in the vocabulary. Indices can be obtained usingAutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call() for details.What are decoder input IDs? T5 uses the pad_token_id as the startingtoken for decoder_input_ids generation. If past_key_values is used, optionally only the lastdecoder_input_ids have to be input (see past_key_values). To know more on how to preparedecoder_input_ids for pretraining take a look at T5 Training."
        },
        {
            "name": "decoder_attention_mask",
            "type": "torch.BoolTensor of shape (batch_size, target_sequence_length",
            "optional": true,
            "default": "",
            "description": "Default behavior: generate a tensor that ignores pad tokens in decoder_input_ids. Causal mask will alsobe used by default."
        },
        {
            "name": "head_mask",
            "type": "torch.FloatTensor of shape (num_heads,),(num_layers, num_heads",
            "optional": true,
            "default": "",
            "description": "Mask to nullify selected heads of the self-attention modules in the encoder. Mask values selected in [0, 1]:1 indicates the head is not masked,0 indicates the head is masked."
        },
        {
            "name": "decoder_head_mask",
            "type": "torch.FloatTensor of shape (num_heads,),(num_layers, num_heads",
            "optional": true,
            "default": "",
            "description": "Mask to nullify selected heads of the self-attention modules in the decoder. Mask values selected in [0, 1]:1 indicates the head is not masked,0 indicates the head is masked."
        },
        {
            "name": "cross_attn_head_mask",
            "type": "torch.Tensor of shape (num_heads,),(num_layers, num_heads",
            "optional": true,
            "default": "",
            "description": "Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in[0, 1]:1 indicates the head is not masked,0 indicates the head is masked."
        },
        {
            "name": "encoder_outputs",
            "type": "tuple(tuple(torch.FloatTensor",
            "optional": true,
            "default": "",
            "description": "Tuple consists of (last_hidden_state, optional: hidden_states, optional: attentions)last_hidden_state of shape (batch_size, sequence_length, hidden_size) is a sequence of hidden states atthe output of the last layer of the encoder. Used in the cross-attention of the decoder."
        },
        {
            "name": "past_key_values",
            "type": "tuple(tuple(torch.FloatTensor)) of length config.n_layers with each tuple having 4 tensors of shape (batch_size, num_heads, sequence_length - 1, embed_size_per_head)",
            "optional": false,
            "default": "",
            "description": "Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.If past_key_values are used, the user can optionally input only the last decoder_input_ids (those thatdont have their past key value states given to this model) of shape (batch_size, 1) instead of alldecoder_input_ids of shape (batch_size, sequence_length)."
        },
        {
            "name": "inputs_embeds",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length, hidden_size",
            "optional": true,
            "default": "",
            "description": "Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. Thisis useful if you want more control over how to convert input_ids indices into associated vectors than themodels internal embedding lookup matrix."
        },
        {
            "name": "decoder_inputs_embeds",
            "type": "torch.FloatTensor of shape (batch_size, target_sequence_length, hidden_size",
            "optional": true,
            "default": "",
            "description": "Optionally, instead of passing decoder_input_ids you can choose to directly pass an embeddedrepresentation. If past_key_values is used, optionally only the last decoder_inputs_embeds have to beinput (see past_key_values). This is useful if you want more control over how to convertdecoder_input_ids indices into associated vectors than the models internal embedding lookup matrix. Ifdecoder_input_ids and decoder_inputs_embeds are both unset, decoder_inputs_embeds takes the value ofinputs_embeds."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "If set to True, past_key_values key value states are returned and can be used to speed up decoding (seepast_key_values)."
        },
        {
            "name": "output_attentions",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the attentions tensors of all attention layers. See attentions under returnedtensors for more detail."
        },
        {
            "name": "output_hidden_states",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the hidden states of all layers. See hidden_states under returned tensors formore detail."
        },
        {
            "name": "return_dict",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return a ModelOutput instead of a plain tuple."
        },
        {
            "name": "cache_position",
            "type": "torch.LongTensor of shape (sequence_length",
            "optional": true,
            "default": "",
            "description": "Indices depicting the position of the input sequence tokens in the sequence. It is used to update thecache in the correct position and to infer the complete sequence length."
        }
    ],
    "return": "transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)"
}