{
    "api": "transformers.PhiForCausalLM.generate",
    "type": "function",
    "version": "main",
    "args_list": [
        "inputs:",
        "generation_config",
        "logits_processor",
        "stopping_criteria",
        "prefix_allowed_tokens_fn",
        "torch.Tensor],",
        "typing.List[int]]]",
        "synced_gpus",
        "assistant_model",
        "streamer",
        "negative_prompt_ids",
        "negative_prompt_attention_mask",
        "use_model_defaults",
        "**kwargs",
        ")"
    ],
    "params": [
        {
            "name": "inputs",
            "type": "torch.Tensor of varying shape depending on the modality",
            "optional": true,
            "default": "",
            "description": "The sequence used as a prompt for the generation or as model inputs to the encoder. If None themethod initializes it with bos_token_id and a batch size of 1. For decoder-only models inputsshould be in the format of input_ids. For encoder-decoder models inputs can represent any ofinput_ids, input_values, input_features, or pixel_values."
        },
        {
            "name": "generation_config",
            "type": "GenerationConfig",
            "optional": true,
            "default": "",
            "description": "The generation configuration to be used as base parametrization for the generation call. **kwargspassed to generate matching the attributes of generation_config will override them. Ifgeneration_config is not provided, the default will be used, which has the following loadingpriority: 1) from the generation_config.json model file, if it exists, 2) from the modelconfiguration. Please note that unspecified parameters will inherit GenerationConfigsdefault values, whose documentation should be checked to parameterize generation."
        },
        {
            "name": "logits_processor",
            "type": "LogitsProcessorList",
            "optional": true,
            "default": "",
            "description": "Custom logits processors that complement the default logits processors built from arguments andgeneration config. If a logit processor is passed that is already created with the arguments or ageneration config an error is thrown. This feature is intended for advanced users."
        },
        {
            "name": "stopping_criteria",
            "type": "StoppingCriteriaList",
            "optional": true,
            "default": "",
            "description": "Custom stopping criteria that complements the default stopping criteria built from arguments and ageneration config. If a stopping criteria is passed that is already created with the arguments or ageneration config an error is thrown. If your stopping criteria depends on the scores input, makesure you pass return_dict_in_generate=True, output_scores=True to generate. This feature isintended for advanced users."
        },
        {
            "name": "prefix_allowed_tokens_fn",
            "type": "Callable[[int, torch.Tensor], List[int]]",
            "optional": true,
            "default": "",
            "description": "If provided, this function constraints the beam search to allowed tokens only at each step. If notprovided no constraint is applied. This function takes 2 arguments: the batch ID batch_id andinput_ids. It has to return a list with the allowed tokens for the next generation step conditionedon the batch ID batch_id and the previously generated tokens inputs_ids. This argument is usefulfor constrained generation conditioned on the prefix, as described in Autoregressive EntityRetrieval."
        },
        {
            "name": "synced_gpus",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to continue running the while loop until max_length. Unless overridden, this flag will be setto True if using FullyShardedDataParallel or DeepSpeed ZeRO Stage 3 with multiple GPUs to avoiddeadlocking if one GPU finishes generating before other GPUs. Otherwise, defaults to False."
        },
        {
            "name": "assistant_model",
            "type": "PreTrainedModel",
            "optional": true,
            "default": "",
            "description": "An assistant model that can be used to accelerate generation. The assistant model must have the exactsame tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistant modelis much faster than running generation with the model youre calling generate from. As such, theassistant model should be much smaller."
        },
        {
            "name": "streamer",
            "type": "BaseStreamer",
            "optional": true,
            "default": "",
            "description": "Streamer object that will be used to stream the generated sequences. Generated tokens are passedthrough streamer.put(token_ids) and the streamer is responsible for any further processing."
        },
        {
            "name": "negative_prompt_ids",
            "type": "torch.LongTensor of shape (batch_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "The negative prompt needed for some processors such as CFG. The batch size must match the input batchsize. This is an experimental feature, subject to breaking API changes in future versions."
        },
        {
            "name": "negative_prompt_attention_mask",
            "type": "torch.LongTensor of shape (batch_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "Attention_mask for negative_prompt_ids."
        },
        {
            "name": "use_model_defaults",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "When it is True, unset parameters in generation_config will be set to the model-specific defaultgeneration configuration (model.generation_config), as opposed to the global defaults(GenerationConfig()). If unset, models saved starting from v4.50 will consider this flag to beTrue."
        },
        {
            "name": "kwargs",
            "type": "Dict[str, Any]",
            "optional": true,
            "default": "",
            "description": "Ad hoc parametrization of generation_config and/or additional model-specific kwargs that will beforwarded to the forward function of the model. If the model is an encoder-decoder model, encoderspecific kwargs should not be prefixed and decoder specific kwargs should be prefixed with decoder_."
        }
    ],
    "return": "ModelOutput or torch.LongTensor"
}