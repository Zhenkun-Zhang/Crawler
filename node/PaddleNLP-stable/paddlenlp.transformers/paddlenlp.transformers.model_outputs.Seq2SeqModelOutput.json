{
    "api": "paddlenlp.transformers.model_outputs.Seq2SeqModelOutput",
    "type": "class",
    "version": "stable",
    "args_list": {
        "last_hidden_state": "Tensor or None = None",
        "past_key_values": "Tuple[Tuple[Tensor]] or None = None",
        "decoder_hidden_states": "Tuple[Tensor] or None = None",
        "decoder_attentions": "Tuple[Tensor] or None = None",
        "cross_attentions": "Tuple[Tensor] or None = None",
        "encoder_last_hidden_state": "Tensor or None = None",
        "encoder_hidden_states": "Tuple[Tensor] or None = None",
        "encoder_attentions": "Tuple[Tensor] or None = None"
    },
    "Bases": "",
    "description": "Base class for model encoderâ€™s outputs that also contains : pre-computed hidden states that can speed up sequentialdecoding.",
    "params": [
        {
            "name": "last_hidden_state",
            "type": "paddle.Tensor",
            "description": "Sequence of hidden-states at the output of the last layer of the decoder of the model, whose shape is (batch_size, Sequence_length, hidden_size).If past_key_values is used only the last hidden-state of the sequences of shape (batch_size, 1,hidden_size) is output.Sequence of hidden-states at the output of the last layer of the decoder of the model, whose shape is (batch_size, Sequence_length, hidden_size).",
            "default": "",
            "optional": false
        },
        {
            "name": "past_key_values",
            "type": "tupletuplepaddle.Tensor",
            "description": "Tuple of tuple(paddle.Tensor) of length config.n_layers, with each tuple having 2 tensors of shape(batch_size, num_heads, sequence_length, embed_size_per_head)) and 2 additional tensors of shape(batch_size, num_heads, encoder_sequence_length, embed_size_per_head).Returned when use_cache=True is passed or when config.use_cache=True.Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attentionblocks) that can be used (see past_key_values input) to speed up sequential decoding.Tuple of tuple(paddle.Tensor) of length config.n_layers, with each tuple having 2 tensors of shape(batch_size, num_heads, sequence_length, embed_size_per_head)) and 2 additional tensors of shape(batch_size, num_heads, encoder_sequence_length, embed_size_per_head).Returned when use_cache=True is passed or when config.use_cache=True.",
            "default": "",
            "optional": true
        },
        {
            "name": "decoder_hidden_states",
            "type": "tuplepaddle.Tensor",
            "description": "Tuple of paddle.Tensor (one for the output of the embeddings, if the model has an embedding layer, +one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).Returned when output_hidden_states=True is passed or when config.output_hidden_states=True.Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.Tuple of paddle.Tensor (one for the output of the embeddings, if the model has an embedding layer, +one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).Returned when output_hidden_states=True is passed or when config.output_hidden_states=True.",
            "default": "",
            "optional": true
        },
        {
            "name": "decoder_attentions",
            "type": "tuplepaddle.Tensor",
            "description": "Tuple of paddle.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length,sequence_length).Returned when output_attentions=True is passed or when config.output_attentions=True.Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in theself-attention heads.Tuple of paddle.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length,sequence_length).Returned when output_attentions=True is passed or when config.output_attentions=True.",
            "default": "",
            "optional": true
        },
        {
            "name": "cross_attentions",
            "type": "tuplepaddle.Tensor",
            "description": "Tuple of paddle.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length,sequence_length).Returned when output_attentions=True is passed or when config.output_attentions=True.Attentions weights of the decoders cross-attention layer, after the attention softmax, used to compute theweighted average in the cross-attention heads.Tuple of paddle.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length,sequence_length).Returned when output_attentions=True is passed or when config.output_attentions=True.",
            "default": "",
            "optional": true
        },
        {
            "name": "encoder_last_hidden_state",
            "type": "paddle.Tensor",
            "description": "Sequence of hidden-states at the output of the last layer of the encoder of the model whose shape is (batch_size, sequence_length, hidden_size),",
            "default": "",
            "optional": true
        },
        {
            "name": "encoder_hidden_states",
            "type": "tuplepaddle.Tensor",
            "description": "Tuple of paddle.Tensor (one for the output of the embeddings, if the model has an embedding layer, +one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).Returned when output_hidden_states=True is passed or when config.output_hidden_states=True.Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.Tuple of paddle.Tensor (one for the output of the embeddings, if the model has an embedding layer, +one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).Returned when output_hidden_states=True is passed or when config.output_hidden_states=True.",
            "default": "",
            "optional": true
        },
        {
            "name": "encoder_attentions",
            "type": "tuplepaddle.Tensor",
            "description": "Tuple of paddle.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length,sequence_length).Returned when output_attentions=True is passed or when config.output_attentions=True.Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in theself-attention heads.Tuple of paddle.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length,sequence_length).Returned when output_attentions=True is passed or when config.output_attentions=True.",
            "default": "",
            "optional": true
        }
    ],
    "return": {
        "description": "",
        "type": ""
    }
}