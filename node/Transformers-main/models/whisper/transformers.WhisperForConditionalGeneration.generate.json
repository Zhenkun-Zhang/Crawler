{
    "api": "transformers.WhisperForConditionalGeneration.generate",
    "type": "function",
    "version": "main",
    "args_list": [
        "input_features:",
        "generation_config",
        "logits_processor",
        "stopping_criteria",
        "prefix_allowed_tokens_fn",
        "torch.Tensor],",
        "typing.List[int]]]",
        "synced_gpus",
        "return_timestamps",
        "task",
        "language",
        "typing.List[str],",
        "NoneType]",
        "is_multilingual",
        "prompt_ids",
        "prompt_condition_type",
        "condition_on_prev_tokens",
        "temperature",
        "typing.Tuple[float,",
        "...],",
        "NoneType]",
        "compression_ratio_threshold",
        "logprob_threshold",
        "no_speech_threshold",
        "num_segment_frames",
        "attention_mask",
        "time_precision",
        "time_precision_features",
        "return_token_timestamps",
        "return_segments",
        "return_dict_in_generate",
        "force_unique_generate_call",
        "**kwargs",
        ")"
    ],
    "params": [
        {
            "name": "input_features",
            "type": "torch.Tensor of shape (batch_size, feature_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "Float values of log-mel features extracted from the raw speech waveform. The raw speech waveform can be obtained byloading a .flac or .wav audio file into an array of type List[float] or a numpy.ndarray, e.g. viathe soundfile library (pip install soundfile). To prepare the array into input_features, theAutoFeatureExtractor should be used for extracting the mel features, padding and conversion into atensor of type torch.FloatTensor. See call() for details."
        },
        {
            "name": "generation_config",
            "type": "GenerationConfig",
            "optional": true,
            "default": "",
            "description": "The generation configuration to be used as base parametrization for the generation call. **kwargspassed to generate matching the attributes of generation_config will override them. Ifgeneration_config is not provided, the default will be used, which had the following loadingpriority: 1) from the generation_config.json model file, if it exists, 2) from the modelconfiguration. Please note that unspecified parameters will inherit GenerationConfigsdefault values, whose documentation should be checked to parameterize generation."
        },
        {
            "name": "logits_processor",
            "type": "LogitsProcessorList",
            "optional": true,
            "default": "",
            "description": "Custom logits processors that complement the default logits processors built from arguments andgeneration config. If a logit processor is passed that is already created with the arguments or ageneration config an error is thrown. This feature is intended for advanced users."
        },
        {
            "name": "stopping_criteria",
            "type": "StoppingCriteriaList",
            "optional": true,
            "default": "",
            "description": "Custom stopping criteria that complement the default stopping criteria built from arguments and ageneration config. If a stopping criteria is passed that is already created with the arguments or ageneration config an error is thrown. This feature is intended for advanced users."
        },
        {
            "name": "prefix_allowed_tokens_fn",
            "type": "Callable[[int, torch.Tensor], List[int]]",
            "optional": true,
            "default": "",
            "description": "If provided, this function constraints the beam search to allowed tokens only at each step. If notprovided no constraint is applied. This function takes 2 arguments: the batch ID batch_id andinput_ids. It has to return a list with the allowed tokens for the next generation step conditionedon the batch ID batch_id and the previously generated tokens inputs_ids. This argument is usefulfor constrained generation conditioned on the prefix, as described in Autoregressive EntityRetrieval."
        },
        {
            "name": "synced_gpus",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to continue running the while loop until max_length (needed to avoid deadlocking withFullyShardedDataParallel and DeepSpeed ZeRO Stage 3)."
        },
        {
            "name": "return_timestamps",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to return the timestamps with the text. This enables the WhisperTimestampsLogitsProcessor."
        },
        {
            "name": "task",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Task to use for generation, either translate or transcribe. The model.config.forced_decoder_idswill be updated accordingly."
        },
        {
            "name": "language",
            "type": "str,list of str",
            "optional": true,
            "default": "",
            "description": "Language token to use for generation, can be either in the form of <|en|>, en or english. Forbatched generation, a list of language tokens can be passed. You can find all the possible languagetokens in the model.generation_config.lang_to_id dictionary."
        },
        {
            "name": "is_multilingual",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not the model is multilingual."
        },
        {
            "name": "prompt_ids",
            "type": "torch.Tensor",
            "optional": true,
            "default": "",
            "description": "Rank-1 tensor of token IDs created by passing text to get_prompt_ids() that isprovided as a prompt to each chunk. This can be used to provide or prompt-engineer a context fortranscription, e.g. custom vocabularies or proper nouns to make it more likely to predict those wordscorrectly. It cannot be used in conjunction with decoder_start_token_id as it overwrites this value."
        },
        {
            "name": "prompt_condition_type",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Only relevant for long-form transcription. Condition type of prompt_ids. first-segment means only the first segment is conditioned on prompt_ids. all-segments means each segment is conditioned on prompt_ids. Make sure to enable condition_on_prev_tokens for all-segments.Defaults to first-segment. For short-term transcription only first-segment is possible."
        },
        {
            "name": "condition_on_prev_tokens",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Only relevant for long-form transcription. Whether to condition each segment on the previous segment.As shown in the the Whisper paper, this can help to improveperformance."
        },
        {
            "name": "temperature",
            "type": "float,list of float",
            "optional": true,
            "default": "",
            "description": "The temperature to be used for generation. Passing a single float value and do_sample=True activatesgeneration using sampling. For long-form transcription, temperature fallback can be activated by passinga list of float values such as (0.0, 0.2, 0.4, 0.6, 0.8, 1.0). As shown in the the Whisper paper, this can help to improveperformance."
        },
        {
            "name": "compression_ratio_threshold",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Only relevant for long-form transcription. If defined, the zlib compression rate of each segment will be computed. If the compression rate ofa segment is higher than compression_ratio_threshold, temperature fallback is activated: the generated segment is discarded and the generation isrepeated using a higher temperature. The intuition behind this feature is that segments with very high compression ratessuffer from a lot of repetition. The unwanted repetition can be reduced by injecting more randomness by increasing the temperature. If compression_ratio_threshold is definedmake sure that temperature is a list of values. A common value for compression_ratio_threshold is 1.35.As shown in the the Whisper paper, this can help to improveperformance."
        },
        {
            "name": "logprob_threshold",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Only relevant for long-form transcription. If defined, the average log-probability of each segment will be computed. If the log-probability ofa given segment is lower than logprob_threshold, temperature fallback is activated: the generated segment is discarded and the generation isrepeated using a higher temperature. The intuition behind this feature is that segments of low log-probabilitycan be improved by injecting more randomness by increasing the temperature. If logprob_threshold is definedmake sure that temperature is a list of values. A common value for logprob_threshold is -1.0.As shown in the the Whisper paper, this can help to improveperformance."
        },
        {
            "name": "no_speech_threshold",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Only relevant for long-form transcription. If defined, the no-speech token combined with the logprob_thresholdis used to determine whether a segment contains only silence. In this case, the transcription for this segmentis skipped.As shown in the the Whisper paper, this can help to improveperformance."
        },
        {
            "name": "num_segment_frames",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The number of frames a single segment is made of. If not defined, num_segment_frames defaults to the models stridetimes the maximum input length."
        },
        {
            "name": "attention_mask",
            "type": "torch.Tensor",
            "optional": true,
            "default": "",
            "description": "attention_mask needs to be passed when doing long-form transcription using a batch size > 1."
        },
        {
            "name": "time_precision",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The duration of output token in seconds. E.g. 0.02 means that a generated token on average accountsfor 20 ms."
        },
        {
            "name": "time_precision_features",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The duration represented by a feature frame in seconds."
        },
        {
            "name": "return_token_timestamps",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to return token-level timestamps with the text. This can be used with or without thereturn_timestamps option. To get word-level timestamps, use the tokenizer to group the tokens intowords."
        },
        {
            "name": "return_segments",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to additionally return a list of all segments. Note that this option can only be enabledwhen doing long-form transcription."
        },
        {
            "name": "return_dict_in_generate",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return a ModelOutput instead of just returning the generated tokens.Note that when doing long-form transcription, return_dict_in_generate can only be enabled whenreturn_segments is set True. In this case the generation outputs of each segment is added to eachsegment."
        },
        {
            "name": "force_unique_generate_call",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to force a unique call to the underlying GenerationMixins generate() method. This is useful for assisted decoding and testing purposes to ensurethat only one call to generate() is made and therefore decoder input token ids and eos token ids are returned."
        },
        {
            "name": "kwargs",
            "type": "Dict[str, Any]",
            "optional": true,
            "default": "",
            "description": "Ad hoc parametrization of generate_config and/or additional model-specific kwargs that will beforwarded to the forward function of the model. If the model is an encoder-decoder model, encoderspecific kwargs should not be prefixed and decoder specific kwargs should be prefixed with decoder_."
        }
    ],
    "return": "ModelOutput or Dict[str, Any] or torch.LongTensor"
}