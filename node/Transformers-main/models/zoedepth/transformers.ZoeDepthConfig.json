{
    "api": "transformers.ZoeDepthConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "backbone_config",
        "backbone",
        "use_pretrained_backbone",
        "backbone_kwargs",
        "hidden_act",
        "initializer_range",
        "batch_norm_eps",
        "readout_type",
        "reassemble_factors",
        "2,",
        "1,",
        "0.5]",
        "neck_hidden_sizes",
        "192,",
        "384,",
        "768]",
        "fusion_hidden_size",
        "head_in_index",
        "use_batch_norm_in_fusion_residual",
        "use_bias_in_fusion_residual",
        "num_relative_features",
        "add_projection",
        "bottleneck_features",
        "num_attractors",
        "8,",
        "4,",
        "1]",
        "bin_embedding_dim",
        "attractor_alpha",
        "attractor_gamma",
        "attractor_kind",
        "min_temp",
        "max_temp",
        "bin_centers_type",
        "bin_configurations",
        "[{'n_bins'",
        "'min_depth'",
        "'max_depth'",
        "num_patch_transformer_layers",
        "patch_transformer_hidden_size",
        "patch_transformer_intermediate_size",
        "patch_transformer_num_attention_heads",
        "**kwargs"
    ],
    "params": [
        {
            "name": "backbone_config",
            "type": "Union[Dict[str, Any], PretrainedConfig]",
            "optional": true,
            "default": "None",
            "description": "The configuration of the backbone model."
        },
        {
            "name": "backbone",
            "type": "str",
            "optional": true,
            "default": "None",
            "description": "Name of backbone to use when backbone_config is None. If use_pretrained_backbone is True, thiswill load the corresponding pretrained weights from the timm or transformers library. If use_pretrained_backboneis False, this loads the backbones config and uses that to initialize the backbone with random weights."
        },
        {
            "name": "use_pretrained_backbone",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to use pretrained weights for the backbone."
        },
        {
            "name": "backbone_kwargs",
            "type": "dict",
            "optional": true,
            "default": "None",
            "description": "Keyword arguments to be passed to AutoBackbone when loading from a checkpointe.g. {out_indices: (0, 1, 2, 3)}. Cannot be specified if backbone_config is set."
        },
        {
            "name": "hidden_act",
            "type": "str,function",
            "optional": true,
            "default": "gelu",
            "description": "The non-linear activation function (function or string) in the encoder and pooler. If string, gelu,relu, selu and gelu_new are supported."
        },
        {
            "name": "initializer_range",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "batch_norm_eps",
            "type": "float",
            "optional": true,
            "default": "1e-05",
            "description": "The epsilon used by the batch normalization layers."
        },
        {
            "name": "readout_type",
            "type": "str",
            "optional": true,
            "default": "project",
            "description": "The readout type to use when processing the readout token (CLS token) of the intermediate hidden states ofthe ViT backbone. Can be one of [ignore, add, project].ignore simply ignores the CLS token.add passes the information from the CLS token to all other tokens by adding the representations.project passes information to the other tokens by concatenating the readout to all other tokens beforeprojecting therepresentation to the original feature dimension D using a linear layer followed by a GELU non-linearity."
        },
        {
            "name": "reassemble_factors",
            "type": "List[int]",
            "optional": true,
            "default": "[4,",
            "description": "The up/downsampling factors of the reassemble layers."
        },
        {
            "name": "neck_hidden_sizes",
            "type": "List[str]",
            "optional": true,
            "default": "[96,",
            "description": "The hidden sizes to project to for the feature maps of the backbone."
        },
        {
            "name": "fusion_hidden_size",
            "type": "int",
            "optional": true,
            "default": "256",
            "description": "The number of channels before fusion."
        },
        {
            "name": "head_in_index",
            "type": "int",
            "optional": true,
            "default": "-1",
            "description": "The index of the features to use in the heads."
        },
        {
            "name": "use_batch_norm_in_fusion_residual",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to use batch normalization in the pre-activate residual units of the fusion blocks."
        },
        {
            "name": "use_bias_in_fusion_residual",
            "type": "bool",
            "optional": true,
            "default": "None",
            "description": "Whether to use bias in the pre-activate residual units of the fusion blocks."
        },
        {
            "name": "num_relative_features",
            "type": "int",
            "optional": true,
            "default": "32",
            "description": "The number of features to use in the relative depth estimation head."
        },
        {
            "name": "add_projection",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to add a projection layer before the depth estimation head."
        },
        {
            "name": "bottleneck_features",
            "type": "int",
            "optional": true,
            "default": "256",
            "description": "The number of features in the bottleneck layer."
        },
        {
            "name": "num_attractors",
            "type": "List[int], *optional*, defaults to [16, 8, 4, 1]`",
            "optional": true,
            "default": "[16,",
            "description": "The number of attractors to use in each stage."
        },
        {
            "name": "bin_embedding_dim",
            "type": "int",
            "optional": true,
            "default": "128",
            "description": "The dimension of the bin embeddings."
        },
        {
            "name": "attractor_alpha",
            "type": "int",
            "optional": true,
            "default": "1000",
            "description": "The alpha value to use in the attractor."
        },
        {
            "name": "attractor_gamma",
            "type": "int",
            "optional": true,
            "default": "2",
            "description": "The gamma value to use in the attractor."
        },
        {
            "name": "attractor_kind",
            "type": "str",
            "optional": true,
            "default": "mean",
            "description": "The kind of attractor to use. Can be one of [mean, sum]."
        },
        {
            "name": "min_temp",
            "type": "float",
            "optional": true,
            "default": "0.0212",
            "description": "The minimum temperature value to consider."
        },
        {
            "name": "max_temp",
            "type": "float",
            "optional": true,
            "default": "50.0",
            "description": "The maximum temperature value to consider."
        },
        {
            "name": "bin_centers_type",
            "type": "str",
            "optional": true,
            "default": "softplus",
            "description": "Activation type used for bin centers. Can be normed or softplus. For normed bin centers, linear normalization trickis applied. This results in bounded bin centers. For softplus, softplus activation is used and thus are unbounded."
        },
        {
            "name": "bin_configurations",
            "type": "List[dict]",
            "optional": true,
            "default": "[{n_bins:",
            "description": ""
        },
        {
            "name": "num_patch_transformer_layers",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "The number of transformer layers to use in the patch transformer. Only used in case of multiple bin configurations."
        },
        {
            "name": "patch_transformer_hidden_size",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "The hidden size to use in the patch transformer. Only used in case of multiple bin configurations."
        },
        {
            "name": "patch_transformer_intermediate_size",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "The intermediate size to use in the patch transformer. Only used in case of multiple bin configurations."
        },
        {
            "name": "patch_transformer_num_attention_heads",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "The number of attention heads to use in the patch transformer. Only used in case of multiple bin configurations."
        }
    ],
    "return": ""
}