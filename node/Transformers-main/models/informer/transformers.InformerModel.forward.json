{
    "api": "transformers.InformerModel.forward",
    "type": "function",
    "version": "main",
    "args_list": [
        "past_values:",
        "past_time_features",
        "past_observed_mask",
        "static_categorical_features",
        "static_real_features",
        "future_values",
        "future_time_features",
        "decoder_attention_mask",
        "head_mask",
        "decoder_head_mask",
        "cross_attn_head_mask",
        "encoder_outputs",
        "past_key_values",
        "output_hidden_states",
        "output_attentions",
        "use_cache",
        "return_dict",
        ")"
    ],
    "params": [
        {
            "name": "past_values",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length),(batch_size, sequence_length, input_size)",
            "optional": false,
            "default": "",
            "description": "Past values of the time series, that serve as context in order to predict the future. The sequence size ofthis tensor must be larger than the context_length of the model, since the model will use the larger sizeto construct lag features, i.e. additional values from the past which are added in order to serve as extracontext.The sequence_length here is equal to config.context_length + max(config.lags_sequence), which if nolags_sequence is configured, is equal to config.context_length + 7 (as by default, the largestlook-back index in config.lags_sequence is 7). The property _past_length returns the actual length ofthe past.The past_values is what the Transformer encoder gets as input (with optional additional features, such asstatic_categorical_features, static_real_features, past_time_features and lags).Optionally, missing values need to be replaced with zeros and indicated via the past_observed_mask.For multivariate time series, the input_size > 1 dimension is required and corresponds to the number ofvariates in the time series per time step."
        },
        {
            "name": "past_time_features",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length, num_features)",
            "optional": false,
            "default": "",
            "description": "Required time features, which the model internally will add to past_values. These could be things likemonth of year, day of the month, etc. encoded as vectors (for instance as Fourier features). Thesecould also be so-called age features, which basically help the model know at which point in life atime-series is. Age features have small values for distant past time steps and increase monotonically themore we approach the current time step. Holiday features are also a good example of time features.These features serve as the positional encodings of the inputs. So contrary to a model like BERT, wherethe position encodings are learned from scratch internally as parameters of the model, the Time SeriesTransformer requires to provide additional time features. The Time Series Transformer only learnsadditional embeddings for static_categorical_features.Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these featuresmust but known at prediction time.The num_features here is equal to config.num_time_features+config.num_dynamic_real_features`."
        },
        {
            "name": "past_observed_mask",
            "type": "torch.BoolTensor of shape (batch_size, sequence_length),(batch_size, sequence_length, input_size",
            "optional": true,
            "default": "",
            "description": "Boolean mask to indicate which past_values were observed and which were missing. Mask values selected in[0, 1]:1 for values that are observed,0 for values that are missing (i.e. NaNs that were replaced by zeros)."
        },
        {
            "name": "static_categorical_features",
            "type": "torch.LongTensor of shape (batch_size, number of static categorical features",
            "optional": true,
            "default": "",
            "description": "Optional static categorical features for which the model will learn an embedding, which it will add to thevalues of the time series.Static categorical features are features which have the same value for all time steps (static over time).A typical example of a static categorical feature is a time series ID."
        },
        {
            "name": "static_real_features",
            "type": "torch.FloatTensor of shape (batch_size, number of static real features",
            "optional": true,
            "default": "",
            "description": "Optional static real features which the model will add to the values of the time series.Static real features are features which have the same value for all time steps (static over time).A typical example of a static real feature is promotion information."
        },
        {
            "name": "future_values",
            "type": "torch.FloatTensor of shape (batch_size, prediction_length),(batch_size, prediction_length, input_size",
            "optional": true,
            "default": "",
            "description": "Future values of the time series, that serve as labels for the model. The future_values is what theTransformer needs during training to learn to output, given the past_values.The sequence length here is equal to prediction_length.See the demo notebook and code snippets for details.Optionally, during training any missing values need to be replaced with zeros and indicated via thefuture_observed_mask.For multivariate time series, the input_size > 1 dimension is required and corresponds to the number ofvariates in the time series per time step."
        },
        {
            "name": "future_time_features",
            "type": "torch.FloatTensor of shape (batch_size, prediction_length, num_features)",
            "optional": false,
            "default": "",
            "description": "Required time features for the prediction window, which the model internally will add to future_values.These could be things like month of year, day of the month, etc. encoded as vectors (for instance asFourier features). These could also be so-called age features, which basically help the model know atwhich point in life a time-series is. Age features have small values for distant past time steps andincrease monotonically the more we approach the current time step. Holiday features are also a good exampleof time features.These features serve as the positional encodings of the inputs. So contrary to a model like BERT, wherethe position encodings are learned from scratch internally as parameters of the model, the Time SeriesTransformer requires to provide additional time features. The Time Series Transformer only learnsadditional embeddings for static_categorical_features.Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these featuresmust but known at prediction time.The num_features here is equal to config.num_time_features+config.num_dynamic_real_features`."
        },
        {
            "name": "future_observed_mask",
            "type": "torch.BoolTensor of shape (batch_size, sequence_length),(batch_size, sequence_length, input_size",
            "optional": true,
            "default": "",
            "description": "Boolean mask to indicate which future_values were observed and which were missing. Mask values selectedin [0, 1]:1 for values that are observed,0 for values that are missing (i.e. NaNs that were replaced by zeros).This mask is used to filter out missing values for the final loss calculation."
        },
        {
            "name": "attention_mask",
            "type": "torch.Tensor of shape (batch_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "Mask to avoid performing attention on certain token indices. Mask values selected in [0, 1]:1 for tokens that are not masked,0 for tokens that are masked.What are attention masks?"
        },
        {
            "name": "decoder_attention_mask",
            "type": "torch.LongTensor of shape (batch_size, target_sequence_length",
            "optional": true,
            "default": "",
            "description": "Mask to avoid performing attention on certain token indices. By default, a causal mask will be used, tomake sure the model can only look at previous inputs in order to predict the future."
        },
        {
            "name": "head_mask",
            "type": "torch.Tensor of shape (encoder_layers, encoder_attention_heads",
            "optional": true,
            "default": "",
            "description": "Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in [0, 1]:1 indicates the head is not masked,0 indicates the head is masked."
        },
        {
            "name": "decoder_head_mask",
            "type": "torch.Tensor of shape (decoder_layers, decoder_attention_heads",
            "optional": true,
            "default": "",
            "description": "Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in [0, 1]:1 indicates the head is not masked,0 indicates the head is masked."
        },
        {
            "name": "cross_attn_head_mask",
            "type": "torch.Tensor of shape (decoder_layers, decoder_attention_heads",
            "optional": true,
            "default": "",
            "description": "Mask to nullify selected heads of the cross-attention modules. Mask values selected in [0, 1]:1 indicates the head is not masked,0 indicates the head is masked."
        },
        {
            "name": "encoder_outputs",
            "type": "tuple(tuple(torch.FloatTensor",
            "optional": true,
            "default": "",
            "description": "Tuple consists of last_hidden_state, hidden_states (optional) and attentions (optional)last_hidden_state of shape (batch_size, sequence_length, hidden_size) (optional) is a sequence ofhidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder."
        },
        {
            "name": "past_key_values",
            "type": "tuple(tuple(torch.FloatTensor)",
            "optional": true,
            "default": "",
            "description": "Tuple of tuple(torch.FloatTensor) of length config.n_layers, with each tuple having 2 tensors of shape(batch_size, num_heads, sequence_length, embed_size_per_head)) and 2 additional tensors of shape(batch_size, num_heads, encoder_sequence_length, embed_size_per_head).Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attentionblocks) that can be used (see past_key_values input) to speed up sequential decoding.If past_key_values are used, the user can optionally input only the last decoder_input_ids (those thatdont have their past key value states given to this model) of shape (batch_size, 1) instead of alldecoder_input_ids of shape (batch_size, sequence_length)."
        },
        {
            "name": "inputs_embeds",
            "type": "torch.FloatTensor of shape (batch_size, sequence_length, hidden_size",
            "optional": true,
            "default": "",
            "description": "Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. Thisis useful if you want more control over how to convert input_ids indices into associated vectors than themodels internal embedding lookup matrix."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "If set to True, past_key_values key value states are returned and can be used to speed up decoding (seepast_key_values)."
        },
        {
            "name": "output_attentions",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the attentions tensors of all attention layers. See attentions under returnedtensors for more detail."
        },
        {
            "name": "output_hidden_states",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the hidden states of all layers. See hidden_states under returned tensors formore detail."
        },
        {
            "name": "return_dict",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return a ModelOutput instead of a plain tuple."
        }
    ],
    "return": "transformers.modeling_outputs.Seq2SeqTSModelOutput or tuple(torch.FloatTensor)"
}