{
    "api": "transformers.JukeboxPrior.sample",
    "type": "function",
    "version": "main",
    "args_list": [
        "n_samples",
        "music_tokens",
        "music_tokens_conds",
        "metadata",
        "temp",
        "top_k",
        "top_p",
        "chunk_size",
        "sample_tokens"
    ],
    "params": [
        {
            "name": "n_samples",
            "type": "int",
            "optional": false,
            "default": "",
            "description": "Number of samples to generate."
        },
        {
            "name": "music_tokens",
            "type": "List[torch.LongTensor]",
            "optional": true,
            "default": "None",
            "description": "Previously gemerated tokens at the current level. Used as context for the generation."
        },
        {
            "name": "music_tokens_conds",
            "type": "List[torch.FloatTensor]",
            "optional": true,
            "default": "None",
            "description": "Upper-level music tokens generated by the previous prior model. Is None if the generation is notconditionned on the upper-level tokens."
        },
        {
            "name": "metadata",
            "type": "List[torch.LongTensor]",
            "optional": true,
            "default": "None",
            "description": "List containing the metatdata tensor with the artist, genre and the lyric tokens."
        },
        {
            "name": "temp",
            "type": "float",
            "optional": true,
            "default": "1.0",
            "description": "Sampling temperature."
        },
        {
            "name": "top_k",
            "type": "int",
            "optional": true,
            "default": "0",
            "description": "Top k probabilities used for filtering."
        },
        {
            "name": "top_p",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "Top p probabilities used for filtering."
        },
        {
            "name": "chunk_size",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Size of the chunks used to prepare the cache of the transformer."
        },
        {
            "name": "sample_tokens",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Number of tokens to sample."
        }
    ],
    "return": ""
}