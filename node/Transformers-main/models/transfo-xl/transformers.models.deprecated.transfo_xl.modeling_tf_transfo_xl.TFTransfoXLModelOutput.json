{
    "api": "transformers.models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput",
    "type": "class",
    "version": "main",
    "args_list": [
        "last_hidden_state:",
        "tf.Tensor",
        "mems",
        "hidden_states",
        "|",
        "attentions",
        "|"
    ],
    "params": [
        {
            "name": "last_hidden_state",
            "type": "tf.Tensor of shape (batch_size, sequence_length, hidden_size)",
            "optional": false,
            "default": "",
            "description": "Sequence of hidden-states at the output of the last layer of the model."
        },
        {
            "name": "mems",
            "type": "List[tf.Tensor] of length config.n_layers",
            "optional": false,
            "default": "",
            "description": "Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see memsinput) to speed up sequential decoding. The token ids which have their past given to this model should notbe passed as input ids as they have already been computed."
        },
        {
            "name": "hidden_states",
            "type": "tuple(tf.Tensor",
            "optional": true,
            "default": "",
            "description": "Tuple of tf.Tensor (one for the output of the embeddings + one for the output of each layer) of shape(batch_size, sequence_length, hidden_size).Hidden-states of the model at the output of each layer plus the initial embedding outputs."
        },
        {
            "name": "attentions",
            "type": "tuple(tf.Tensor",
            "optional": true,
            "default": "",
            "description": "Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).Attentions weights after the attention softmax, used to compute the weighted average in the self-attentionheads."
        }
    ],
    "return": ""
}