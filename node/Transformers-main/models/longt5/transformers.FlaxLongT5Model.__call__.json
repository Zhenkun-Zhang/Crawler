{
    "api": "transformers.FlaxLongT5Model.__call__",
    "type": "function",
    "version": "main",
    "args_list": [
        "input_ids:",
        "attention_mask",
        "decoder_input_ids",
        "decoder_attention_mask",
        "output_attentions",
        "output_hidden_states",
        "return_dict",
        "train",
        "params",
        "dropout_rng",
        "PRNGKey",
        "at",
        "0x7f3a03b75090>",
        ")"
    ],
    "params": [
        {
            "name": "input_ids",
            "type": "jnp.ndarray of shape (batch_size, sequence_length)",
            "optional": false,
            "default": "",
            "description": "Indices of input sequence tokens in the vocabulary. LongT5 is a model with relative position embeddings soyou should be able to pad the inputs on both the right and the left.Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() andPreTrainedTokenizer.call() for detail.What are input IDs?To know more on how to prepare input_ids for pretraining take a look a LONGT5Training."
        },
        {
            "name": "attention_mask",
            "type": "jnp.ndarray of shape (batch_size, sequence_length",
            "optional": true,
            "default": "",
            "description": "Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:1 for tokens that are not masked,0 for tokens that are masked.What are attention masks?"
        },
        {
            "name": "decoder_input_ids",
            "type": "jnp.ndarray of shape (batch_size, target_sequence_length",
            "optional": true,
            "default": "",
            "description": "Indices of decoder input sequence tokens in the vocabulary.Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() andPreTrainedTokenizer.call() for details.What are decoder input IDs?LONGT5 uses the pad_token_id as the starting token for decoder_input_ids generation. Ifpast_key_values is used, optionally only the last decoder_input_ids have to be input (seepast_key_values).To know more on how to prepare decoder_input_ids for pretraining take a look at LONGT5Training."
        },
        {
            "name": "decoder_attention_mask",
            "type": "jnp.ndarray of shape (batch_size, target_sequence_length",
            "optional": true,
            "default": "",
            "description": "Default behavior: generate a tensor that ignores pad tokens in decoder_input_ids. Causal mask will alsobe used by default."
        },
        {
            "name": "encoder_outputs",
            "type": "tuple(tuple(jnp.ndarray",
            "optional": true,
            "default": "",
            "description": "Tuple consists of (last_hidden_state, optional: hidden_states, optional: attentions)last_hidden_state of shape (batch_size, sequence_length, hidden_size) is a sequence of hidden states atthe output of the last layer of the encoder. Used in the cross-attention of the decoder."
        },
        {
            "name": "past_key_values",
            "type": "tuple(tuple(jnp.ndarray)) of length config.n_layers with each tuple having 4 tensors of shape (batch_size, num_heads, sequence_length - 1, embed_size_per_head)",
            "optional": false,
            "default": "",
            "description": "Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.If past_key_values are used, the user can optionally input only the last decoder_input_ids (those thatdont have their past key value states given to this model) of shape (batch_size, 1) instead of alldecoder_input_ids of shape (batch_size, sequence_length)."
        }
    ],
    "return": "transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)"
}