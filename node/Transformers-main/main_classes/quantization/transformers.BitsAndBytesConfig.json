{
    "api": "transformers.BitsAndBytesConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "load_in_8bit",
        "load_in_4bit",
        "llm_int8_threshold",
        "llm_int8_skip_modules",
        "llm_int8_enable_fp32_cpu_offload",
        "llm_int8_has_fp16_weight",
        "bnb_4bit_compute_dtype",
        "bnb_4bit_quant_type",
        "bnb_4bit_use_double_quant",
        "bnb_4bit_quant_storage",
        "**kwargs"
    ],
    "params": [
        {
            "name": "load_in_8bit",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "This flag is used to enable 8-bit quantization with LLM.int8()."
        },
        {
            "name": "load_in_4bit",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers frombitsandbytes."
        },
        {
            "name": "llm_int8_threshold",
            "type": "float",
            "optional": true,
            "default": "6.0",
            "description": "This corresponds to the outlier threshold for outlier detection as described in LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale paper: https://arxiv.org/abs/2208.07339 Any hidden states valuethat is above this threshold will be considered an outlier and the operation on those values will be donein fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], butthere are some exceptional systematic outliers that are very differently distributed for large models.These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values ofmagnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6,but a lower threshold might be needed for more unstable models (small models, fine-tuning)."
        },
        {
            "name": "llm_int8_skip_modules",
            "type": "List[str]",
            "optional": true,
            "default": "None",
            "description": "An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such asJukebox that has several heads in different places and not necessarily at the last position. For examplefor CausalLM models, the last lm_head is kept in its original dtype."
        },
        {
            "name": "llm_int8_enable_fp32_cpu_offload",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "This flag is used for advanced use cases and users that are aware of this feature. If you want to splityour model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can usethis flag. This is useful for offloading large models such as google/flan-t5-xxl. Note that the int8operations will not be run on CPU."
        },
        {
            "name": "llm_int8_has_fp16_weight",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do nothave to be converted back and forth for the backward pass."
        },
        {
            "name": "bnb_4bit_compute_dtype",
            "type": "torch.dtype,str",
            "optional": true,
            "default": "None",
            "description": "This sets the computational type which might be different than the input type. For example, inputs might befp32, but computation can be set to bf16 for speedups."
        },
        {
            "name": "bnb_4bit_quant_type",
            "type": "str,  optional, defaults to \"fp4\"",
            "optional": true,
            "default": "fp4",
            "description": "This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data typeswhich are specified by fp4 or nf4."
        },
        {
            "name": "bnb_4bit_use_double_quant",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "This flag is used for nested quantization where the quantization constants from the first quantization arequantized again."
        },
        {
            "name": "bnb_4bit_quant_storage",
            "type": "torch.dtype,str",
            "optional": true,
            "default": "None",
            "description": "This sets the storage type to pack the quanitzed 4-bit prarams."
        },
        {
            "name": "kwargs",
            "type": "Dict[str, Any]",
            "optional": true,
            "default": "",
            "description": "Additional parameters from which to initialize the configuration object."
        }
    ],
    "return": ""
}