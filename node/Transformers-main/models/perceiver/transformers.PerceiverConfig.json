{
    "api": "transformers.PerceiverConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "num_latents",
        "d_latents",
        "d_model",
        "num_blocks",
        "num_self_attends_per_block",
        "num_self_attention_heads",
        "num_cross_attention_heads",
        "qk_channels",
        "v_channels",
        "cross_attention_shape_for_attention",
        "self_attention_widening_factor",
        "cross_attention_widening_factor",
        "hidden_act",
        "attention_probs_dropout_prob",
        "initializer_range",
        "layer_norm_eps",
        "use_query_residual",
        "vocab_size",
        "max_position_embeddings",
        "image_size",
        "train_size",
        "496]",
        "num_frames",
        "audio_samples_per_frame",
        "samples_per_patch",
        "output_shape",
        "16,",
        "224,",
        "224]",
        "output_num_channels",
        "_label_trainable_num_channels",
        "**kwargs"
    ],
    "params": [
        {
            "name": "num_latents",
            "type": "int",
            "optional": true,
            "default": "256",
            "description": "The number of latents."
        },
        {
            "name": "d_latents",
            "type": "int",
            "optional": true,
            "default": "1280",
            "description": "Dimension of the latent embeddings."
        },
        {
            "name": "d_model",
            "type": "int",
            "optional": true,
            "default": "768",
            "description": "Dimension of the inputs. Should only be provided in case [PerceiverTextPreprocessor] is used or nopreprocessor is provided."
        },
        {
            "name": "num_blocks",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Number of blocks in the Transformer encoder."
        },
        {
            "name": "num_self_attends_per_block",
            "type": "int",
            "optional": true,
            "default": "26",
            "description": "The number of self-attention layers per block."
        },
        {
            "name": "num_self_attention_heads",
            "type": "int",
            "optional": true,
            "default": "8",
            "description": "Number of attention heads for each self-attention layer in the Transformer encoder."
        },
        {
            "name": "num_cross_attention_heads",
            "type": "int",
            "optional": true,
            "default": "8",
            "description": "Number of attention heads for each cross-attention layer in the Transformer encoder."
        },
        {
            "name": "qk_channels",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Dimension to project the queries + keys before applying attention in the cross-attention and self-attentionlayers of the encoder. Will default to preserving the dimension of the queries if not specified."
        },
        {
            "name": "v_channels",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Dimension to project the values before applying attention in the cross-attention and self-attention layersof the encoder. Will default to preserving the dimension of the queries if not specified."
        },
        {
            "name": "cross_attention_shape_for_attention",
            "type": "str",
            "optional": true,
            "default": "kv",
            "description": "Dimension to use when downsampling the queries and keys in the cross-attention layer of the encoder."
        },
        {
            "name": "self_attention_widening_factor",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Dimension of the feed-forward layer in the cross-attention layer of the Transformer encoder."
        },
        {
            "name": "cross_attention_widening_factor",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Dimension of the feed-forward layer in the self-attention layers of the Transformer encoder."
        },
        {
            "name": "hidden_act",
            "type": "str,function",
            "optional": true,
            "default": "gelu",
            "description": "The non-linear activation function (function or string) in the encoder and pooler. If string, gelu,relu, selu and gelu_new are supported."
        },
        {
            "name": "attention_probs_dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout ratio for the attention probabilities."
        },
        {
            "name": "initializer_range",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "layer_norm_eps",
            "type": "float",
            "optional": true,
            "default": "1e-12",
            "description": "The epsilon used by the layer normalization layers."
        },
        {
            "name": "use_query_residual",
            "type": "float",
            "optional": true,
            "default": "True",
            "description": "Whether to add a query residual in the cross-attention layer of the encoder."
        },
        {
            "name": "vocab_size",
            "type": "int",
            "optional": true,
            "default": "262",
            "description": "Vocabulary size for the masked language modeling model."
        },
        {
            "name": "max_position_embeddings",
            "type": "int",
            "optional": true,
            "default": "2048",
            "description": "The maximum sequence length that the masked language modeling model might ever be used with. Typically setthis to something large just in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "name": "image_size",
            "type": "int",
            "optional": true,
            "default": "56",
            "description": "Size of the images after preprocessing, for PerceiverForImageClassificationLearned."
        },
        {
            "name": "train_size",
            "type": "List[int]",
            "optional": true,
            "default": "[368,",
            "description": "Training size of the images for the optical flow model."
        },
        {
            "name": "num_frames",
            "type": "int",
            "optional": true,
            "default": "16",
            "description": "Number of video frames used for the multimodal autoencoding model."
        },
        {
            "name": "audio_samples_per_frame",
            "type": "int",
            "optional": true,
            "default": "1920",
            "description": "Number of audio samples per frame for the multimodal autoencoding model."
        },
        {
            "name": "samples_per_patch",
            "type": "int",
            "optional": true,
            "default": "16",
            "description": "Number of audio samples per patch when preprocessing the audio for the multimodal autoencoding model."
        },
        {
            "name": "output_shape",
            "type": "List[int]",
            "optional": true,
            "default": "[1,",
            "description": "Shape of the output (batch_size, num_frames, height, width) for the video decoder queries of the multimodalautoencoding model. This excludes the channel dimension."
        },
        {
            "name": "output_num_channels",
            "type": "int",
            "optional": true,
            "default": "512",
            "description": "Number of output channels for each modalitiy decoder."
        }
    ],
    "return": ""
}