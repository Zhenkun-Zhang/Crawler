{
    "api": "transformers.VitPoseImageProcessor.preprocess",
    "type": "function",
    "version": "main",
    "args_list": [
        "images:",
        "typing.Union[ForwardRef('PIL.Image.Image'),",
        "numpy.ndarray,",
        "ForwardRef('torch.Tensor'),",
        "list['PIL.Image.Image'],",
        "list[numpy.ndarray],",
        "list['torch.Tensor']]",
        "boxes",
        "numpy.ndarray]",
        "do_affine_transform",
        "size",
        "int]",
        "do_rescale",
        "rescale_factor",
        "do_normalize",
        "image_mean",
        "typing.List[float],",
        "NoneType]",
        "image_std",
        "typing.List[float],",
        "NoneType]",
        "return_tensors",
        "transformers.utils.generic.TensorType,",
        "NoneType]",
        "data_format",
        "transformers.image_utils.ChannelDimension]",
        "<ChannelDimension.FIRST",
        "input_data_format",
        "transformers.image_utils.ChannelDimension,",
        "NoneType]",
        ")"
    ],
    "params": [
        {
            "name": "images",
            "type": "ImageInput",
            "optional": false,
            "default": "",
            "description": "Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. Ifpassing in images with pixel values between 0 and 1, set do_rescale=False."
        },
        {
            "name": "boxes",
            "type": "List[List[List[float]]],np.ndarray",
            "optional": false,
            "default": "",
            "description": "List or array of bounding boxes for each image. Each box should be a list of 4 floats representing the boundingbox coordinates in COCO format (top_left_x, top_left_y, width, height)."
        },
        {
            "name": "do_affine_transform",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to apply an affine transformation to the input images."
        },
        {
            "name": "size",
            "type": "Dict[str, int] optional, defaults to self.size",
            "optional": true,
            "default": "",
            "description": "Dictionary in the format {height: h, width: w} specifying the size of the output image afterresizing."
        },
        {
            "name": "do_rescale",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to rescale the image values between [0 - 1]."
        },
        {
            "name": "rescale_factor",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Rescale factor to rescale the image by if do_rescale is set to True."
        },
        {
            "name": "do_normalize",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to normalize the image."
        },
        {
            "name": "image_mean",
            "type": "float,List[float]",
            "optional": true,
            "default": "",
            "description": "Image mean to use if do_normalize is set to True."
        },
        {
            "name": "image_std",
            "type": "float,List[float]",
            "optional": true,
            "default": "",
            "description": "Image standard deviation to use if do_normalize is set to True."
        },
        {
            "name": "return_tensors",
            "type": "str,TensorType",
            "optional": true,
            "default": "",
            "description": "If set, will return tensors of a particular framework. Acceptable values are:tf: Return TensorFlow tf.constant objects.pt: Return PyTorch torch.Tensor objects.np: Return NumPy np.ndarray objects.jax: Return JAX jnp.ndarray objects."
        }
    ],
    "return": "BatchFeature"
}