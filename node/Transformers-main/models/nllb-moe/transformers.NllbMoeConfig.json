{
    "api": "transformers.NllbMoeConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "vocab_size",
        "max_position_embeddings",
        "encoder_layers",
        "encoder_ffn_dim",
        "encoder_attention_heads",
        "decoder_layers",
        "decoder_ffn_dim",
        "decoder_attention_heads",
        "encoder_layerdrop",
        "decoder_layerdrop",
        "use_cache",
        "is_encoder_decoder",
        "activation_function",
        "d_model",
        "dropout",
        "attention_dropout",
        "activation_dropout",
        "init_std",
        "decoder_start_token_id",
        "scale_embedding",
        "router_bias",
        "router_dtype",
        "router_ignore_padding_tokens",
        "num_experts",
        "expert_capacity",
        "encoder_sparse_step",
        "decoder_sparse_step",
        "router_z_loss_coef",
        "router_aux_loss_coef",
        "second_expert_policy",
        "normalize_router_prob_before_dropping",
        "batch_prioritized_routing",
        "moe_eval_capacity_token_fraction",
        "moe_token_dropout",
        "pad_token_id",
        "bos_token_id",
        "eos_token_id",
        "output_router_logits",
        "**kwargs"
    ],
    "params": [
        {
            "name": "vocab_size",
            "type": "int",
            "optional": true,
            "default": "128112",
            "description": "Vocabulary size of the NllbMoe model. Defines the number of different tokens that can be represented by theinputs_ids passed when calling NllbMoeModel or"
        },
        {
            "name": "d_model",
            "type": "int",
            "optional": true,
            "default": "1024",
            "description": "Dimensionality of the layers and the pooler layer."
        },
        {
            "name": "encoder_layers",
            "type": "int",
            "optional": true,
            "default": "12",
            "description": "Number of encoder layers."
        },
        {
            "name": "decoder_layers",
            "type": "int",
            "optional": true,
            "default": "12",
            "description": "Number of decoder layers."
        },
        {
            "name": "encoder_attention_heads",
            "type": "int",
            "optional": true,
            "default": "16",
            "description": "Number of attention heads for each attention layer in the Transformer encoder."
        },
        {
            "name": "decoder_attention_heads",
            "type": "int",
            "optional": true,
            "default": "16",
            "description": "Number of attention heads for each attention layer in the Transformer decoder."
        },
        {
            "name": "decoder_ffn_dim",
            "type": "int",
            "optional": true,
            "default": "4096",
            "description": "Dimensionality of the intermediate (often named feed-forward) layer in decoder."
        },
        {
            "name": "encoder_ffn_dim",
            "type": "int",
            "optional": true,
            "default": "4096",
            "description": "Dimensionality of the intermediate (often named feed-forward) layer in encoder."
        },
        {
            "name": "activation_function",
            "type": "str,function",
            "optional": true,
            "default": "relu",
            "description": "The non-linear activation function (function or string) in the encoder and pooler. If string, gelu,relu, silu and gelu_new are supported."
        },
        {
            "name": "dropout",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout probability for all fully connected layers in the embeddings, encoder, and pooler."
        },
        {
            "name": "attention_dropout",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout ratio for the attention probabilities."
        },
        {
            "name": "activation_dropout",
            "type": "float",
            "optional": true,
            "default": "0.0",
            "description": "The dropout ratio for activations inside the fully connected layer."
        },
        {
            "name": "classifier_dropout",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The dropout ratio for classifier."
        },
        {
            "name": "max_position_embeddings",
            "type": "int",
            "optional": true,
            "default": "1024",
            "description": "The maximum sequence length that this model might ever be used with. Typically set this to something largejust in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "name": "init_std",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "encoder_layerdrop",
            "type": "float",
            "optional": true,
            "default": "0.05",
            "description": "The LayerDrop probability for the encoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)for more details."
        },
        {
            "name": "decoder_layerdrop",
            "type": "float",
            "optional": true,
            "default": "0.05",
            "description": "The LayerDrop probability for the decoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)for more details."
        },
        {
            "name": "second_expert_policy",
            "type": " str",
            "optional": true,
            "default": "all",
            "description": "The policy used for the sampling the probability of being sampled to a second expert for each token."
        },
        {
            "name": "normalize_router_prob_before_dropping",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether or not to normalize the router probabilities before applying a mask based on the experts capacity(capacity dropping)."
        },
        {
            "name": "batch_prioritized_routing",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether or not to orders the tokens by their router probabilities before capacity dropping. This means thatthe tokens that have the highest probabilities will be routed before other tokens that might be further inthe sequence."
        },
        {
            "name": "moe_eval_capacity_token_fraction",
            "type": "float",
            "optional": true,
            "default": "1.0",
            "description": "Fraction of tokens as capacity during validation, if set to negative, uses the same as training. Should bein range: (0.0, 1.0]."
        },
        {
            "name": "num_experts",
            "type": "int",
            "optional": true,
            "default": "128",
            "description": "Number of experts for each NllbMoeSparseMlp layer."
        },
        {
            "name": "expert_capacity",
            "type": "int",
            "optional": true,
            "default": "64",
            "description": "Number of tokens that can be stored in each expert."
        },
        {
            "name": "encoder_sparse_step",
            "type": "int",
            "optional": true,
            "default": "4",
            "description": "Frequency of the sparse layers in the encoder. 4 means that one out of 4 layers will be sparse."
        },
        {
            "name": "decoder_sparse_step",
            "type": "int",
            "optional": true,
            "default": "4",
            "description": "Frequency of the sparse layers in the decoder. 4 means that one out of 4 layers will be sparse."
        },
        {
            "name": "router_dtype",
            "type": "str",
            "optional": true,
            "default": "float32",
            "description": "The dtype used for the routers. It is preferable to keep the dtype to float32 as specified in theselective precision discussion in the paper."
        },
        {
            "name": "router_ignore_padding_tokens",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether to ignore padding tokens when routing. if False, the padding tokens are not routed to anyexperts."
        },
        {
            "name": "router_bias",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether or not the classifier of the router should have a bias."
        },
        {
            "name": "moe_token_dropout",
            "type": "float",
            "optional": true,
            "default": "0.2",
            "description": "Masking rate for MoE expert output masking (EOM), which is implemented via a Dropout2d on the expertoutputs."
        },
        {
            "name": "output_router_logits",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether or not to return the router logits. Only set to True to get the auxiliary loss when training."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not the model should return the last key/values attentions (not used by all models)."
        }
    ],
    "return": ""
}