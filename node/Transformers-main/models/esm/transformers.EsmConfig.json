{
    "api": "transformers.EsmConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "vocab_size",
        "mask_token_id",
        "pad_token_id",
        "hidden_size",
        "num_hidden_layers",
        "num_attention_heads",
        "intermediate_size",
        "hidden_dropout_prob",
        "attention_probs_dropout_prob",
        "max_position_embeddings",
        "initializer_range",
        "layer_norm_eps",
        "position_embedding_type",
        "use_cache",
        "emb_layer_norm_before",
        "token_dropout",
        "is_folding_model",
        "esmfold_config",
        "vocab_list",
        "**kwargs"
    ],
    "params": [
        {
            "name": "vocab_size",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Vocabulary size of the ESM model. Defines the number of different tokens that can be represented by theinputs_ids passed when calling ESMModel."
        },
        {
            "name": "mask_token_id",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "The index of the mask token in the vocabulary. This must be included in the config because of themask-dropout scaling trick, which will scale the inputs depending on the number of masked tokens."
        },
        {
            "name": "pad_token_id",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "The index of the padding token in the vocabulary. This must be included in the config because certain partsof the ESM code use this instead of the attention mask."
        },
        {
            "name": "hidden_size",
            "type": "int",
            "optional": true,
            "default": "768",
            "description": "Dimensionality of the encoder layers and the pooler layer."
        },
        {
            "name": "num_hidden_layers",
            "type": "int",
            "optional": true,
            "default": "12",
            "description": "Number of hidden layers in the Transformer encoder."
        },
        {
            "name": "num_attention_heads",
            "type": "int",
            "optional": true,
            "default": "12",
            "description": "Number of attention heads for each attention layer in the Transformer encoder."
        },
        {
            "name": "intermediate_size",
            "type": "int",
            "optional": true,
            "default": "3072",
            "description": "Dimensionality of the intermediate (often named feed-forward) layer in the Transformer encoder."
        },
        {
            "name": "hidden_dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout probability for all fully connected layers in the embeddings, encoder, and pooler."
        },
        {
            "name": "attention_probs_dropout_prob",
            "type": "float",
            "optional": true,
            "default": "0.1",
            "description": "The dropout ratio for the attention probabilities."
        },
        {
            "name": "max_position_embeddings",
            "type": "int",
            "optional": true,
            "default": "1026",
            "description": "The maximum sequence length that this model might ever be used with. Typically set this to something largejust in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "name": "initializer_range",
            "type": "float",
            "optional": true,
            "default": "0.02",
            "description": "The standard deviation of the truncated_normal_initializer for initializing all weight matrices."
        },
        {
            "name": "layer_norm_eps",
            "type": "float",
            "optional": true,
            "default": "1e-12",
            "description": "The epsilon used by the layer normalization layers."
        },
        {
            "name": "position_embedding_type",
            "type": "str",
            "optional": true,
            "default": "absolute",
            "description": "Type of position embedding. Choose one of absolute, relative_key, relative_key_query, rotary.For positional embeddings use absolute. For more information on relative_key, please refer toSelf-Attention with Relative Position Representations (Shaw et al.).For more information on relative_key_query, please refer to Method 4 in Improve Transformer Modelswith Better Relative Position Embeddings (Huang et al.)."
        },
        {
            "name": "is_decoder",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether the model is used as a decoder or not. If False, the model is used as an encoder."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not the model should return the last key/values attentions (not used by all models). Onlyrelevant if config.is_decoder=True."
        },
        {
            "name": "emb_layer_norm_before",
            "type": "bool",
            "optional": true,
            "default": "None",
            "description": "Whether to apply layer normalization after embeddings but before the main stem of the network."
        },
        {
            "name": "token_dropout",
            "type": "bool, defaults to False",
            "optional": false,
            "default": "False",
            "description": "When this is enabled, masked tokens are treated as if they had been dropped out by input dropout."
        }
    ],
    "return": ""
}