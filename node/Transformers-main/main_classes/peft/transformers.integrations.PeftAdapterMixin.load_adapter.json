{
    "api": "transformers.integrations.PeftAdapterMixin.load_adapter",
    "type": "function",
    "version": "main",
    "args_list": [
        "peft_model_id:",
        "adapter_name",
        "revision",
        "token",
        "device_map",
        "max_memory",
        "offload_folder",
        "offload_index",
        "peft_config",
        "typing.Any]",
        "adapter_state_dict",
        "ForwardRef('torch.Tensor')]]",
        "low_cpu_mem_usage",
        "is_trainable",
        "adapter_kwargs",
        "typing.Any]]"
    ],
    "params": [
        {
            "name": "peft_model_id",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The identifier of the model to look for on the Hub, or a local path to the saved adapter config fileand adapter weights."
        },
        {
            "name": "adapter_name",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The adapter name to use. If not set, will use the default adapter."
        },
        {
            "name": "revision",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use agit-based system for storing models and other artifacts on huggingface.co, so revision can be anyidentifier allowed by git.To test a pull request you made on the Hub, you can pass revision=refs/pr/<pr_number>."
        },
        {
            "name": "token",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Whether to use authentication token to load the remote folder. Useful to load private repositoriesthat are on HuggingFace Hub. You might need to call huggingface-cli login and paste your tokens tocache it."
        },
        {
            "name": "device_map",
            "type": "str,Dict[str, Union[int, str, torch.device]],int,torch.device",
            "optional": true,
            "default": "",
            "description": "A map that specifies where each submodule should go. It doesnt need to be refined to eachparameter/buffer name, once a given module name is inside, every submodule of it will be sent to thesame device. If we only pass the device (e.g., cpu, cuda:1, mps, or a GPU ordinal ranklike 1) on which the model will be allocated, the device map will map the entire model to thisdevice. Passing device_map = 0 means put the whole model on GPU 0.To have Accelerate compute the most optimized device_map automatically, set device_map=auto. Formore information about each option see designing a devicemap."
        },
        {
            "name": "max_memory",
            "type": "Dict",
            "optional": true,
            "default": "",
            "description": "A dictionary device identifier to maximum memory. Will default to the maximum memory available for eachGPU and the available CPU RAM if unset."
        },
        {
            "name": "offload_folder",
            "type": "str,os.PathLike",
            "optional": true,
            "default": "",
            "description": "If the device_map contains any value disk, the folder where we will offload weights."
        },
        {
            "name": "offload_index",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "offload_index argument to be passed to accelerate.dispatch_model method."
        },
        {
            "name": "peft_config",
            "type": "Dict[str, Any]",
            "optional": true,
            "default": "",
            "description": "The configuration of the adapter to add, supported adapters are non-prefix tuning and adaption promptsmethods. This argument is used in case users directly pass PEFT state dicts"
        },
        {
            "name": "adapter_state_dict",
            "type": "Dict[str, torch.Tensor]",
            "optional": true,
            "default": "",
            "description": "The state dict of the adapter to load. This argument is used in case users directly pass PEFT statedicts"
        },
        {
            "name": "low_cpu_mem_usage",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Reduce memory usage while loading the PEFT adapter. This should also speed up the loading process.Requires PEFT version 0.13.0 or higher."
        },
        {
            "name": "is_trainable",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether the adapter should be trainable or not. If False, the adapter will be frozen and can only beused for inference."
        },
        {
            "name": "adapter_kwargs",
            "type": "Dict[str, Any]",
            "optional": true,
            "default": "",
            "description": "Additional keyword arguments passed along to the from_pretrained method of the adapter config andfind_adapter_config_file method."
        }
    ],
    "return": ""
}