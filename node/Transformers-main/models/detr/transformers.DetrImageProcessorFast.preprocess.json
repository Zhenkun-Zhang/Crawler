{
    "api": "transformers.DetrImageProcessorFast.preprocess",
    "type": "function",
    "version": "main",
    "args_list": [
        "images:",
        "typing.Union[ForwardRef('PIL.Image.Image'),",
        "numpy.ndarray,",
        "ForwardRef('torch.Tensor'),",
        "list['PIL.Image.Image'],",
        "list[numpy.ndarray],",
        "list['torch.Tensor']]",
        "annotations",
        "typing.Union[int,",
        "str,",
        "list[dict]]],",
        "typing.List[dict[str,",
        "typing.Union[int,",
        "str,",
        "list[dict]]]],",
        "NoneType]",
        "masks_path",
        "pathlib.Path,",
        "NoneType]",
        "**kwargs"
    ],
    "params": [
        {
            "name": "images",
            "type": "ImageInput",
            "optional": false,
            "default": "",
            "description": "Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. Ifpassing in images with pixel values between 0 and 1, set do_rescale=False."
        },
        {
            "name": "do_resize",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to resize the image."
        },
        {
            "name": "size",
            "type": "Dict[str, int]",
            "optional": true,
            "default": "",
            "description": "Describes the maximum input dimensions to the model."
        },
        {
            "name": "resample",
            "type": "PILImageResampling,InterpolationMode",
            "optional": true,
            "default": "",
            "description": "Resampling filter to use if resizing the image. This can be one of the enum PILImageResampling. Onlyhas an effect if do_resize is set to True."
        },
        {
            "name": "do_center_crop",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to center crop the image."
        },
        {
            "name": "crop_size",
            "type": "Dict[str, int]",
            "optional": true,
            "default": "",
            "description": "Size of the output image after applying center_crop."
        },
        {
            "name": "do_rescale",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to rescale the image."
        },
        {
            "name": "rescale_factor",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Rescale factor to rescale the image by if do_rescale is set to True."
        },
        {
            "name": "do_normalize",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to normalize the image."
        },
        {
            "name": "image_mean",
            "type": "float,List[float]",
            "optional": true,
            "default": "",
            "description": "Image mean to use for normalization. Only has an effect if do_normalize is set to True."
        },
        {
            "name": "image_std",
            "type": "float,List[float]",
            "optional": true,
            "default": "",
            "description": "Image standard deviation to use for normalization. Only has an effect if do_normalize is set toTrue."
        },
        {
            "name": "do_convert_rgb",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to convert the image to RGB."
        },
        {
            "name": "return_tensors",
            "type": "str,TensorType",
            "optional": true,
            "default": "",
            "description": "Returns stacked tensors if set to `pt, otherwise returns a list of tensors."
        },
        {
            "name": "data_format",
            "type": "ChannelDimension,str",
            "optional": true,
            "default": "",
            "description": "Only ChannelDimension.FIRST is supported. Added for compatibility with slow processors."
        },
        {
            "name": "input_data_format",
            "type": "ChannelDimension,str",
            "optional": true,
            "default": "",
            "description": "The channel dimension format for the input image. If unset, the channel dimension format is inferredfrom the input image. Can be one of:channels_first or ChannelDimension.FIRST: image in (num_channels, height, width) format.channels_last or ChannelDimension.LAST: image in (height, width, num_channels) format.none or ChannelDimension.NONE: image in (height, width) format."
        },
        {
            "name": "device",
            "type": "torch.device",
            "optional": true,
            "default": "",
            "description": "The device to process the images on. If unset, the device is inferred from the input images."
        },
        {
            "name": "annotations",
            "type": "AnnotationType,List[AnnotationType]",
            "optional": true,
            "default": "",
            "description": "List of annotations associated with the image or batch of images. If annotation is for objectdetection, the annotations should be a dictionary with the following keys:image_id (int): The image id.annotations (List[Dict]): List of annotations for an image. Each annotation should be adictionary. An image can have no annotations, in which case the list should be empty.If annotation is for segmentation, the annotations should be a dictionary with the following keys:image_id (int): The image id.segments_info (List[Dict]): List of segments for an image. Each segment should be a dictionary.An image can have no segments, in which case the list should be empty.file_name (str): The file name of the image."
        },
        {
            "name": "format",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Data format of the annotations. One of coco_detection or coco_panoptic."
        },
        {
            "name": "do_convert_annotations",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Controls whether to convert the annotations to the format expected by the DETR model. Converts thebounding boxes to the format (center_x, center_y, width, height) and in the range [0, 1].Can be overridden by the do_convert_annotations parameter in the preprocess method."
        },
        {
            "name": "do_pad",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Controls whether to pad the image. Can be overridden by the do_pad parameter in the preprocessmethod. If True, padding will be applied to the bottom and right of the image with zeros.If pad_size is provided, the image will be padded to the specified dimensions.Otherwise, the image will be padded to the maximum height and width of the batch."
        },
        {
            "name": "pad_size",
            "type": "Dict[str, int]",
            "optional": true,
            "default": "",
            "description": "The size {height: int, width int} to pad the images to. Must be larger than any image sizeprovided for preprocessing. If pad_size is not provided, images will be padded to the largestheight and width in the batch."
        },
        {
            "name": "return_segmentation_masks",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to return segmentation masks."
        },
        {
            "name": "masks_path",
            "type": "str,pathlib.Path",
            "optional": true,
            "default": "",
            "description": "Path to the directory containing the segmentation masks."
        }
    ],
    "return": ""
}