{
    "api": "transformers.JukeboxPriorConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "act_fn",
        "level",
        "alignment_head",
        "alignment_layer",
        "attention_multiplier",
        "attention_pattern",
        "attn_dropout",
        "attn_res_scale",
        "blocks",
        "conv_res_scale",
        "num_layers",
        "emb_dropout",
        "encoder_config",
        "encoder_loss_fraction",
        "hidden_size",
        "init_scale",
        "is_encoder_decoder",
        "lyric_vocab_size",
        "mask",
        "max_duration",
        "max_nb_genres",
        "merged_decoder",
        "metadata_conditioning",
        "metadata_dims",
        "7898]",
        "min_duration",
        "mlp_multiplier",
        "music_vocab_size",
        "n_ctx",
        "n_heads",
        "nb_relevant_lyric_tokens",
        "res_conv_depth",
        "res_conv_width",
        "res_convolution_multiplier",
        "res_dilation_cycle",
        "res_dilation_growth_rate",
        "res_downs_t",
        "2,",
        "2]",
        "res_strides_t",
        "2,",
        "2]",
        "resid_dropout",
        "sampling_rate",
        "spread",
        "timing_dims",
        "zero_out",
        "**kwargs"
    ],
    "params": [
        {
            "name": "act_fn",
            "type": "str",
            "optional": true,
            "default": "quick_gelu",
            "description": "Activation function."
        },
        {
            "name": "alignment_head",
            "type": "int",
            "optional": true,
            "default": "2",
            "description": "Head that is responsible of the alignment between lyrics and music. Only used to compute the lyric to audioalignment"
        },
        {
            "name": "alignment_layer",
            "type": "int",
            "optional": true,
            "default": "68",
            "description": "Index of the layer that is responsible of the alignment between lyrics and music. Only used to compute thelyric to audio alignment"
        },
        {
            "name": "attention_multiplier",
            "type": "float",
            "optional": true,
            "default": "0.25",
            "description": "Multiplier coefficient used to define the hidden dimension of the attention layers. 0.25 means that0.25*width of the model will be used."
        },
        {
            "name": "attention_pattern",
            "type": "str",
            "optional": true,
            "default": "enc_dec_with_lyrics",
            "description": "Which attention pattern to use for the decoder/"
        },
        {
            "name": "attn_dropout",
            "type": "int",
            "optional": true,
            "default": "0",
            "description": "Dropout probability for the post-attention layer dropout in the decoder."
        },
        {
            "name": "attn_res_scale",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether or not to scale the residuals in the attention conditioner block."
        },
        {
            "name": "blocks",
            "type": "int",
            "optional": true,
            "default": "64",
            "description": "Number of blocks used in the block_attn. A sequence of length seq_len is factored as [blocks, seq_len // blocks] in the JukeboxAttention layer."
        },
        {
            "name": "conv_res_scale",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Whether or not to scale the residuals in the conditioner block. Since the top level prior does not have aconditioner, the default value is to None and should not be modified."
        },
        {
            "name": "num_layers",
            "type": "int",
            "optional": true,
            "default": "72",
            "description": "Number of layers of the transformer architecture."
        },
        {
            "name": "emb_dropout",
            "type": "int",
            "optional": true,
            "default": "0",
            "description": "Embedding dropout used in the lyric decoder."
        },
        {
            "name": "encoder_config",
            "type": "JukeboxPriorConfig",
            "optional": true,
            "default": "None",
            "description": "Configuration of the encoder which models the prior on the lyrics."
        },
        {
            "name": "encoder_loss_fraction",
            "type": "float",
            "optional": true,
            "default": "0.4",
            "description": "Multiplication factor used in front of the lyric encoder loss."
        },
        {
            "name": "hidden_size",
            "type": "int",
            "optional": true,
            "default": "2048",
            "description": "Hidden dimension of the attention layers."
        },
        {
            "name": "init_scale",
            "type": "float",
            "optional": true,
            "default": "0.2",
            "description": "Initialization scales for the prior modules."
        },
        {
            "name": "is_encoder_decoder",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not the prior is an encoder-decoder model. In case it is not, and nb_relevant_lyric_tokens isgreater than 0, the encoder args should be specified for the lyric encoding."
        },
        {
            "name": "mask",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether or not to mask the previous positions in the attention."
        },
        {
            "name": "max_duration",
            "type": "int",
            "optional": true,
            "default": "600",
            "description": "Maximum supported duration of the generated song in seconds."
        },
        {
            "name": "max_nb_genres",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Maximum number of genres that can be used to condition the model."
        },
        {
            "name": "merged_decoder",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not the decoder and the encoder inputs are merged. This is used for the separatedencoder-decoder architecture"
        },
        {
            "name": "metadata_conditioning",
            "type": "bool",
            "optional": true,
            "default": "True",
            "description": "Whether or not to condition on the artist and genre metadata."
        },
        {
            "name": "metadata_dims",
            "type": "List[int]",
            "optional": true,
            "default": "[604,",
            "description": "Number of genres and the number of artists that were used to train the embedding layers of the priormodels."
        },
        {
            "name": "min_duration",
            "type": "int",
            "optional": true,
            "default": "0",
            "description": "Minimum duration of the generated audio on which the model was trained."
        },
        {
            "name": "mlp_multiplier",
            "type": "float",
            "optional": true,
            "default": "1.0",
            "description": "Multiplier coefficient used to define the hidden dimension of the MLP layers. 0.25 means that 0.25*width ofthe model will be used."
        },
        {
            "name": "music_vocab_size",
            "type": "int",
            "optional": true,
            "default": "2048",
            "description": "Number of different music tokens. Should be similar to the JukeboxVQVAEConfig.nb_discrete_codes."
        },
        {
            "name": "n_ctx",
            "type": "int",
            "optional": true,
            "default": "6144",
            "description": "Number of context tokens for each prior. The context tokens are the music tokens that are attended to whengenerating music tokens."
        },
        {
            "name": "n_heads",
            "type": "int",
            "optional": true,
            "default": "2",
            "description": "Number of attention heads."
        },
        {
            "name": "nb_relevant_lyric_tokens",
            "type": "int",
            "optional": true,
            "default": "384",
            "description": "Number of lyric tokens that are used when sampling a single window of length n_ctx"
        },
        {
            "name": "res_conv_depth",
            "type": "int",
            "optional": true,
            "default": "3",
            "description": "Depth of the JukeboxDecoderConvBock used to upsample the previously sampled audio in theJukeboxMusicTokenConditioner."
        },
        {
            "name": "res_conv_width",
            "type": "int",
            "optional": true,
            "default": "128",
            "description": "Width of the JukeboxDecoderConvBock used to upsample the previously sampled audio in theJukeboxMusicTokenConditioner."
        },
        {
            "name": "res_convolution_multiplier",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Multiplier used to scale the hidden_dim of the JukeboxResConv1DBlock."
        },
        {
            "name": "res_dilation_cycle",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Dilation cycle used to define the JukeboxMusicTokenConditioner. Usually similar to the ones used in thecorresponding level of the VQVAE. The first prior does not use it as it is not conditioned on upper leveltokens."
        },
        {
            "name": "res_dilation_growth_rate",
            "type": "int",
            "optional": true,
            "default": "1",
            "description": "Dilation grow rate used between each convolutionnal block of the JukeboxMusicTokenConditioner"
        },
        {
            "name": "res_downs_t",
            "type": "List[int]",
            "optional": true,
            "default": "[3,",
            "description": "Downsampling rates used in the audio conditioning network"
        },
        {
            "name": "res_strides_t",
            "type": "List[int]",
            "optional": true,
            "default": "[2,",
            "description": "Striding used in the audio conditioning network"
        },
        {
            "name": "resid_dropout",
            "type": "int",
            "optional": true,
            "default": "0",
            "description": "Residual dropout used in the attention pattern."
        },
        {
            "name": "sampling_rate",
            "type": "int",
            "optional": true,
            "default": "44100",
            "description": "Sampling rate used for training."
        },
        {
            "name": "spread",
            "type": "int",
            "optional": true,
            "default": "None",
            "description": "Spread used in the summary_spread_attention pattern"
        },
        {
            "name": "timing_dims",
            "type": "int",
            "optional": true,
            "default": "64",
            "description": "Dimension of the timing embedding."
        },
        {
            "name": "zero_out",
            "type": "bool",
            "optional": true,
            "default": "False",
            "description": "Whether or not to zero out convolution weights when initializing."
        }
    ],
    "return": ""
}