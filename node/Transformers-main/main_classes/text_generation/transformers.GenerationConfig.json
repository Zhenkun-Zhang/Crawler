{
    "api": "transformers.GenerationConfig",
    "type": "class",
    "version": "main",
    "args_list": [
        "**kwargs"
    ],
    "params": [
        {
            "name": "max_length",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The maximum length the generated tokens can have. Corresponds to the length of the input prompt +max_new_tokens. Its effect is overridden by max_new_tokens, if also set."
        },
        {
            "name": "max_new_tokens",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt."
        },
        {
            "name": "min_length",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The minimum length of the sequence to be generated. Corresponds to the length of the input prompt +min_new_tokens. Its effect is overridden by min_new_tokens, if also set."
        },
        {
            "name": "min_new_tokens",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The minimum numbers of tokens to generate, ignoring the number of tokens in the prompt."
        },
        {
            "name": "early_stopping",
            "type": "bool,str",
            "optional": true,
            "default": "",
            "description": "Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:True, where the generation stops as soon as there are num_beams complete candidates, False, where anheuristic is applied and the generation stops when is it very unlikely to find better candidates,never, where the beam search procedure only stops when there cannot be better candidates (canonicalbeam search algorithm)."
        },
        {
            "name": "max_time",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The maximum amount of time you allow the computation to run for in seconds. generation will still finishthe current pass after allocated time has been passed."
        },
        {
            "name": "stop_strings",
            "type": "str,List[str]",
            "optional": true,
            "default": "",
            "description": "A string or a list of strings that should terminate generation if the model outputs them."
        },
        {
            "name": "do_sample",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to use sampling , use greedy decoding otherwise."
        },
        {
            "name": "num_beams",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of beams for beam search. 1 means no beam search."
        },
        {
            "name": "num_beam_groups",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Number of groups to divide num_beams into in order to ensure diversity among different groups of beams.this paper for more details."
        },
        {
            "name": "penalty_alpha",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The values balance the model confidence and the degeneration penalty in contrastive search decoding."
        },
        {
            "name": "dola_layers",
            "type": "str,List[int]",
            "optional": true,
            "default": "",
            "description": "The layers to use for DoLa decoding. If None, DoLa decoding is not used. If a string, it mustbe one of low or high, which means using the lower part or higher part of the model layers, respectively.low means the first half of the layers up to the first 20 layers, and high means the last half of thelayers up to the last 20 layers.If a list of integers, it must contain the indices of the layers to use for candidate premature layers in DoLa.The 0-th layer is the word embedding layer of the model. Set to low to improve long-answer reasoning tasks,high to improve short-answer tasks. Check the documentationor the paper for more details."
        },
        {
            "name": "use_cache",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not the model should use the past last key/values attentions (if applicable to the model) tospeed up decoding."
        },
        {
            "name": "cache_implementation",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Name of the cache class that will be instantiated in generate, for faster decoding. Possible values are:dynamic: DynamicCachestatic: StaticCacheoffloaded_static: OffloadedStaticCachesliding_window: SlidingWindowCachehybrid: HybridCachemamba: MambaCachequantized: QuantizedCacheIf none is specified, we will use the default cache for the model (which is often DynamicCache). Seeour cache documentation for further information."
        },
        {
            "name": "cache_config",
            "type": "CacheConfig,dict",
            "optional": true,
            "default": "",
            "description": "Arguments used in the key-value cache class can be passed in cache_config. Can be passed as a Dict andit will be converted to its repsective CacheConfig internally.Otherwise can be passed as a CacheConfig class matching the indicated cache_implementation."
        },
        {
            "name": "return_legacy_cache",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to return the legacy or new format of the cache when DynamicCache is used by default."
        },
        {
            "name": "temperature",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The value used to module the next token probabilities. This value is set in a models generation_config.json file. If it isnt set, the default value is 1.0"
        },
        {
            "name": "top_k",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The number of highest probability vocabulary tokens to keep for top-k-filtering. This value is set in a models generation_config.json file. If it isnt set, the default value is 50."
        },
        {
            "name": "top_p",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "If set to float < 1, only the smallest set of most probable tokens with probabilities that add up totop_p or higher are kept for generation. This value is set in a models generation_config.json file. If it isnt set, the default value is 1.0"
        },
        {
            "name": "min_p",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Minimum token probability, which will be scaled by the probability of the most likely token. It must be avalue between 0 and 1. Typical values are in the 0.01-0.2 range, comparably selective as setting top_p inthe 0.99-0.8 range (use the opposite of normal top_p values)."
        },
        {
            "name": "typical_p",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Local typicality measures how similar the conditional probability of predicting a target token next is tothe expected conditional probability of predicting a random token next, given the partial text alreadygenerated. If set to float < 1, the smallest set of the most locally typical tokens with probabilities thatadd up to typical_p or higher are kept for generation. See thispaper for more details."
        },
        {
            "name": "epsilon_cutoff",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "If set to float strictly between 0 and 1, only tokens with a conditional probability greater thanepsilon_cutoff will be sampled. In the paper, suggested values range from 3e-4 to 9e-4, depending on thesize of the model. See Truncation Sampling as Language ModelDesmoothing for more details."
        },
        {
            "name": "eta_cutoff",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to float strictly between0 and 1, a token is only considered if it is greater than either eta_cutoff or sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits))). The latter term is intuitively the expected next tokenprobability, scaled by sqrt(eta_cutoff). In the paper, suggested values range from 3e-4 to 2e-3,depending on the size of the model. See Truncation Sampling as Language ModelDesmoothing for more details."
        },
        {
            "name": "diversity_penalty",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "This value is subtracted from a beams score if it generates a token same as any beam from other group at aparticular time. Note that diversity_penalty is only effective if group beam search is enabled."
        },
        {
            "name": "repetition_penalty",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The parameter for repetition penalty. 1.0 means no penalty. See thispaper for more details."
        },
        {
            "name": "encoder_repetition_penalty",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The paramater for encoder_repetition_penalty. An exponential penalty on sequences that are not in theoriginal input. 1.0 means no penalty."
        },
        {
            "name": "length_penalty",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent tothe sequence length, which in turn is used to divide the score of the sequence. Since the score is the loglikelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, whilelength_penalty < 0.0 encourages shorter sequences."
        },
        {
            "name": "no_repeat_ngram_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "If set to int > 0, all ngrams of that size can only occur once."
        },
        {
            "name": "bad_words_ids",
            "type": "List[List[int]]",
            "optional": true,
            "default": "",
            "description": "List of list of token ids that are not allowed to be generated. CheckNoBadWordsLogitsProcessor for further documentation and examples."
        },
        {
            "name": "force_words_ids",
            "type": "List[List[int]],List[List[List[int]]]",
            "optional": true,
            "default": "",
            "description": "List of token ids that must be generated. If given a List[List[int]], this is treated as a simple list ofwords that must be included, the opposite to bad_words_ids. If given List[List[List[int]]], thistriggers a disjunctive constraint, where onecan allow different forms of each word."
        },
        {
            "name": "renormalize_logits",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to renormalize the logits after applying all the logits processors (including the customones). Its highly recommended to set this flag to True as the search algorithms suppose the score logitsare normalized but some logit processors break the normalization."
        },
        {
            "name": "constraints",
            "type": "List[Constraint]",
            "optional": true,
            "default": "",
            "description": "Custom constraints that can be added to the generation to ensure that the output will contain the use ofcertain tokens as defined by Constraint objects, in the most sensible way possible."
        },
        {
            "name": "forced_bos_token_id",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The id of the token to force as the first generated token after the decoder_start_token_id. Useful formultilingual models like mBART where the first generated token needs to be the targetlanguage token."
        },
        {
            "name": "forced_eos_token_id",
            "type": "int,List[int], *optional*, defaults to model.config.forced_eos_token_id) -- The id of the token to force as the last generated token when max_length` is reached. Optionally, use alist to set multiple end-of-sequence tokens.",
            "optional": true,
            "default": "",
            "description": ""
        },
        {
            "name": "remove_invalid_values",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to remove possible nan and inf outputs of the model to prevent the generation method to crash.Note that using remove_invalid_values can slow down generation."
        },
        {
            "name": "exponential_decay_length_penalty",
            "type": "tuple(int, float",
            "optional": true,
            "default": "",
            "description": "This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have beengenerated. The tuple shall consist of: (start_index, decay_factor) where start_index indicates wherepenalty starts and decay_factor represents the factor of exponential decay"
        },
        {
            "name": "suppress_tokens",
            "type": "List[int]",
            "optional": true,
            "default": "",
            "description": "A list of tokens that will be suppressed at generation. The SupressTokens logit processor will set theirlog probs to -inf so that they are not sampled."
        },
        {
            "name": "begin_suppress_tokens",
            "type": "List[int]",
            "optional": true,
            "default": "",
            "description": "A list of tokens that will be suppressed at the beginning of the generation. The SupressBeginTokens logitprocessor will set their log probs to -inf so that they are not sampled."
        },
        {
            "name": "forced_decoder_ids",
            "type": "List[List[int]]",
            "optional": true,
            "default": "",
            "description": "A list of pairs of integers which indicates a mapping from generation indices to token indices that will beforced before sampling. For example, [[1, 123]] means the second generated token will always be a tokenof index 123."
        },
        {
            "name": "sequence_bias",
            "type": "Dict[Tuple[int], float]",
            "optional": true,
            "default": "",
            "description": "Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of thesequence being selected, while negative biases do the opposite. CheckSequenceBiasLogitsProcessor for further documentation and examples."
        },
        {
            "name": "token_healing",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Heal tail tokens of prompts by replacing them with their appropriate extensions.This enhances the quality of completions for prompts affected by greedy tokenization bias."
        },
        {
            "name": "guidance_scale",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The guidance scale for classifier free guidance (CFG). CFG is enabled by setting guidance_scale > 1.Higher guidance scale encourages the model to generate samples that are more closely linked to the inputprompt, usually at the expense of poorer quality."
        },
        {
            "name": "low_memory",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Switch to sequential beam search and sequential topk for contrastive search to reduce peak memory.Used with beam search and contrastive search."
        },
        {
            "name": "watermarking_config",
            "type": "BaseWatermarkingConfig,dict",
            "optional": true,
            "default": "",
            "description": "Arguments used to watermark the model outputs by adding a small bias to randomly selected set of greentokens. See the docs of SynthIDTextWatermarkingConfig and WatermarkingConfig for moredetails. If passed as Dict, it will be converted to a WatermarkingConfig internally."
        },
        {
            "name": "num_return_sequences",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The number of independently computed returned sequences for each element in the batch."
        },
        {
            "name": "output_attentions",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the attentions tensors of all attention layers. See attentions under returnedtensors for more details."
        },
        {
            "name": "output_hidden_states",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the hidden states of all layers. See hidden_states under returned tensors formore details."
        },
        {
            "name": "output_scores",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the prediction scores. See scores under returned tensors for more details."
        },
        {
            "name": "output_logits",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return the unprocessed prediction logit scores. See logits under returned tensors formore details."
        },
        {
            "name": "return_dict_in_generate",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether or not to return a ModelOutput, as opposed to returning exclusively the generatedsequence. This flag must be set to True to return the generation cache (when use_cache is True)or optional outputs (see flags starting with output_)"
        },
        {
            "name": "pad_token_id",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The id of the padding token."
        },
        {
            "name": "bos_token_id",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The id of the beginning-of-sequence token."
        },
        {
            "name": "eos_token_id",
            "type": "Union[int, List[int]]",
            "optional": true,
            "default": "",
            "description": "The id of the end-of-sequence token. Optionally, use a list to set multiple end-of-sequence tokens."
        },
        {
            "name": "encoder_no_repeat_ngram_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "If set to int > 0, all ngrams of that size that occur in the encoder_input_ids cannot occur in thedecoder_input_ids."
        },
        {
            "name": "decoder_start_token_id",
            "type": "int,List[int]",
            "optional": true,
            "default": "",
            "description": "If an encoder-decoder model starts decoding with a different token than bos, the id of that token or a list of lengthbatch_size. Indicating a list enables different start ids for each element in the batch(e.g. multilingual models with different target languages in one batch)"
        },
        {
            "name": "is_assistant",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether the model is an assistant (draft) model."
        },
        {
            "name": "num_assistant_tokens",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Defines the number of speculative tokens that shall be generated by the assistant model before beingchecked by the target model at each iteration. Higher values for num_assistant_tokens make the generationmore speculative : If the assistant model is performant larger speed-ups can be reached, if the assistantmodel requires lots of corrections, lower speed-ups are reached."
        },
        {
            "name": "num_assistant_tokens_schedule",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "Defines the schedule at which max assistant tokens shall be changed during inference.heuristic: When all speculative tokens are correct, increase num_assistant_tokens by 2 elsereduce by 1. num_assistant_tokens value is persistent over multiple generation calls with the same assistant model.heuristic_transient: Same as heuristic but num_assistant_tokens is reset to its initial value after each generation call.constant: num_assistant_tokens stays unchanged during generation"
        },
        {
            "name": "assistant_confidence_threshold",
            "type": "float",
            "optional": true,
            "default": "",
            "description": "The confidence threshold for the assistant model. If the assistant models confidence in its prediction for the current token is lowerthan this threshold, the assistant model stops the current token generation iteration, even if the number of speculative tokens(defined by num_assistant_tokens) is not yet reached. The assistants confidence threshold is adjusted throughout the speculative iterations to reduce the number of unnecessary draft and target forward passes, biased towards avoiding false negatives.assistant_confidence_threshold value is persistent over multiple generation calls with the same assistant model.It is an unsupervised version of the dynamic speculation lookaheadfrom Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large Language Models https://arxiv.org/abs/2405.04304."
        },
        {
            "name": "prompt_lookup_num_tokens",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The number of tokens to be output as candidate tokens."
        },
        {
            "name": "max_matching_ngram_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "The maximum ngram size to be considered for matching in the prompt. Default to 2 if not provided."
        },
        {
            "name": "assistant_early_exit(int,",
            "type": "optional",
            "optional": true,
            "default": "",
            "description": "If set to a positive integer, early exit of the model will be used as an assistant. Can only be used withmodels that support early exit (i.e. models where logits from intermediate layers can be interpreted by the LM head)."
        },
        {
            "name": "assistant_lookbehind(int,",
            "type": "optional, defaults to 10",
            "optional": true,
            "default": "",
            "description": "If set to a positive integer, the re-encodeing process will additionally consider the last assistant_lookbehind assistant tokensto correctly align tokens. Can only be used with different tokenizers in speculative decoding.See this blog for more details."
        },
        {
            "name": "target_lookbehind(int,",
            "type": "optional, defaults to 10",
            "optional": true,
            "default": "",
            "description": "If set to a positive integer, the re-encodeing process will additionally consider the last target_lookbehind target tokensto correctly align tokens. Can only be used with different tokenizers in speculative decoding.See this blog for more details."
        },
        {
            "name": "compile_config",
            "type": "CompileConfig",
            "optional": true,
            "default": "",
            "description": "If using a static cache, this controls how generate will compile the forward pass for performancegains."
        },
        {
            "name": "disable_compile",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Whether to disable the automatic compilation of the forward pass. Automatic compilation happens when specific criteria are met, including using a compileable cache. Please open an issue if you find the need to use this flag."
        },
        {
            "name": "generation_kwargs",
            "type": "",
            "optional": false,
            "default": "",
            "description": "Additional generation kwargs will be forwarded to the generate function of the model. Kwargs that are notpresent in generates signature will be used in the model forward pass."
        }
    ],
    "return": ""
}