{
    "api": "transformers.TranslationPipeline",
    "type": "class",
    "version": "main",
    "args_list": [
        "*args",
        "**kwargs"
    ],
    "params": [
        {
            "name": "model",
            "type": "PreTrainedModel,TFPreTrainedModel",
            "optional": false,
            "default": "",
            "description": "The model that will be used by the pipeline to make predictions. This needs to be a model inheriting fromPreTrainedModel for PyTorch and TFPreTrainedModel for TensorFlow."
        },
        {
            "name": "tokenizer",
            "type": "PreTrainedTokenizer",
            "optional": false,
            "default": "",
            "description": "The tokenizer that will be used by the pipeline to encode data for the model. This object inherits fromPreTrainedTokenizer."
        },
        {
            "name": "modelcard",
            "type": "str,ModelCard",
            "optional": true,
            "default": "",
            "description": "Model card attributed to the model for this pipeline."
        },
        {
            "name": "framework",
            "type": "str",
            "optional": true,
            "default": "",
            "description": "The framework to use, either pt for PyTorch or tf for TensorFlow. The specified framework must beinstalled.If no framework is specified, will default to the one currently installed. If no framework is specified andboth frameworks are installed, will default to the framework of the model, or to PyTorch if no model isprovided."
        },
        {
            "name": "task",
            "type": "str, defaults to \"\"",
            "optional": false,
            "default": "",
            "description": "A task-identifier for the pipeline."
        },
        {
            "name": "num_workers",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "When the pipeline will use DataLoader (when passing a dataset, on GPU for a Pytorch model), the number ofworkers to be used."
        },
        {
            "name": "batch_size",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "When the pipeline will use DataLoader (when passing a dataset, on GPU for a Pytorch model), the size ofthe batch to use, for inference this is not always beneficial, please read Batching withpipelines ."
        },
        {
            "name": "args_parser",
            "type": "ArgumentHandler",
            "optional": true,
            "default": "",
            "description": "Reference to the object in charge of parsing supplied pipeline parameters."
        },
        {
            "name": "device",
            "type": "int",
            "optional": true,
            "default": "",
            "description": "Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model onthe associated CUDA device id. You can pass native torch.device or a str too"
        },
        {
            "name": "torch_dtype",
            "type": "str,torch.dtype",
            "optional": true,
            "default": "",
            "description": "Sent directly as model_kwargs (just a simpler shortcut) to use the available precision for this model(torch.float16, torch.bfloat16, â€¦ or auto)"
        },
        {
            "name": "binary_output",
            "type": "bool",
            "optional": true,
            "default": "",
            "description": "Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or asthe raw output data e.g. text."
        }
    ],
    "return": ""
}