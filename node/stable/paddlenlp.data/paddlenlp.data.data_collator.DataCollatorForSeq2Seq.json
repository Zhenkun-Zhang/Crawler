{
    "api": "paddlenlp.data.data_collator.DataCollatorForSeq2Seq",
    "type": "class",
    "version": "stable",
    "args_list": {
        "tokenizer": "PretrainedTokenizerBase",
        "model": "Any or None = None",
        "padding": "bool or str or PaddingStrategy = True",
        "max_length": "int or None = None",
        "pad_to_multiple_of": "int or None = None",
        "label_pad_token_id": "int = -100",
        "return_tensors": "str = 'pd'",
        "return_attention_mask": "bool or None = None",
        "max_label_length": "int or None = None"
    },
    "Bases": "object",
    "description": "Data collator that will dynamically pad the inputs received, as well as the labels.",
    "params": [
        {
            "name": "tokenizer",
            "type": "[PretrainedTokenizer] or [PretrainedFasterTokenizer]",
            "description": "The tokenizer used for encoding the data.",
            "default": "",
            "optional": false
        },
        {
            "name": "model",
            "type": "[PreTrainedModel]",
            "description": "The model that is being trained. If set and has the prepare_decoder_input_ids_from_labels, use it toprepare the decoder_input_idsThis is useful when using label_smoothing to avoid calculating loss twice.The model that is being trained. If set and has the prepare_decoder_input_ids_from_labels, use it toprepare the decoder_input_ids",
            "default": "",
            "optional": false
        },
        {
            "name": "padding",
            "type": "bool, str or [PaddingStrategy]",
            "description": "Select a strategy to pad the returned sequences (according to the models padding side and padding index)among:True or longest: Pad to the longest sequence in the batch (or no padding if only a single sequenceis provided).max_length: Pad to a maximum length specified with the argument max_length or to the maximumacceptable input length for the model if that argument is not provided.False or do_not_pad (default): No padding (i.e., can output a batch with sequences of differentlengths).Select a strategy to pad the returned sequences (according to the models padding side and padding index)among:True or longest: Pad to the longest sequence in the batch (or no padding if only a single sequenceis provided).max_length: Pad to a maximum length specified with the argument max_length or to the maximumacceptable input length for the model if that argument is not provided.",
            "default": "",
            "optional": true
        },
        {
            "name": "max_length",
            "type": "int",
            "description": "Maximum length of the returned list and optionally padding length (see above).",
            "default": "",
            "optional": true
        },
        {
            "name": "pad_to_multiple_of",
            "type": "int",
            "description": "If set will pad the sequence to a multiple of the provided value.This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=7.5 (Volta).If set will pad the sequence to a multiple of the provided value.",
            "default": "",
            "optional": true
        },
        {
            "name": "label_pad_token_id",
            "type": "int",
            "description": "The id to use when padding the labels (-100 will be automatically ignored by PaddlePaddle loss functions).",
            "default": "",
            "optional": true
        },
        {
            "name": "return_tensors",
            "type": "str",
            "description": "The type of Tensor to return. Allowable values are np, pt and tf.",
            "default": "",
            "optional": false
        },
        {
            "name": "max_label_length",
            "type": "int",
            "description": "",
            "default": "",
            "optional": true
        }
    ],
    "return": {
        "description": "",
        "type": ""
    }
}